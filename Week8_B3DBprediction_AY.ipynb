{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7XGEKpbm4ki",
        "outputId": "fd414197-65f2-4cd7-fcf4-5ab9f6283f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n",
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (553 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Collecting torchdata>=0.5.0 (from dgl)\n",
            "  Downloading torchdata-0.9.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.8.30)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (3.0.2)\n",
            "Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.9.0-cp310-cp310-manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata, dgl\n",
            "Successfully installed dgl-2.1.0 torchdata-0.9.0\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install dgl\n",
        "!pip install shap\n",
        "!pip install optuna\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing packages\n",
        "!pip uninstall -y deepchem dgl torchdata torch\n",
        "\n",
        "# Install compatible versions\n",
        "!pip install torch==1.12.0 torchdata==0.4.0 dgl==1.1.0 deepchem\n",
        "\n",
        "# Set DGL backend to PyTorch\n",
        "import os\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "# Verify installation\n",
        "import deepchem as dc\n",
        "import torch\n",
        "import dgl\n",
        "\n",
        "print(\"DeepChem, Torch, and DGL imports successful!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzJbiG2hnGXI",
        "outputId": "805df673-8bf3-466a-834a-d22fff93c694"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping deepchem as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: dgl 2.1.0\n",
            "Uninstalling dgl-2.1.0:\n",
            "  Successfully uninstalled dgl-2.1.0\n",
            "Found existing installation: torchdata 0.9.0\n",
            "Uninstalling torchdata-0.9.0:\n",
            "  Successfully uninstalled torchdata-0.9.0\n",
            "Found existing installation: torch 2.5.0+cu121\n",
            "Uninstalling torch-2.5.0+cu121:\n",
            "  Successfully uninstalled torch-2.5.0+cu121\n",
            "Collecting torch==1.12.0\n",
            "  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n",
            "Collecting torchdata==0.4.0\n",
            "  Downloading torchdata-0.4.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting dgl==1.1.0\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (557 bytes)\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.8.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.0) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.0) (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.0) (2.32.3)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.4.0)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (3.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (5.9.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.13.1)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2024.3.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.0) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2024.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (10.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->deepchem) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
            "Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.4.0-cp310-cp310-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: torch, portalocker, torchdata, dgl, deepchem\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deepchem-2.8.0 dgl-1.1.0 portalocker-2.10.1 torch-1.12.0 torchdata-0.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for SPS. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepChem, Torch, and DGL imports successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "file = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "xkmSv-a3nHvp",
        "outputId": "5ab7302f-2b41-4e5c-de1c-6e274065e369"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57f7311d-b673-44a5-a8e1-6c20f4f55292\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57f7311d-b673-44a5-a8e1-6c20f4f55292\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving B3DB_regression.tsv to B3DB_regression.tsv\n",
            "Saving B3DB_classification.tsv to B3DB_classification.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load regression dataset\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\",\n",
        "                              sep=\"\\t\")\n",
        "\n",
        "# load classification dataset\n",
        "classification_data = pd.read_csv(\"B3DB_classification.tsv\",\n",
        "                                  sep=\"\\t\")\n",
        "print(regression_data.head())\n",
        "print(classification_data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-nhl-KrnJt3",
        "outputId": "d1bdd8c2-2c44-4f25-b4c3-7ccc5295750b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NO.                                      compound_name  \\\n",
            "0    1                                         moxalactam   \n",
            "1    2                                      schembl614298   \n",
            "2    3                             morphine-6-glucuronide   \n",
            "3    4  2-[4-(5-bromo-3-methylpyridin-2-yl)butylamino]...   \n",
            "4    5                                                NaN   \n",
            "\n",
            "                                          IUPAC_name  \\\n",
            "0  7-[[2-carboxy-2-(4-hydroxyphenyl)acetyl]amino]...   \n",
            "1  (2s,3s,4s,5r)-6-[[(4r,4ar,7s,7ar,12bs)-7-hydro...   \n",
            "2  (2s,3s,4s,5r)-6-[[(4r,4ar,7s,7ar,12bs)-9-hydro...   \n",
            "3  2-[4-(5-bromo-3-methylpyridin-2-yl)butylamino]...   \n",
            "4                                                NaN   \n",
            "\n",
            "                                              SMILES          CID  logBB  \\\n",
            "0  CN1C(=NN=N1)SCC2=C(N3C(C(C3=O)(NC(=O)C(C4=CC=C...      3889.0|  -2.52   \n",
            "1  CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)OC6[C@...  18595497.0|  -2.15   \n",
            "2  CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)O)O[C@...   9847115.0|  -2.09   \n",
            "3   CC1=NC=C(C=C1)CC2CNC(NC2=O)NCCCCC3=NC=C(C=C3C)Br  74041479.0|  -1.88   \n",
            "4  c1(c2c3n(c4c(C(N(C)C3)=O)c(Cl)ccc4)cn2)noc(C(O...         nan|  -1.82   \n",
            "\n",
            "                                               Inchi reference group comments  \n",
            "0  InChI=1S/C20H20N6O9S/c1-25-19(22-23-24-25)36-8...      R25|     A      NaN  \n",
            "1  InChI=1S/C23H27NO9/c1-24-7-6-23-10-3-4-12(25)2...      R25|     A      NaN  \n",
            "2  InChI=1S/C23H27NO9/c1-24-7-6-23-10-3-5-13(31-2...      R25|     A      NaN  \n",
            "3  InChI=1S/C21H28BrN5O/c1-14-9-18(22)13-25-19(14...      R35|     A      NaN  \n",
            "4  InChI=1S/C16H14ClN5O4/c1-16(24,25)15-19-13(20-...      R35|     A      NaN  \n",
            "   NO.                        compound_name  \\\n",
            "0    1                       sulphasalazine   \n",
            "1    2                           moxalactam   \n",
            "2    3                           clioquinol   \n",
            "3    4  bbcpd11 (cimetidine analog) (y-g13)   \n",
            "4    5                        schembl614298   \n",
            "\n",
            "                                          IUPAC_name  \\\n",
            "0  2-hydroxy-5-[[4-(pyridin-2-ylsulfamoyl)phenyl]...   \n",
            "1  7-[[2-carboxy-2-(4-hydroxyphenyl)acetyl]amino]...   \n",
            "2                       5-chloro-7-iodoquinolin-8-ol   \n",
            "3  2-[2-[(3-bromopyridin-2-yl)methylsulfanyl]ethy...   \n",
            "4  (2s,3s,4s,5r)-6-[[(4r,4ar,7s,7ar,12bs)-7-hydro...   \n",
            "\n",
            "                                              SMILES         CID  logBB  \\\n",
            "0   O=C(O)c1cc(N=Nc2ccc(S(=O)(=O)Nc3ccccn3)cc2)ccc1O      5339.0  -2.69   \n",
            "1  COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...      3889.0  -2.52   \n",
            "2                             Oc1c(I)cc(Cl)c2cccnc12      2788.0  -2.40   \n",
            "3                         CCNC(=NCCSCc1ncccc1Br)NC#N  14022517.0  -2.15   \n",
            "4  CN1CC[C@]23c4c5ccc(OC6O[C@H](C(=O)O)[C@@H](O)[...  18595497.0  -2.15   \n",
            "\n",
            "  BBB+/BBB-                                              Inchi  threshold  \\\n",
            "0      BBB-  InChI=1S/C18H14N4O5S/c23-16-9-6-13(11-15(16)18...        NaN   \n",
            "1      BBB-  InChI=1S/C20H20N6O9S/c1-25-19(22-23-24-25)36-8...        NaN   \n",
            "2      BBB-  InChI=1S/C9H5ClINO/c10-6-4-7(11)9(13)8-5(6)2-1...        NaN   \n",
            "3      BBB-  InChI=1S/C12H16BrN5S/c1-2-15-12(18-9-14)17-6-7...        NaN   \n",
            "4      BBB-  InChI=1S/C23H27NO9/c1-24-7-6-23-10-3-4-12(25)2...        NaN   \n",
            "\n",
            "                                           reference group comments  \n",
            "0                                     R2|R2|R25|R46|     A      NaN  \n",
            "1                                               R25|     A      NaN  \n",
            "2                                       R18|R26|R27|     A      NaN  \n",
            "3  R2|R2|R8|R40|R2|R2|R2|R2|R18|R21|R25|R25|R26|R...     A      NaN  \n",
            "4                                               R25|     A      NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Classification**"
      ],
      "metadata": {
        "id": "4qtAulRhvQQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    ''' Calculate the full list of descriptors for a molecule '''\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating descriptor {name}: {e}\")\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Load the classification dataset\n",
        "classification_data = pd.read_csv(\"B3DB_classification.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Drop rows with missing target labels\n",
        "classification_data = classification_data.dropna(subset=['BBB+/BBB-'])\n",
        "\n",
        "# Prepare to store descriptors\n",
        "descriptor_data = []\n",
        "\n",
        "for i, row in classification_data.iterrows():\n",
        "    smiles = row['SMILES']\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    if mol is not None:\n",
        "        # Calculate descriptors for the molecule\n",
        "        descriptors = getMolDescriptors(mol)\n",
        "        descriptor_data.append(descriptors)\n",
        "    else:\n",
        "        print(f\"Invalid SMILES string: {smiles}\")\n",
        "        descriptor_data.append({name: None for name, _ in Descriptors._descList})\n",
        "\n",
        "# Create a DataFrame for descriptors only\n",
        "descriptor_df = pd.DataFrame(descriptor_data)\n",
        "\n",
        "# Drop rows with missing descriptors or target labels\n",
        "classification_data = classification_data.reset_index(drop=True)\n",
        "classification_df = pd.concat([classification_data[['BBB+/BBB-']], descriptor_df], axis=1).dropna()\n",
        "\n",
        "# Combine descriptors with the target column, then keep only descriptor columns\n",
        "classification_df = pd.concat([classification_data[['BBB+/BBB-']], descriptor_df], axis=1).dropna()\n",
        "X = classification_df[descriptor_df.columns]  # Only use descriptor columns as features\n",
        "y = LabelEncoder().fit_transform(classification_df['BBB+/BBB-'])\n",
        "print(f\"count of labels: {pd.Series(y).value_counts()}\")\n",
        "print(f\"summary of descriptors: \\n{X.describe()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BULZp7kjokBl",
        "outputId": "717fecaa-fe88-4700-ab0e-ba91ea7dee31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:02] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:03] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:04] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:06] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:08] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:09] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:10] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:11] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:12] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:13] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:14] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:15] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:16] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:17] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:18] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:19] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:20] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:22] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:23] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:24] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:25] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:26] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:27] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n",
            "[07:14:28] DEPRECATION WARNING: please use MorganGenerator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of labels: 0    4951\n",
            "1    2851\n",
            "Name: count, dtype: int64\n",
            "summary of descriptors: \n",
            "       MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex  \\\n",
            "count        7802.000000     7802.000000        7802.000000     7802.000000   \n",
            "mean           11.648751       11.648671           0.183437       -1.065562   \n",
            "std             3.063385        3.063424           0.294825        1.285377   \n",
            "min             0.000000        0.000000           0.000000      -10.472222   \n",
            "25%            10.940804       10.940804           0.038580       -1.602720   \n",
            "50%            12.392110       12.392110           0.095475       -0.881852   \n",
            "75%            13.174413       13.174413           0.210645       -0.213405   \n",
            "max            17.815441       17.815441           9.847222        6.000000   \n",
            "\n",
            "               qed          SPS        MolWt  HeavyAtomMolWt   ExactMolWt  \\\n",
            "count  7802.000000  7802.000000  7802.000000     7802.000000  7802.000000   \n",
            "mean      0.579746    25.744136   385.658238      360.194416   385.246249   \n",
            "std       0.225891    12.848109   170.714679      159.470214   170.582003   \n",
            "min       0.010267     0.000000    16.043000       12.011000    16.031300   \n",
            "25%       0.426712    15.428571   285.343000      266.191000   285.092042   \n",
            "50%       0.635835    22.083333   362.486000      341.107000   362.163043   \n",
            "75%       0.763406    33.727273   451.607000      422.313000   451.270247   \n",
            "max       0.945893    64.384615  1882.332000     1781.904000  1881.070533   \n",
            "\n",
            "       NumValenceElectrons  ...   fr_sulfide  fr_sulfonamd   fr_sulfone  \\\n",
            "count          7802.000000  ...  7802.000000   7802.000000  7802.000000   \n",
            "mean            145.327865  ...     0.153935      0.029864     0.022430   \n",
            "std              66.338577  ...     0.440851      0.185364     0.149809   \n",
            "min               8.000000  ...     0.000000      0.000000     0.000000   \n",
            "25%             106.000000  ...     0.000000      0.000000     0.000000   \n",
            "50%             136.000000  ...     0.000000      0.000000     0.000000   \n",
            "75%             170.000000  ...     0.000000      0.000000     0.000000   \n",
            "max             738.000000  ...     4.000000      2.000000     2.000000   \n",
            "\n",
            "       fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  \\\n",
            "count        7802.000000   7802.000000  7802.000000       7802.0   \n",
            "mean            0.008844      0.014099     0.036914          0.0   \n",
            "std             0.093631      0.117907     0.203598          0.0   \n",
            "min             0.000000      0.000000     0.000000          0.0   \n",
            "25%             0.000000      0.000000     0.000000          0.0   \n",
            "50%             0.000000      0.000000     0.000000          0.0   \n",
            "75%             0.000000      0.000000     0.000000          0.0   \n",
            "max             1.000000      1.000000     2.000000          0.0   \n",
            "\n",
            "       fr_thiophene  fr_unbrch_alkane      fr_urea  \n",
            "count   7802.000000       7802.000000  7802.000000  \n",
            "mean       0.018072          0.122020     0.033581  \n",
            "std        0.139795          0.851026     0.185765  \n",
            "min        0.000000          0.000000     0.000000  \n",
            "25%        0.000000          0.000000     0.000000  \n",
            "50%        0.000000          0.000000     0.000000  \n",
            "75%        0.000000          0.000000     0.000000  \n",
            "max        2.000000         19.000000     2.000000  \n",
            "\n",
            "[8 rows x 210 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count the number of positive and negative infinity values in each column\n",
        "inf_counts = (X == np.inf).sum().sum()\n",
        "neg_inf_counts = (X == -np.inf).sum().sum()\n",
        "\n",
        "print(f\"Count of positive infinity values: {inf_counts}\")\n",
        "print(f\"Count of negative infinity values: {neg_inf_counts}\")\n",
        "\n",
        "# Check for NaN values\n",
        "nan_counts = X.isna().sum().sum()\n",
        "print(f\"Count of NaN values: {nan_counts}\")\n",
        "import numpy as np\n",
        "\n",
        "# Check for extremely large values in each column\n",
        "extreme_thresholds = X.quantile(0.999)\n",
        "extreme_values = (X > extreme_thresholds).sum()\n",
        "\n",
        "print(\"Count of extreme values in each column:\")\n",
        "print(extreme_values[extreme_values > 0])\n",
        "# Cap values above the 99.9th percentile to the threshold value\n",
        "for column in X.columns:\n",
        "    cap_value = extreme_thresholds[column]\n",
        "    X[column] = np.where(X[column] > cap_value, cap_value, X[column])\n",
        "\n",
        "# Verify if the capping removed the issue by checking max values again\n",
        "print(\"\\nSummary after capping extreme values:\")\n",
        "print(X.describe().loc['max'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52oAS_O9rI_I",
        "outputId": "b71196df-0bf1-4f58-da4a-91b686f342ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of positive infinity values: 0\n",
            "Count of negative infinity values: 0\n",
            "Count of NaN values: 0\n",
            "Count of extreme values in each column:\n",
            "MaxAbsEStateIndex    3\n",
            "MaxEStateIndex       3\n",
            "MinAbsEStateIndex    7\n",
            "MinEStateIndex       7\n",
            "qed                  5\n",
            "                    ..\n",
            "fr_sulfide           8\n",
            "fr_sulfone           2\n",
            "fr_thiophene         7\n",
            "fr_unbrch_alkane     8\n",
            "fr_urea              8\n",
            "Length: 183, dtype: int64\n",
            "\n",
            "Summary after capping extreme values:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-be5307f8de14>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[column] = np.where(X[column] > cap_value, cap_value, X[column])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaxAbsEStateIndex    17.808922\n",
            "MaxEStateIndex       17.808922\n",
            "MinAbsEStateIndex     2.921682\n",
            "MinEStateIndex        1.500000\n",
            "qed                   0.941855\n",
            "                       ...    \n",
            "fr_thiazole           2.000000\n",
            "fr_thiocyan           0.000000\n",
            "fr_thiophene          1.000000\n",
            "fr_unbrch_alkane     13.199000\n",
            "fr_urea               1.199000\n",
            "Name: max, Length: 210, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types and value ranges\n",
        "print(\"Data types and max values in each column after capping:\")\n",
        "print(X.dtypes)\n",
        "print(\"\\nMaximum values in each column:\")\n",
        "print(X.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dVkN4ylsnBV",
        "outputId": "97c9f684-9278-4843-8421-547ba9f446d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types and max values in each column after capping:\n",
            "MaxAbsEStateIndex    float64\n",
            "MaxEStateIndex       float64\n",
            "MinAbsEStateIndex    float64\n",
            "MinEStateIndex       float64\n",
            "qed                  float64\n",
            "                      ...   \n",
            "fr_thiazole          float64\n",
            "fr_thiocyan          float64\n",
            "fr_thiophene         float64\n",
            "fr_unbrch_alkane     float64\n",
            "fr_urea              float64\n",
            "Length: 210, dtype: object\n",
            "\n",
            "Maximum values in each column:\n",
            "MaxAbsEStateIndex    17.808922\n",
            "MaxEStateIndex       17.808922\n",
            "MinAbsEStateIndex     2.921682\n",
            "MinEStateIndex        1.500000\n",
            "qed                   0.941855\n",
            "                       ...    \n",
            "fr_thiazole           2.000000\n",
            "fr_thiocyan           0.000000\n",
            "fr_thiophene          1.000000\n",
            "fr_unbrch_alkane     13.199000\n",
            "fr_urea               1.199000\n",
            "Length: 210, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the RandomForest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Set Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yitR9LbqS57",
        "outputId": "15bc2630-81d2-4c16-8358-8d5c83d8a0a3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.86\n",
            "Recall: 0.81\n",
            "F1 Score: 0.84\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91       991\n",
            "           1       0.86      0.81      0.84       570\n",
            "\n",
            "    accuracy                           0.88      1561\n",
            "   macro avg       0.88      0.87      0.87      1561\n",
            "weighted avg       0.88      0.88      0.88      1561\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1', verbose=3, return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Extract and display each parameter combination with its corresponding mean test score\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "display_columns = [\n",
        "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
        "    'mean_test_score', 'std_test_score', 'mean_train_score', 'std_train_score'\n",
        "]\n",
        "results_df = results_df[display_columns].sort_values(by='mean_test_score', ascending=False)\n",
        "\n",
        "print(\"Detailed results for each hyperparameter combination:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL8fUy_nu6oC",
        "outputId": "f5e77d7c-54f6-48f2-b8d6-0af189693c43"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.982, test=0.811) total time=   2.8s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.980, test=0.826) total time=   2.9s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.981, test=0.834) total time=   2.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.981, test=0.829) total time=   2.8s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.981, test=0.848) total time=   3.0s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.982, test=0.812) total time=   5.6s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.980, test=0.832) total time=   5.6s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.981, test=0.840) total time=   5.6s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.981, test=0.828) total time=   5.5s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.981, test=0.853) total time=   5.8s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.982, test=0.809) total time=   8.5s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.980, test=0.832) total time=   8.4s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.981, test=0.838) total time=   8.4s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.981, test=0.820) total time=   8.7s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.981, test=0.849) total time=   8.8s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.809) total time=   2.7s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.831) total time=   2.7s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.981, test=0.835) total time=   2.7s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.979, test=0.832) total time=   2.9s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.843) total time=   2.8s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.981, test=0.809) total time=   5.5s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.980, test=0.836) total time=   5.6s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.981, test=0.841) total time=   5.4s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.980, test=0.829) total time=   5.5s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.980, test=0.846) total time=   5.5s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.981, test=0.809) total time=   8.2s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.980, test=0.832) total time=   8.4s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.981, test=0.835) total time=   8.1s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.981, test=0.826) total time=   8.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.980, test=0.843) total time=   8.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.972, test=0.813) total time=   2.7s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.971, test=0.821) total time=   2.7s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.974, test=0.833) total time=   2.6s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.971, test=0.834) total time=   2.6s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.970, test=0.843) total time=   2.8s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.971, test=0.814) total time=   5.6s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.972, test=0.821) total time=   5.4s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.972, test=0.828) total time=   5.4s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.970, test=0.821) total time=   5.3s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.972, test=0.843) total time=   5.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.972, test=0.814) total time=   8.0s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.972, test=0.821) total time=   8.1s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.973, test=0.830) total time=   7.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.971, test=0.824) total time=   7.9s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.973, test=0.847) total time=   8.0s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.895, test=0.777) total time=   1.9s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.893, test=0.795) total time=   1.9s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.895, test=0.813) total time=   1.9s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.895, test=0.796) total time=   1.9s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.890, test=0.813) total time=   1.9s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.899, test=0.781) total time=   3.7s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.893, test=0.797) total time=   3.7s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.898, test=0.815) total time=   3.8s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.895, test=0.793) total time=   3.7s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.892, test=0.819) total time=   3.7s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.899, test=0.783) total time=   5.7s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.893, test=0.794) total time=   5.6s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.898, test=0.812) total time=   5.7s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.896, test=0.797) total time=   5.5s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.895, test=0.818) total time=   5.7s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.891, test=0.777) total time=   1.9s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.889, test=0.798) total time=   1.8s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.893, test=0.815) total time=   1.9s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.892, test=0.786) total time=   1.9s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.888, test=0.823) total time=   1.9s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.894, test=0.783) total time=   3.7s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.890, test=0.789) total time=   3.7s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.893, test=0.810) total time=   3.7s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.893, test=0.778) total time=   3.7s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.891, test=0.820) total time=   3.7s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.893, test=0.780) total time=   5.6s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.889, test=0.787) total time=   5.5s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.893, test=0.807) total time=   5.5s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.893, test=0.785) total time=   5.5s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.889, test=0.818) total time=   5.8s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.887, test=0.777) total time=   1.9s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.881, test=0.791) total time=   1.8s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.888, test=0.810) total time=   1.9s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.886, test=0.787) total time=   1.8s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.875, test=0.804) total time=   1.9s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.887, test=0.773) total time=   3.7s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.882, test=0.795) total time=   3.7s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.890, test=0.814) total time=   3.7s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.888, test=0.794) total time=   3.7s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.878, test=0.813) total time=   3.7s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.888, test=0.773) total time=   5.6s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.883, test=0.794) total time=   5.5s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.889, test=0.808) total time=   5.6s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.887, test=0.791) total time=   5.5s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.878, test=0.808) total time=   5.5s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.979, test=0.812) total time=   2.7s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.977, test=0.823) total time=   2.6s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.979, test=0.834) total time=   2.6s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.975, test=0.834) total time=   2.6s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.979, test=0.849) total time=   2.7s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.979, test=0.811) total time=   5.2s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.977, test=0.832) total time=   5.2s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.979, test=0.838) total time=   5.2s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.977, test=0.826) total time=   5.2s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.978, test=0.847) total time=   5.2s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.978, test=0.809) total time=   8.0s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.976, test=0.829) total time=   7.8s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.979, test=0.836) total time=   7.8s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.977, test=0.821) total time=   7.8s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.978, test=0.847) total time=   7.8s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.973, test=0.806) total time=   2.6s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.974, test=0.823) total time=   2.6s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.975, test=0.838) total time=   2.5s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.974, test=0.823) total time=   2.5s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.974, test=0.846) total time=   2.6s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.974, test=0.814) total time=   5.1s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.975, test=0.823) total time=   5.1s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.975, test=0.834) total time=   5.2s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.974, test=0.822) total time=   5.0s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.973, test=0.851) total time=   5.1s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.973, test=0.808) total time=   7.6s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.975, test=0.820) total time=   7.7s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.975, test=0.835) total time=   7.6s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.974, test=0.818) total time=   7.8s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.973, test=0.848) total time=   7.6s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.963, test=0.808) total time=   2.6s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.962, test=0.818) total time=   2.6s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.964, test=0.824) total time=   2.5s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.963, test=0.828) total time=   2.5s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.961, test=0.838) total time=   2.6s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.963, test=0.808) total time=   5.1s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.961, test=0.823) total time=   5.1s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.965, test=0.831) total time=   5.0s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.963, test=0.822) total time=   5.0s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.963, test=0.838) total time=   5.0s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.964, test=0.811) total time=   7.5s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.962, test=0.825) total time=   7.6s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.965, test=0.826) total time=   7.5s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.963, test=0.823) total time=   7.5s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.964, test=0.843) total time=   7.6s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.982, test=0.806) total time=   2.8s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.980, test=0.822) total time=   2.8s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.981, test=0.835) total time=   2.8s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.981, test=0.829) total time=   2.7s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.981, test=0.847) total time=   2.8s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.982, test=0.811) total time=   5.5s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.980, test=0.831) total time=   5.5s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.981, test=0.839) total time=   5.5s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.981, test=0.825) total time=   5.4s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.981, test=0.850) total time=   5.5s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.982, test=0.808) total time=   8.2s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.980, test=0.837) total time=   8.3s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.981, test=0.838) total time=   8.1s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.981, test=0.822) total time=   8.1s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.981, test=0.845) total time=   8.4s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.981, test=0.806) total time=   2.8s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.834) total time=   2.7s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.834) total time=   2.7s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.831) total time=   2.6s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.980, test=0.839) total time=   2.7s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.981, test=0.806) total time=   5.4s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.980, test=0.837) total time=   5.5s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.981, test=0.836) total time=   5.4s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.980, test=0.828) total time=   5.3s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.980, test=0.847) total time=   5.4s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.981, test=0.810) total time=   7.9s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.980, test=0.833) total time=   8.1s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.981, test=0.839) total time=   7.9s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.981, test=0.824) total time=   7.8s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.980, test=0.846) total time=   8.0s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.971, test=0.811) total time=   2.7s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.970, test=0.823) total time=   2.6s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.974, test=0.835) total time=   2.6s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.971, test=0.831) total time=   2.6s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.970, test=0.849) total time=   2.6s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.971, test=0.815) total time=   5.2s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.972, test=0.821) total time=   5.3s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.974, test=0.829) total time=   5.5s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.970, test=0.826) total time=   5.2s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.972, test=0.850) total time=   5.4s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.972, test=0.810) total time=   7.8s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.972, test=0.825) total time=   7.9s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.973, test=0.834) total time=   7.7s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.971, test=0.826) total time=   7.8s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.972, test=0.849) total time=   7.9s\n",
            "Detailed results for each hyperparameter combination:\n",
            "    param_n_estimators param_max_depth  param_min_samples_split  \\\n",
            "1                  200            None                        2   \n",
            "4                  200            None                        5   \n",
            "28                 200              30                        2   \n",
            "31                 200              30                        5   \n",
            "32                 300              30                        5   \n",
            "19                 200              20                        2   \n",
            "18                 100              20                        2   \n",
            "3                  100            None                        5   \n",
            "29                 300              30                        2   \n",
            "33                 100              30                       10   \n",
            "0                  100            None                        2   \n",
            "2                  300            None                        2   \n",
            "5                  300            None                        5   \n",
            "30                 100              30                        5   \n",
            "22                 200              20                        5   \n",
            "35                 300              30                       10   \n",
            "6                  100            None                       10   \n",
            "20                 300              20                        2   \n",
            "34                 200              30                       10   \n",
            "27                 100              30                        2   \n",
            "8                  300            None                       10   \n",
            "21                 100              20                        5   \n",
            "23                 300              20                        5   \n",
            "26                 300              20                       10   \n",
            "7                  200            None                       10   \n",
            "25                 200              20                       10   \n",
            "24                 100              20                       10   \n",
            "10                 200              10                        2   \n",
            "11                 300              10                        2   \n",
            "12                 100              10                        5   \n",
            "9                  100              10                        2   \n",
            "16                 200              10                       10   \n",
            "13                 200              10                        5   \n",
            "14                 300              10                        5   \n",
            "17                 300              10                       10   \n",
            "15                 100              10                       10   \n",
            "\n",
            "    mean_test_score  std_test_score  mean_train_score  std_train_score  \n",
            "1          0.832747        0.013380          0.980963         0.000508  \n",
            "4          0.832075        0.012849          0.980538         0.000385  \n",
            "28         0.831039        0.013089          0.980954         0.000507  \n",
            "31         0.830697        0.013711          0.980536         0.000384  \n",
            "32         0.830513        0.012394          0.980631         0.000307  \n",
            "19         0.830510        0.012114          0.977946         0.000772  \n",
            "18         0.830151        0.012312          0.977810         0.001438  \n",
            "3          0.830057        0.011249          0.980048         0.000466  \n",
            "29         0.829862        0.013364          0.980963         0.000503  \n",
            "33         0.829798        0.012519          0.971033         0.001338  \n",
            "0          0.829691        0.011900          0.980967         0.000502  \n",
            "2          0.829465        0.014016          0.980963         0.000514  \n",
            "5          0.829070        0.011336          0.980633         0.000309  \n",
            "30         0.829015        0.011981          0.980103         0.000370  \n",
            "22         0.828867        0.012811          0.973928         0.000928  \n",
            "35         0.828850        0.012637          0.971830         0.000649  \n",
            "6          0.828587        0.010780          0.971246         0.001330  \n",
            "20         0.828414        0.012654          0.977714         0.000939  \n",
            "34         0.828059        0.011793          0.971655         0.001210  \n",
            "27         0.827756        0.013790          0.980967         0.000501  \n",
            "8          0.827252        0.011281          0.972156         0.000851  \n",
            "21         0.826977        0.013796          0.974104         0.000653  \n",
            "23         0.825793        0.014221          0.973860         0.000672  \n",
            "26         0.825616        0.010404          0.963461         0.001041  \n",
            "7          0.825523        0.009806          0.971534         0.000979  \n",
            "25         0.824288        0.010120          0.963020         0.001445  \n",
            "24         0.823323        0.010038          0.962449         0.001086  \n",
            "10         0.800929        0.014188          0.895506         0.002545  \n",
            "11         0.800552        0.012488          0.896203         0.002217  \n",
            "12         0.799646        0.017186          0.890655         0.002039  \n",
            "9          0.798703        0.013354          0.893617         0.001963  \n",
            "16         0.797825        0.014920          0.884772         0.004471  \n",
            "13         0.795755        0.016238          0.892135         0.001665  \n",
            "14         0.795367        0.014457          0.891593         0.001935  \n",
            "17         0.794716        0.012934          0.885025         0.003925  \n",
            "15         0.793802        0.012088          0.883413         0.004954  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Regression**"
      ],
      "metadata": {
        "id": "YiOqj7P1wlYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-1. Machine learning models**"
      ],
      "metadata": {
        "id": "-U03F-g_we1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ": Random Forest, XGBoost, PLSRgression"
      ],
      "metadata": {
        "id": "EEjWEomUwIaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load regression dataset and drop rows with missing SMILES or target values\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])\n",
        "\n",
        "# Function to calculate molecular descriptors\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Prepare descriptor data\n",
        "descriptor_data = []\n",
        "for smiles in regression_data['SMILES']:\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        descriptors = getMolDescriptors(mol)\n",
        "        descriptor_data.append(descriptors)\n",
        "    else:\n",
        "        print(f\"Invalid SMILES string: {smiles}\")\n",
        "        descriptor_data.append({name: None for name, _ in Descriptors._descList})\n",
        "\n",
        "# Convert descriptor data to DataFrame and combine with target variable\n",
        "descriptor_df = pd.DataFrame(descriptor_data)\n",
        "regression_df = pd.concat([regression_data[['logBB']].reset_index(drop=True), descriptor_df], axis=1).dropna()\n",
        "X = descriptor_df  # Use only descriptor columns as features\n",
        "y = regression_data['logBB']  # Target variable for regression\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(f\"Summary of target variable 'logBB': \\n{y.describe()}\\n\")\n",
        "\n",
        "\n",
        "# Print summary statistics for the imputed features\n",
        "print(f\"Summary of descriptors:\\n{X.describe()}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZv_y60kwkn0",
        "outputId": "39e3a8c3-b6d2-4fe8-c463-4e995d2de6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of target variable 'logBB': \n",
            "count    1058.000000\n",
            "mean       -0.077873\n",
            "std         0.751623\n",
            "min        -2.690000\n",
            "25%        -0.535000\n",
            "50%        -0.020000\n",
            "75%         0.420000\n",
            "max         1.700000\n",
            "Name: logBB, dtype: float64\n",
            "\n",
            "Summary of descriptors:\n",
            "       MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex  \\\n",
            "count        1058.000000     1058.000000        1058.000000     1058.000000   \n",
            "mean           10.186494       10.185903           0.317928       -0.672585   \n",
            "std             3.546924        3.546929           0.528992        1.462202   \n",
            "min             0.000000        0.000000           0.000000      -10.472222   \n",
            "25%             8.198082        8.198082           0.062509       -0.995625   \n",
            "50%            11.576130       11.576130           0.159553       -0.383909   \n",
            "75%            12.710785       12.710785           0.365237        0.138878   \n",
            "max            16.859063       16.859063           9.847222        6.000000   \n",
            "\n",
            "               qed          SPS        MolWt  HeavyAtomMolWt   ExactMolWt  \\\n",
            "count  1058.000000  1058.000000  1058.000000     1058.000000  1058.000000   \n",
            "mean      0.603109    18.097201   320.529852      299.614328   320.148174   \n",
            "std       0.192676     8.835858   160.526804      149.766782   160.390491   \n",
            "min       0.042511     0.000000     6.941000        6.941000     7.016005   \n",
            "25%       0.469015    11.931034   230.098000      216.152000   229.017353   \n",
            "50%       0.629466    15.900000   309.359500      288.221000   309.149087   \n",
            "75%       0.758954    21.545313   392.448000      368.011000   392.165426   \n",
            "max       0.939490    55.545455  1802.733000     1699.917000  1800.629293   \n",
            "\n",
            "       NumValenceElectrons  ...   fr_sulfide  fr_sulfonamd   fr_sulfone  \\\n",
            "count          1058.000000  ...  1058.000000   1058.000000  1058.000000   \n",
            "mean            119.568998  ...     0.055766      0.032136     0.009452   \n",
            "std              61.869101  ...     0.274611      0.176445     0.096806   \n",
            "min               1.000000  ...     0.000000      0.000000     0.000000   \n",
            "25%              84.000000  ...     0.000000      0.000000     0.000000   \n",
            "50%             116.000000  ...     0.000000      0.000000     0.000000   \n",
            "75%             146.000000  ...     0.000000      0.000000     0.000000   \n",
            "max             680.000000  ...     4.000000      1.000000     1.000000   \n",
            "\n",
            "       fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  \\\n",
            "count        1058.000000   1058.000000  1058.000000       1058.0   \n",
            "mean            0.004726      0.007561     0.029301          0.0   \n",
            "std             0.068615      0.086668     0.179592          0.0   \n",
            "min             0.000000      0.000000     0.000000          0.0   \n",
            "25%             0.000000      0.000000     0.000000          0.0   \n",
            "50%             0.000000      0.000000     0.000000          0.0   \n",
            "75%             0.000000      0.000000     0.000000          0.0   \n",
            "max             1.000000      1.000000     2.000000          0.0   \n",
            "\n",
            "       fr_thiophene  fr_unbrch_alkane      fr_urea  \n",
            "count   1058.000000       1058.000000  1058.000000  \n",
            "mean       0.018904          0.144612     0.041588  \n",
            "std        0.136249          0.865000     0.199740  \n",
            "min        0.000000          0.000000     0.000000  \n",
            "25%        0.000000          0.000000     0.000000  \n",
            "50%        0.000000          0.000000     0.000000  \n",
            "75%        0.000000          0.000000     0.000000  \n",
            "max        1.000000         15.000000     1.000000  \n",
            "\n",
            "[8 rows x 210 columns]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(y, kde=True, bins=30, color='skyblue')\n",
        "plt.title(\"Distribution of 'logBB' Target Variable\")\n",
        "plt.xlabel(\"logBB\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "O38eL7g0yZYn",
        "outputId": "e2c5a563-8adc-41fa-c56b-ff834537db27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize RandomForestRegressor with default parameters\n",
        "regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance with MSE and R² score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RandomForestRegressor Performance :\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldzg_BlCyMKS",
        "outputId": "4bb26f09-c977-4f90-9445-5e62590d4a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor Performance :\n",
            "Mean Squared Error: 0.27\n",
            "R² Score: 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize RandomForestRegressor\n",
        "regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "# Track progress during hyperparameter tuning with GridSearchCV\n",
        "grid_search = GridSearchCV(regressor, param_grid, cv=3, scoring='r2', verbose=3, return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display results of each hyperparameter combination\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "display_columns = [\n",
        "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
        "    'mean_test_score', 'std_test_score', 'mean_train_score', 'std_train_score'\n",
        "]\n",
        "results_df = results_df[display_columns].sort_values(by='mean_test_score', ascending=False)\n",
        "print(\"Detailed results for each hyperparameter combination:\")\n",
        "print(results_df)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"\\nTest Set Evaluation:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHIkvD5owp3K",
        "outputId": "b54d91e7-3873-44cf-b66a-ca6a7e424401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.938, test=0.523) total time=   7.1s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.935, test=0.678) total time=   8.2s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.942, test=0.556) total time=   4.2s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.940, test=0.590) total time=   7.3s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=(train=0.939, test=0.637) total time=   4.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.941, test=0.526) total time=   9.2s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.937, test=0.672) total time=   8.5s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.944, test=0.557) total time=  10.5s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.940, test=0.599) total time=   9.5s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=(train=0.942, test=0.639) total time=   8.6s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.942, test=0.526) total time=  14.0s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.938, test=0.669) total time=  14.0s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.944, test=0.560) total time=  14.7s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.941, test=0.600) total time=  13.6s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=300;, score=(train=0.941, test=0.642) total time=  13.6s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.526) total time=   3.8s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.922, test=0.671) total time=   4.8s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.929, test=0.550) total time=   4.3s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.583) total time=   4.1s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.634) total time=   4.6s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.929, test=0.525) total time=   9.4s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.924, test=0.666) total time=   7.4s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.931, test=0.555) total time=   8.3s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.927, test=0.593) total time=   7.6s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=(train=0.929, test=0.638) total time=   8.0s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.930, test=0.525) total time=  11.7s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.925, test=0.666) total time=  12.2s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.932, test=0.559) total time=  11.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.928, test=0.596) total time=  11.3s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=300;, score=(train=0.929, test=0.640) total time=  11.5s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.893, test=0.516) total time=   4.1s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.890, test=0.658) total time=   3.3s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.898, test=0.555) total time=   3.2s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.897, test=0.583) total time=   3.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=(train=0.893, test=0.632) total time=   4.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.897, test=0.515) total time=   6.5s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.893, test=0.656) total time=   8.3s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.900, test=0.554) total time=   6.3s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.897, test=0.590) total time=   7.5s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=(train=0.895, test=0.631) total time=   6.4s\n",
            "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.898, test=0.513) total time=  10.4s\n",
            "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.893, test=0.655) total time=  10.7s\n",
            "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.900, test=0.555) total time=  10.1s\n",
            "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.899, test=0.592) total time=  10.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=300;, score=(train=0.896, test=0.632) total time=  10.6s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.927, test=0.522) total time=   3.5s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.923, test=0.665) total time=   4.5s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.930, test=0.552) total time=   3.6s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.928, test=0.586) total time=   3.6s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=(train=0.927, test=0.638) total time=   3.8s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.930, test=0.524) total time=   7.8s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.925, test=0.658) total time=   8.0s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.933, test=0.554) total time=   8.4s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.928, test=0.594) total time=   8.2s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=(train=0.929, test=0.638) total time=   7.1s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.931, test=0.522) total time=  11.6s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.926, test=0.660) total time=  11.8s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.933, test=0.557) total time=  11.8s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.929, test=0.595) total time=  11.8s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=300;, score=(train=0.930, test=0.638) total time=  11.5s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.916, test=0.521) total time=   3.3s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.912, test=0.660) total time=   3.3s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.919, test=0.552) total time=   3.3s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.917, test=0.587) total time=   4.3s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=(train=0.915, test=0.635) total time=   3.3s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.919, test=0.522) total time=   6.5s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.914, test=0.657) total time=   7.5s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.922, test=0.555) total time=   6.8s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.917, test=0.591) total time=   8.6s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=(train=0.918, test=0.636) total time=   7.5s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.920, test=0.520) total time=   9.9s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.915, test=0.659) total time=  11.0s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.922, test=0.558) total time=  10.8s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.919, test=0.594) total time=  10.9s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=300;, score=(train=0.918, test=0.636) total time=  10.3s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.885, test=0.516) total time=   3.6s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.882, test=0.654) total time=   3.1s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.889, test=0.551) total time=   3.1s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.889, test=0.579) total time=   3.2s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=(train=0.885, test=0.627) total time=   3.8s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.889, test=0.515) total time=   6.0s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.884, test=0.651) total time=   7.1s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.892, test=0.551) total time=   6.0s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.889, test=0.586) total time=   7.0s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=(train=0.887, test=0.627) total time=   6.0s\n",
            "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.891, test=0.513) total time=  11.2s\n",
            "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.885, test=0.651) total time=   9.2s\n",
            "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.892, test=0.553) total time=   9.8s\n",
            "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.891, test=0.589) total time=  10.1s\n",
            "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=300;, score=(train=0.887, test=0.629) total time=   9.8s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.939, test=0.523) total time=   4.2s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.935, test=0.676) total time=   4.2s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.942, test=0.555) total time=   5.2s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.940, test=0.591) total time=   4.4s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=(train=0.939, test=0.638) total time=   4.2s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.941, test=0.527) total time=   9.3s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.937, test=0.671) total time=   9.5s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.944, test=0.557) total time=   8.4s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.940, test=0.597) total time=   9.5s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=(train=0.941, test=0.641) total time=   9.4s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.942, test=0.526) total time=  14.8s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.938, test=0.669) total time=  13.7s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.944, test=0.560) total time=  13.6s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.941, test=0.599) total time=  13.6s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=300;, score=(train=0.941, test=0.643) total time=  13.7s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.526) total time=   3.6s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.922, test=0.671) total time=   3.7s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.929, test=0.551) total time=   4.5s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.582) total time=   3.9s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.633) total time=   3.6s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.929, test=0.524) total time=   8.3s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.924, test=0.667) total time=   7.4s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.931, test=0.555) total time=   8.3s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.927, test=0.593) total time=   7.7s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=(train=0.929, test=0.638) total time=   9.2s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.930, test=0.525) total time=  11.8s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.924, test=0.666) total time=  12.1s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.932, test=0.559) total time=  11.9s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.928, test=0.596) total time=  11.8s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=300;, score=(train=0.929, test=0.640) total time=  11.3s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.893, test=0.516) total time=   3.9s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.890, test=0.658) total time=   3.3s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.898, test=0.555) total time=   3.2s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.897, test=0.582) total time=   3.6s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=(train=0.893, test=0.632) total time=   3.8s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.897, test=0.515) total time=   6.2s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.893, test=0.656) total time=   7.5s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.900, test=0.554) total time=   6.4s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.897, test=0.590) total time=   7.4s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=(train=0.895, test=0.631) total time=   6.3s\n",
            "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.898, test=0.513) total time=  11.5s\n",
            "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.893, test=0.655) total time=  10.7s\n",
            "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.900, test=0.555) total time=  10.5s\n",
            "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.899, test=0.592) total time=   9.6s\n",
            "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=300;, score=(train=0.896, test=0.632) total time=  10.5s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.938, test=0.523) total time=   4.3s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.935, test=0.678) total time=   5.1s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.942, test=0.556) total time=   4.2s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.940, test=0.590) total time=   4.5s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=(train=0.939, test=0.637) total time=   5.1s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.941, test=0.526) total time=   8.5s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.937, test=0.672) total time=   9.4s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.944, test=0.557) total time=   9.5s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.940, test=0.599) total time=   8.6s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=(train=0.942, test=0.639) total time=   9.2s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.942, test=0.526) total time=  14.9s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.938, test=0.669) total time=  13.9s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.944, test=0.560) total time=  13.6s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.941, test=0.600) total time=  13.6s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=300;, score=(train=0.941, test=0.642) total time=  13.6s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.526) total time=   3.6s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.922, test=0.671) total time=   4.7s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.929, test=0.550) total time=   3.6s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.583) total time=   3.6s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=(train=0.926, test=0.634) total time=   4.0s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.929, test=0.525) total time=   7.9s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.924, test=0.666) total time=   8.4s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.931, test=0.555) total time=   7.2s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.927, test=0.593) total time=   8.3s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=(train=0.929, test=0.638) total time=   8.6s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.930, test=0.525) total time=  11.5s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.925, test=0.666) total time=  12.1s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.932, test=0.559) total time=  11.9s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.928, test=0.596) total time=  12.0s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=300;, score=(train=0.929, test=0.640) total time=  12.0s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.893, test=0.516) total time=   3.1s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.890, test=0.658) total time=   3.3s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.898, test=0.555) total time=   4.0s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.897, test=0.583) total time=   3.5s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=(train=0.893, test=0.632) total time=   3.2s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.897, test=0.515) total time=   7.1s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.893, test=0.656) total time=   6.8s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.900, test=0.554) total time=   7.4s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.897, test=0.590) total time=   6.5s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=(train=0.895, test=0.631) total time=   7.3s\n",
            "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.898, test=0.513) total time=  11.2s\n",
            "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.893, test=0.655) total time=  10.3s\n",
            "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.900, test=0.555) total time=  10.7s\n",
            "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.899, test=0.592) total time=  10.7s\n",
            "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=300;, score=(train=0.896, test=0.632) total time=  10.4s\n",
            "Detailed results for each hyperparameter combination:\n",
            "    param_n_estimators param_max_depth  param_min_samples_split  \\\n",
            "20                 300              20                        2   \n",
            "2                  300            None                        2   \n",
            "29                 300              30                        2   \n",
            "1                  200            None                        2   \n",
            "28                 200              30                        2   \n",
            "19                 200              20                        2   \n",
            "32                 300              30                        5   \n",
            "5                  300            None                        5   \n",
            "23                 300              20                        5   \n",
            "18                 100              20                        2   \n",
            "27                 100              30                        2   \n",
            "0                  100            None                        2   \n",
            "4                  200            None                        5   \n",
            "31                 200              30                        5   \n",
            "22                 200              20                        5   \n",
            "11                 300              10                        2   \n",
            "10                 200              10                        2   \n",
            "14                 300              10                        5   \n",
            "3                  100            None                        5   \n",
            "30                 100              30                        5   \n",
            "21                 100              20                        5   \n",
            "9                  100              10                        2   \n",
            "13                 200              10                        5   \n",
            "12                 100              10                        5   \n",
            "35                 300              30                       10   \n",
            "8                  300            None                       10   \n",
            "26                 300              20                       10   \n",
            "7                  200            None                       10   \n",
            "34                 200              30                       10   \n",
            "25                 200              20                       10   \n",
            "33                 100              30                       10   \n",
            "6                  100            None                       10   \n",
            "24                 100              20                       10   \n",
            "17                 300              10                       10   \n",
            "16                 200              10                       10   \n",
            "15                 100              10                       10   \n",
            "\n",
            "    mean_test_score  std_test_score  mean_train_score  std_train_score  \n",
            "20         0.599506        0.052172          0.941250         0.002122  \n",
            "2          0.599417        0.052096          0.941257         0.002106  \n",
            "29         0.599417        0.052096          0.941257         0.002106  \n",
            "1          0.598590        0.052969          0.940870         0.002339  \n",
            "28         0.598590        0.052969          0.940870         0.002339  \n",
            "19         0.598543        0.052772          0.940894         0.002341  \n",
            "32         0.597343        0.051393          0.928473         0.002326  \n",
            "5          0.597343        0.051393          0.928473         0.002326  \n",
            "23         0.597327        0.051361          0.928492         0.002382  \n",
            "18         0.596671        0.055329          0.938815         0.002280  \n",
            "27         0.596653        0.055613          0.938873         0.002168  \n",
            "0          0.596653        0.055613          0.938873         0.002168  \n",
            "4          0.595359        0.052009          0.927978         0.002488  \n",
            "31         0.595359        0.052009          0.927978         0.002488  \n",
            "22         0.595301        0.052109          0.927946         0.002586  \n",
            "11         0.594486        0.050919          0.929878         0.002250  \n",
            "10         0.593542        0.050038          0.929079         0.002564  \n",
            "14         0.593197        0.050612          0.918754         0.002527  \n",
            "3          0.592884        0.053201          0.925806         0.002264  \n",
            "30         0.592884        0.053201          0.925806         0.002264  \n",
            "21         0.592725        0.052860          0.925746         0.002326  \n",
            "9          0.592499        0.052801          0.927001         0.002484  \n",
            "13         0.592320        0.050109          0.918032         0.002729  \n",
            "12         0.591267        0.051141          0.915930         0.002427  \n",
            "35         0.589594        0.051280          0.897274         0.002577  \n",
            "8          0.589594        0.051280          0.897274         0.002577  \n",
            "26         0.589501        0.051255          0.897277         0.002579  \n",
            "7          0.589371        0.051163          0.896424         0.002423  \n",
            "34         0.589371        0.051163          0.896424         0.002423  \n",
            "25         0.589242        0.051140          0.896435         0.002419  \n",
            "33         0.588501        0.051228          0.894098         0.002727  \n",
            "6          0.588501        0.051228          0.894098         0.002727  \n",
            "24         0.588388        0.051417          0.894087         0.002695  \n",
            "17         0.586761        0.050009          0.889089         0.002874  \n",
            "16         0.585950        0.049258          0.888113         0.002571  \n",
            "15         0.585219        0.050148          0.886112         0.002656  \n",
            "\n",
            "Test Set Evaluation:\n",
            "Mean Squared Error: 0.27\n",
            "R² Score: 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize RandomForestRegressor\n",
        "regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],          # Lowered number of trees\n",
        "    'max_depth': [5, 10, 15],                # Reduced depth choices\n",
        "    'min_samples_split': [2, 5, 10],         # Existing parameter\n",
        "    'min_samples_leaf': [5, 10, 20],         # New regularization parameter\n",
        "}\n",
        "\n",
        "\n",
        "# Track progress during hyperparameter tuning with GridSearchCV\n",
        "grid_search = GridSearchCV(regressor, param_grid, cv=3, scoring='r2', verbose=3, return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display results of each hyperparameter combination\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "display_columns = [\n",
        "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
        "    'mean_test_score', 'std_test_score', 'mean_train_score', 'std_train_score'\n",
        "]\n",
        "results_df = results_df[display_columns].sort_values(by='mean_test_score', ascending=False)\n",
        "print(\"Detailed results for each hyperparameter combination:\")\n",
        "print(results_df)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"\\nTest Set Evaluation:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anvTBNvu4_uu",
        "outputId": "b55c9137-0626-4192-af5d-afc3e25b2252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.747, test=0.492) total time=   0.8s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.754, test=0.504) total time=   0.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.726, test=0.600) total time=   0.8s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.751, test=0.490) total time=   1.6s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.761, test=0.514) total time=   1.9s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.728, test=0.600) total time=   2.2s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.752, test=0.493) total time=   2.5s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.761, test=0.513) total time=   2.5s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.730, test=0.600) total time=   3.5s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.747, test=0.492) total time=   0.8s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.754, test=0.504) total time=   0.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.726, test=0.600) total time=   1.0s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.751, test=0.490) total time=   2.2s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.761, test=0.514) total time=   1.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.728, test=0.600) total time=   1.6s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.752, test=0.493) total time=   2.4s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.761, test=0.513) total time=   2.4s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.730, test=0.600) total time=   2.5s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.747, test=0.492) total time=   1.1s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.754, test=0.504) total time=   1.2s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.726, test=0.600) total time=   1.0s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.751, test=0.490) total time=   1.6s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.761, test=0.514) total time=   1.6s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.728, test=0.600) total time=   1.7s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.752, test=0.493) total time=   2.3s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.761, test=0.513) total time=   2.5s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.730, test=0.600) total time=   3.3s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.712, test=0.469) total time=   0.8s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.717, test=0.494) total time=   0.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.691, test=0.588) total time=   0.8s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.716, test=0.480) total time=   1.5s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.724, test=0.498) total time=   1.5s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.693, test=0.588) total time=   1.5s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.718, test=0.484) total time=   2.2s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.725, test=0.500) total time=   2.6s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.694, test=0.591) total time=   2.9s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.712, test=0.469) total time=   0.7s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.717, test=0.494) total time=   0.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.691, test=0.588) total time=   0.7s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.716, test=0.480) total time=   1.4s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.724, test=0.498) total time=   1.5s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.693, test=0.588) total time=   1.5s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.718, test=0.484) total time=   2.2s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.725, test=0.500) total time=   2.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.694, test=0.591) total time=   2.7s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.712, test=0.469) total time=   0.7s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.717, test=0.494) total time=   0.7s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.691, test=0.588) total time=   0.8s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.716, test=0.480) total time=   1.5s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.724, test=0.498) total time=   1.5s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.693, test=0.588) total time=   1.5s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.718, test=0.484) total time=   2.2s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.725, test=0.500) total time=   3.1s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.694, test=0.591) total time=   2.4s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.640, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.642, test=0.467) total time=   0.6s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.611, test=0.569) total time=   0.6s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.641, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.646, test=0.472) total time=   1.2s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.617, test=0.575) total time=   1.2s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.643, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.648, test=0.477) total time=   2.3s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.616, test=0.576) total time=   2.4s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.640, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.642, test=0.467) total time=   0.6s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.611, test=0.569) total time=   0.6s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.641, test=0.457) total time=   1.3s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.646, test=0.472) total time=   1.2s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.617, test=0.575) total time=   1.3s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.643, test=0.460) total time=   1.9s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.648, test=0.477) total time=   1.9s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.616, test=0.576) total time=   2.5s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.640, test=0.457) total time=   0.9s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.642, test=0.467) total time=   0.8s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.611, test=0.569) total time=   0.6s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.641, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.646, test=0.472) total time=   1.2s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.617, test=0.575) total time=   1.2s\n",
            "[CV 1/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.643, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.648, test=0.477) total time=   1.9s\n",
            "[CV 3/3] END max_depth=5, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.616, test=0.576) total time=   1.9s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.848, test=0.528) total time=   1.5s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.852, test=0.531) total time=   1.5s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.843, test=0.634) total time=   1.2s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.849, test=0.526) total time=   2.2s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.857, test=0.545) total time=   3.0s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.848, test=0.637) total time=   2.3s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.849, test=0.527) total time=   3.7s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.857, test=0.547) total time=   3.7s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.849, test=0.638) total time=   3.3s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.848, test=0.528) total time=   1.0s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.852, test=0.531) total time=   1.1s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.843, test=0.634) total time=   1.1s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.849, test=0.526) total time=   2.3s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.857, test=0.545) total time=   2.9s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.848, test=0.637) total time=   2.2s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.849, test=0.527) total time=   3.1s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.857, test=0.547) total time=   3.3s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.849, test=0.638) total time=   3.9s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.848, test=0.528) total time=   1.2s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.852, test=0.531) total time=   1.1s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.843, test=0.634) total time=   1.1s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.849, test=0.526) total time=   2.1s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.857, test=0.545) total time=   2.1s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.848, test=0.637) total time=   2.1s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.849, test=0.527) total time=   4.1s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.857, test=0.547) total time=   3.3s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.849, test=0.638) total time=   3.3s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.754, test=0.488) total time=   0.8s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.759, test=0.505) total time=   0.8s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.742, test=0.602) total time=   0.8s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.754, test=0.497) total time=   1.9s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.765, test=0.511) total time=   2.2s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.745, test=0.603) total time=   1.7s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.755, test=0.500) total time=   2.5s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.766, test=0.513) total time=   2.5s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.744, test=0.605) total time=   2.5s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.754, test=0.488) total time=   0.9s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.759, test=0.505) total time=   1.2s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.742, test=0.602) total time=   1.2s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.754, test=0.497) total time=   2.0s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.765, test=0.511) total time=   1.6s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.745, test=0.603) total time=   1.7s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.755, test=0.500) total time=   2.4s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.766, test=0.513) total time=   2.5s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.744, test=0.605) total time=   3.2s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.754, test=0.488) total time=   1.0s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.759, test=0.505) total time=   0.8s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.742, test=0.602) total time=   0.8s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.754, test=0.497) total time=   1.7s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.765, test=0.511) total time=   1.6s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.745, test=0.603) total time=   1.7s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.755, test=0.500) total time=   2.4s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.766, test=0.513) total time=   3.3s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.744, test=0.605) total time=   2.7s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.642, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.646, test=0.468) total time=   0.6s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.615, test=0.572) total time=   0.6s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.643, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.650, test=0.473) total time=   1.2s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.621, test=0.577) total time=   1.2s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.645, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.651, test=0.478) total time=   2.3s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.620, test=0.578) total time=   2.4s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.642, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.646, test=0.468) total time=   0.6s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.615, test=0.572) total time=   0.6s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.643, test=0.457) total time=   1.3s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.650, test=0.473) total time=   1.5s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.621, test=0.577) total time=   1.8s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.645, test=0.460) total time=   2.1s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.651, test=0.478) total time=   2.3s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.620, test=0.578) total time=   2.5s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.642, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.646, test=0.468) total time=   0.6s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.615, test=0.572) total time=   0.6s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.643, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.650, test=0.473) total time=   1.2s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.621, test=0.577) total time=   1.3s\n",
            "[CV 1/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.645, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.651, test=0.478) total time=   1.9s\n",
            "[CV 3/3] END max_depth=10, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.620, test=0.578) total time=   2.5s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.849, test=0.530) total time=   1.5s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.853, test=0.528) total time=   1.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=50;, score=(train=0.844, test=0.634) total time=   1.1s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.849, test=0.527) total time=   2.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.858, test=0.544) total time=   2.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=100;, score=(train=0.849, test=0.637) total time=   2.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.850, test=0.528) total time=   3.9s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.859, test=0.546) total time=   3.5s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=2, n_estimators=150;, score=(train=0.850, test=0.638) total time=   3.3s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.849, test=0.530) total time=   1.0s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.853, test=0.528) total time=   1.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=50;, score=(train=0.844, test=0.634) total time=   1.1s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.849, test=0.527) total time=   2.5s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.858, test=0.544) total time=   2.7s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=100;, score=(train=0.849, test=0.637) total time=   2.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.850, test=0.528) total time=   3.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.859, test=0.546) total time=   3.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=5, n_estimators=150;, score=(train=0.850, test=0.638) total time=   4.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.849, test=0.530) total time=   1.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.853, test=0.528) total time=   1.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=50;, score=(train=0.844, test=0.634) total time=   1.1s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.849, test=0.527) total time=   2.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.858, test=0.544) total time=   2.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=100;, score=(train=0.849, test=0.637) total time=   2.1s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.850, test=0.528) total time=   4.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.859, test=0.546) total time=   3.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=5, min_samples_split=10, n_estimators=150;, score=(train=0.850, test=0.638) total time=   3.3s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.754, test=0.488) total time=   0.8s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.759, test=0.505) total time=   0.8s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=50;, score=(train=0.742, test=0.602) total time=   0.8s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.754, test=0.497) total time=   1.8s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.765, test=0.511) total time=   2.3s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=100;, score=(train=0.745, test=0.603) total time=   1.7s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.755, test=0.500) total time=   2.4s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.766, test=0.513) total time=   2.5s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=2, n_estimators=150;, score=(train=0.745, test=0.605) total time=   2.5s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.754, test=0.488) total time=   0.8s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.759, test=0.505) total time=   1.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=50;, score=(train=0.742, test=0.602) total time=   1.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.754, test=0.497) total time=   2.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.765, test=0.511) total time=   1.6s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=100;, score=(train=0.745, test=0.603) total time=   1.7s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.755, test=0.500) total time=   2.4s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.766, test=0.513) total time=   2.5s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=5, n_estimators=150;, score=(train=0.745, test=0.605) total time=   2.9s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.754, test=0.488) total time=   1.1s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.759, test=0.505) total time=   1.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=50;, score=(train=0.742, test=0.602) total time=   0.8s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.754, test=0.497) total time=   1.6s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.765, test=0.511) total time=   2.1s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=100;, score=(train=0.745, test=0.603) total time=   2.3s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.755, test=0.500) total time=   2.4s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.766, test=0.513) total time=   3.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=10, min_samples_split=10, n_estimators=150;, score=(train=0.745, test=0.605) total time=   2.8s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.642, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.646, test=0.468) total time=   0.6s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=50;, score=(train=0.615, test=0.572) total time=   0.6s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.643, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.650, test=0.473) total time=   1.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=100;, score=(train=0.621, test=0.577) total time=   1.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.645, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.651, test=0.478) total time=   2.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=2, n_estimators=150;, score=(train=0.620, test=0.578) total time=   2.5s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.642, test=0.457) total time=   0.6s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.646, test=0.468) total time=   0.6s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=50;, score=(train=0.615, test=0.572) total time=   0.6s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.643, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.650, test=0.473) total time=   1.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=100;, score=(train=0.621, test=0.577) total time=   1.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.645, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.651, test=0.478) total time=   1.9s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=5, n_estimators=150;, score=(train=0.620, test=0.578) total time=   2.4s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.642, test=0.457) total time=   0.9s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.646, test=0.468) total time=   0.9s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=50;, score=(train=0.615, test=0.572) total time=   0.6s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.643, test=0.457) total time=   1.2s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.650, test=0.473) total time=   1.2s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=100;, score=(train=0.621, test=0.577) total time=   1.2s\n",
            "[CV 1/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.645, test=0.460) total time=   1.8s\n",
            "[CV 2/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.651, test=0.478) total time=   1.9s\n",
            "[CV 3/3] END max_depth=15, min_samples_leaf=20, min_samples_split=10, n_estimators=150;, score=(train=0.620, test=0.578) total time=   1.9s\n",
            "Detailed results for each hyperparameter combination:\n",
            "    param_n_estimators  param_max_depth  param_min_samples_split  \\\n",
            "59                 150               15                        5   \n",
            "56                 150               15                        2   \n",
            "62                 150               15                       10   \n",
            "35                 150               10                       10   \n",
            "32                 150               10                        5   \n",
            "..                 ...              ...                      ...   \n",
            "48                  50               10                        5   \n",
            "51                  50               10                       10   \n",
            "24                  50                5                       10   \n",
            "18                  50                5                        2   \n",
            "21                  50                5                        5   \n",
            "\n",
            "    mean_test_score  std_test_score  mean_train_score  std_train_score  \n",
            "59         0.570638        0.048300          0.852982         0.004016  \n",
            "56         0.570638        0.048300          0.852982         0.004016  \n",
            "62         0.570638        0.048300          0.852982         0.004016  \n",
            "35         0.570467        0.048130          0.852025         0.003835  \n",
            "32         0.570467        0.048130          0.852025         0.003835  \n",
            "..              ...             ...               ...              ...  \n",
            "48         0.498732        0.051703          0.634285         0.013862  \n",
            "51         0.498732        0.051703          0.634285         0.013862  \n",
            "24         0.497612        0.050951          0.630912         0.014032  \n",
            "18         0.497612        0.050951          0.630912         0.014032  \n",
            "21         0.497612        0.050951          0.630912         0.014032  \n",
            "\n",
            "[81 rows x 7 columns]\n",
            "\n",
            "Test Set Evaluation:\n",
            "Mean Squared Error: 0.28\n",
            "R² Score: 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],          # Number of boosting rounds\n",
        "    'learning_rate': [0.01, 0.05, 0.1],      # Step size shrinkage\n",
        "    'max_depth': [3, 5, 7],                  # Maximum depth of a tree\n",
        "    'min_child_weight': [1, 3, 5],           # Minimum sum of instance weight needed in a child\n",
        "    'subsample': [0.7, 0.8, 1.0],            # Fraction of samples to be used for each tree\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0],     # Fraction of features to be used for each tree\n",
        "    'gamma': [0, 0.1, 0.3],                  # Minimum loss reduction for a split\n",
        "    'reg_alpha': [0, 0.01, 0.1],             # L1 regularization term on weights\n",
        "    'reg_lambda': [1, 1.5, 2]                # L2 regularization term on weights\n",
        "}\n",
        "\n",
        "# Set up the XGBoost regressor and GridSearchCV\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='r2', verbose=3, return_train_score=True)\n",
        "\n",
        "# Fit GridSearchCV on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best R² score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "043B5PFL9ueH",
        "outputId": "01c96442-3077-4da3-8525-5d950c7ac7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 19683 candidates, totalling 59049 fits\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.338, test=0.256) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.334, test=0.262) total time=   0.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.308, test=0.293) total time=   1.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.338, test=0.257) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.312, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.337, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.266) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.308, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.337, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.332, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.293) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.335, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.264) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.309, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.334, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.305, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.304, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.333, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.331, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.307, test=0.289) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.334, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.290) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.338, test=0.256) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.334, test=0.262) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.308, test=0.294) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.338, test=0.257) total time=   1.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.265) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.312, test=0.292) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.337, test=0.254) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.266) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.308, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.337, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.332, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.335, test=0.256) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.309, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.334, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.306, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.304, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.333, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.331, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.307, test=0.289) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.333, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.338, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.333, test=0.261) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.307, test=0.293) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.337, test=0.257) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.335, test=0.265) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.311, test=0.291) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.336, test=0.253) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.265) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.307, test=0.292) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.336, test=0.254) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.331, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.293) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.333, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.308, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.332, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.305, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.303, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.332, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.330, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.306, test=0.288) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.333, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.500, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.499, test=0.384) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.468, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.502, test=0.368) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.504, test=0.390) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.471, test=0.432) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.500, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.501, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.467, test=0.426) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.497, test=0.366) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.496, test=0.380) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.464, test=0.430) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.499, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.500, test=0.388) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.467, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.499, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.464, test=0.426) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.495, test=0.362) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.378) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.461, test=0.429) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.497, test=0.386) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.463, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.365) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.497, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.462, test=0.426) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.500, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.499, test=0.383) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.468, test=0.432) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.501, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.503, test=0.390) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.471, test=0.432) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.500, test=0.367) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.501, test=0.378) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.467, test=0.427) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.497, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.463, test=0.430) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.499, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.500, test=0.388) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.467, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.498, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.499, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.426) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.494, test=0.362) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.378) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.460, test=0.429) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.385) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.464, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.462, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.499, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.498, test=0.381) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.466, test=0.430) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.500, test=0.367) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.502, test=0.389) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.469, test=0.431) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.499, test=0.366) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.500, test=0.380) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.467, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.496, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.494, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.463, test=0.429) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.497, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.499, test=0.386) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.466, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.365) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.493, test=0.361) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.491, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.459, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.495, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.386) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.463, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.495, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.376) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.461, test=0.426) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.653, test=0.463) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.657, test=0.480) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.629, test=0.542) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.653, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.662, test=0.483) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.634, test=0.546) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.652, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.660, test=0.472) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.627, test=0.537) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.649, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.653, test=0.476) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.623, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.651, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.658, test=0.482) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.628, test=0.545) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.649, test=0.457) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.656, test=0.474) total time=   1.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.624, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.646, test=0.459) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.650, test=0.474) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.620, test=0.540) total time=   1.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.647, test=0.460) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.654, test=0.481) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.625, test=0.543) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.646, test=0.455) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.654, test=0.470) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.621, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.653, test=0.463) total time=   1.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.657, test=0.479) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.629, test=0.542) total time=   1.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.653, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.661, test=0.483) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.635, test=0.546) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.651, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.660, test=0.472) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.627, test=0.537) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.649, test=0.463) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.653, test=0.476) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.624, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.650, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.658, test=0.481) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.628, test=0.545) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.649, test=0.457) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.657, test=0.472) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.623, test=0.536) total time=   1.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.646, test=0.459) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.650, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.620, test=0.540) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.647, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.654, test=0.480) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.625, test=0.543) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.646, test=0.457) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.654, test=0.470) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.620, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.652, test=0.464) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.656, test=0.477) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.627, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.652, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.660, test=0.482) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.632, test=0.546) total time=   1.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.650, test=0.456) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.658, test=0.472) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.628, test=0.540) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.648, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.651, test=0.474) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.622, test=0.540) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.649, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.657, test=0.481) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.627, test=0.545) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.647, test=0.456) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.655, test=0.471) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.623, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.645, test=0.456) total time=   1.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.648, test=0.471) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.619, test=0.540) total time=   1.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.646, test=0.458) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.653, test=0.480) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.624, test=0.542) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.644, test=0.454) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.652, test=0.468) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.620, test=0.535) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.339, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.334, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.308, test=0.294) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.338, test=0.257) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.312, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.337, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.266) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.308, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.336, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.331, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.335, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.264) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.309, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.334, test=0.265) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.305, test=0.291) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.253) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.303, test=0.291) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.333, test=0.255) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.331, test=0.263) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.307, test=0.288) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.334, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.263) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.339, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.334, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.308, test=0.294) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.337, test=0.258) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.312, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.337, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.336, test=0.266) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.308, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.336, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.331, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.335, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.309, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.334, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.306, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.303, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.333, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.331, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.307, test=0.288) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.333, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.263) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.290) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.338, test=0.255) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.333, test=0.262) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.307, test=0.293) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.337, test=0.257) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.335, test=0.265) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.311, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.336, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.307, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.336, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.331, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.333, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.308, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.332, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.305, test=0.291) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.302, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.332, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.330, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.306, test=0.288) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.333, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.500, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.498, test=0.383) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.468, test=0.432) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.501, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.504, test=0.390) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.471, test=0.433) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.499, test=0.369) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.501, test=0.380) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.467, test=0.427) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.497, test=0.365) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.463, test=0.429) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.498, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.500, test=0.388) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.467, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.499, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.426) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.494, test=0.362) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.460, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.386) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.463, test=0.429) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.462, test=0.426) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.499, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.498, test=0.382) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.468, test=0.432) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.501, test=0.369) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.503, test=0.390) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.471, test=0.433) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.499, test=0.368) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.501, test=0.383) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.467, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.496, test=0.365) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.463, test=0.430) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.498, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.500, test=0.387) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.467, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.499, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.493, test=0.362) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.460, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.386) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.463, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.364) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.378) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.461, test=0.426) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.499, test=0.367) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.497, test=0.381) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.466, test=0.431) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.500, test=0.368) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.502, test=0.389) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.469, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.498, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.500, test=0.381) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.466, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.494, test=0.378) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.462, test=0.429) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.497, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.499, test=0.388) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.466, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.366) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.380) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.428) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.361) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.491, test=0.377) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.459, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.494, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.495, test=0.385) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.462, test=0.428) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.495, test=0.366) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.376) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.461, test=0.427) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.652, test=0.465) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.656, test=0.480) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.629, test=0.543) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.653, test=0.463) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.661, test=0.483) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.634, test=0.547) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.651, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.659, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.627, test=0.538) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.649, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.652, test=0.477) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.624, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.649, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.657, test=0.481) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.628, test=0.546) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.648, test=0.457) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.657, test=0.473) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.623, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.645, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.649, test=0.475) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.619, test=0.539) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.646, test=0.459) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.653, test=0.480) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.624, test=0.543) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.645, test=0.457) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.654, test=0.471) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.621, test=0.536) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.652, test=0.464) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.656, test=0.480) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.628, test=0.542) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.653, test=0.464) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.660, test=0.483) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.634, test=0.547) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.651, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.659, test=0.477) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.627, test=0.539) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.649, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.652, test=0.477) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.624, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.649, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.657, test=0.481) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.628, test=0.546) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.648, test=0.458) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.656, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.624, test=0.539) total time=   0.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.645, test=0.460) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.649, test=0.474) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.619, test=0.539) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.646, test=0.459) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.653, test=0.480) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.625, test=0.543) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.646, test=0.456) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.654, test=0.473) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.621, test=0.536) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.651, test=0.465) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.655, test=0.477) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.626, test=0.542) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.651, test=0.463) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.658, test=0.482) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.632, test=0.545) total time=   0.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.650, test=0.458) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.658, test=0.473) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.627, test=0.539) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.647, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.651, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.622, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.648, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.655, test=0.482) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.627, test=0.545) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.647, test=0.458) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.656, test=0.473) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.622, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.644, test=0.459) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.648, test=0.470) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.617, test=0.539) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.645, test=0.458) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.652, test=0.479) total time=   1.2s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.623, test=0.542) total time=   1.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.644, test=0.456) total time=   0.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.652, test=0.470) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.619, test=0.534) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.337, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.334, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.307, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.337, test=0.257) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.311, test=0.293) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.337, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.266) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.307, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.336, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.331, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.335, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.333, test=0.264) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.309, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.334, test=0.264) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.303, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.333, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.330, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.306, test=0.289) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.334, test=0.253) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.263) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.304, test=0.290) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.337, test=0.256) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.334, test=0.263) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.307, test=0.293) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.337, test=0.257) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.311, test=0.293) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.336, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.266) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.307, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.336, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.331, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.335, test=0.256) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.264) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.308, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.333, test=0.264) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.305, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.334, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.260) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.303, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.333, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.331, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.306, test=0.289) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.333, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.303, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.337, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.333, test=0.261) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.307, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.336, test=0.257) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.335, test=0.265) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.310, test=0.293) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.336, test=0.254) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.335, test=0.265) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.307, test=0.293) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.335, test=0.253) total time=   0.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.330, test=0.260) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.304, test=0.291) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.334, test=0.255) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.333, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.308, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.335, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.332, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.304, test=0.292) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.333, test=0.252) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.329, test=0.259) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.302, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.332, test=0.254) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.330, test=0.263) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.305, test=0.288) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.333, test=0.253) total time=   0.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.331, test=0.262) total time=   0.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.303, test=0.290) total time=   0.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.497, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.498, test=0.382) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.466, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.500, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.503, test=0.390) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.469, test=0.432) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.498, test=0.369) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.501, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.466, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.378) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.462, test=0.430) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.498, test=0.366) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.500, test=0.388) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.465, test=0.431) total time=   0.5s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.498, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.362) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.459, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.495, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.384) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.462, test=0.430) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.495, test=0.367) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.376) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.461, test=0.425) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.497, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.498, test=0.381) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.466, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.500, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.503, test=0.390) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.469, test=0.432) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.498, test=0.369) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.501, test=0.379) total time=   0.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.466, test=0.428) total time=   0.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.365) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.495, test=0.378) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.462, test=0.431) total time=   0.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.498, test=0.366) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.500, test=0.387) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.465, test=0.432) total time=   0.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.499, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.463, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.361) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.492, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.459, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.494, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.496, test=0.385) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.462, test=0.430) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.495, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.496, test=0.376) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.461, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.497, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.497, test=0.379) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.465, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.499, test=0.367) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.502, test=0.389) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.467, test=0.433) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.498, test=0.368) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.500, test=0.380) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.465, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.493, test=0.363) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.494, test=0.377) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.462, test=0.430) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.496, test=0.365) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.499, test=0.387) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.465, test=0.431) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.496, test=0.366) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.497, test=0.380) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.462, test=0.427) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.491, test=0.360) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.491, test=0.375) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.458, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.494, test=0.364) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.495, test=0.385) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.461, test=0.429) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.495, test=0.365) total time=   0.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.495, test=0.377) total time=   0.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.460, test=0.428) total time=   0.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.648, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.654, test=0.478) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.625, test=0.540) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.650, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.659, test=0.483) total time=   0.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.631, test=0.546) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.644, test=0.457) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.659, test=0.470) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.625, test=0.537) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.646, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.651, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.622, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.647, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.655, test=0.479) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.625, test=0.546) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.642, test=0.457) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.655, test=0.471) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.622, test=0.535) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.642, test=0.458) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.647, test=0.475) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.617, test=0.538) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.644, test=0.459) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.652, test=0.478) total time=   1.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.622, test=0.544) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.640, test=0.454) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.652, test=0.468) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.620, test=0.535) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.648, test=0.463) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.654, test=0.478) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.625, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.650, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.659, test=0.483) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.631, test=0.547) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.643, test=0.457) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.659, test=0.472) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.626, test=0.539) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.646, test=0.461) total time=   1.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.650, test=0.475) total time=   1.2s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.621, test=0.541) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.647, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.656, test=0.480) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.625, test=0.546) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.641, test=0.456) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.655, test=0.471) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.623, test=0.537) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.642, test=0.458) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.647, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.618, test=0.539) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.644, test=0.461) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.652, test=0.479) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.622, test=0.543) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.640, test=0.454) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.652, test=0.467) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.619, test=0.536) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.647, test=0.462) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.653, test=0.476) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.624, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.649, test=0.462) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.657, test=0.481) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.629, test=0.547) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.643, test=0.456) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.656, test=0.471) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.624, test=0.535) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.644, test=0.459) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.649, test=0.474) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.620, test=0.540) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.645, test=0.460) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.655, test=0.481) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.625, test=0.546) total time=   1.1s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.642, test=0.454) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.655, test=0.471) total time=   1.2s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.621, test=0.537) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.641, test=0.458) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.647, test=0.472) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.616, test=0.541) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.643, test=0.459) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.651, test=0.479) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.621, test=0.543) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.639, test=0.453) total time=   0.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.651, test=0.469) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.618, test=0.536) total time=   0.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.436, test=0.287) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.432, test=0.294) total time=   0.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.415, test=0.345) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.442, test=0.294) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.442, test=0.296) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.423, test=0.340) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.451, test=0.300) total time=   0.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.454, test=0.300) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.427, test=0.334) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.430, test=0.285) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.425, test=0.288) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.409, test=0.341) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.436, test=0.291) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.434, test=0.290) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.418, test=0.336) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.447, test=0.301) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.446, test=0.296) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.421, test=0.334) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.423, test=0.284) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.418, test=0.287) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.402, test=0.337) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.430, test=0.290) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.426, test=0.288) total time=   0.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.412, test=0.336) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.440, test=0.297) total time=   1.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.441, test=0.294) total time=   0.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.416, test=0.330) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.436, test=0.287) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.432, test=0.293) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.415, test=0.344) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.442, test=0.294) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.442, test=0.295) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.423, test=0.340) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.451, test=0.300) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.454, test=0.300) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.427, test=0.332) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.429, test=0.286) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.424, test=0.287) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.408, test=0.341) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.435, test=0.291) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.434, test=0.290) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.418, test=0.337) total time=   0.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.445, test=0.297) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.446, test=0.296) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.421, test=0.332) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.423, test=0.284) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.417, test=0.287) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.402, test=0.337) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.430, test=0.290) total time=   0.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.426, test=0.288) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.412, test=0.336) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.440, test=0.298) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.441, test=0.294) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.416, test=0.328) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.433, test=0.286) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.429, test=0.292) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.413, test=0.343) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.440, test=0.292) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.439, test=0.293) total time=   0.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.422, test=0.338) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.451, test=0.299) total time=   1.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.451, test=0.299) total time=   0.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.424, test=0.335) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.427, test=0.288) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.422, test=0.288) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.406, test=0.341) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.433, test=0.291) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.431, test=0.288) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.416, test=0.336) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.444, test=0.301) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.444, test=0.295) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.420, test=0.331) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.422, test=0.285) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.416, test=0.286) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.399, test=0.336) total time=   0.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.427, test=0.288) total time=   0.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.425, test=0.287) total time=   0.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.410, test=0.335) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.439, test=0.295) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.440, test=0.292) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=50, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.415, test=0.329) total time=   0.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.645, test=0.416) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.642, test=0.426) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.624, test=0.495) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.658, test=0.420) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.657, test=0.433) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.636, test=0.492) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.668, test=0.424) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.672, test=0.430) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.646, test=0.488) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.638, test=0.413) total time=   1.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.634, test=0.421) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.617, test=0.493) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.651, test=0.416) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.648, test=0.426) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.629, test=0.489) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.660, test=0.424) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.663, test=0.428) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.638, test=0.486) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.630, test=0.413) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.626, test=0.418) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.609, test=0.490) total time=   1.7s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.644, test=0.413) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.639, test=0.423) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.622, test=0.487) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.654, test=0.418) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.657, test=0.427) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.631, test=0.485) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.645, test=0.415) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.642, test=0.426) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.624, test=0.494) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.658, test=0.420) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.657, test=0.432) total time=   1.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.636, test=0.492) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.668, test=0.424) total time=   1.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.672, test=0.433) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.646, test=0.488) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.637, test=0.414) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.633, test=0.421) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.616, test=0.492) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.650, test=0.416) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.648, test=0.427) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.629, test=0.489) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.659, test=0.418) total time=   1.3s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.663, test=0.427) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.638, test=0.486) total time=   1.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.629, test=0.413) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.625, test=0.419) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.609, test=0.489) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.644, test=0.413) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.638, test=0.421) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.621, test=0.488) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.653, test=0.419) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.657, test=0.426) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.631, test=0.484) total time=   1.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.642, test=0.414) total time=   1.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.639, test=0.427) total time=   1.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.621, test=0.494) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.655, test=0.419) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.654, test=0.431) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.634, test=0.488) total time=   1.3s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.665, test=0.424) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.669, test=0.429) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.643, test=0.486) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.635, test=0.416) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.631, test=0.421) total time=   1.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.613, test=0.492) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.647, test=0.414) total time=   1.4s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.644, test=0.423) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.626, test=0.488) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.658, test=0.424) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.660, test=0.426) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.636, test=0.487) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.627, test=0.410) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.623, test=0.418) total time=   1.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.605, test=0.487) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.641, test=0.412) total time=   1.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.636, test=0.422) total time=   1.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.618, test=0.486) total time=   1.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.651, test=0.416) total time=   1.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.655, test=0.421) total time=   1.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=2, subsample=1.0;, score=(train=0.629, test=0.486) total time=   1.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.824, test=0.521) total time=   2.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.826, test=0.526) total time=   2.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=(train=0.812, test=0.603) total time=   2.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.833, test=0.520) total time=   2.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.839, test=0.536) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=0.8;, score=(train=0.826, test=0.600) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.849, test=0.523) total time=   1.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.855, test=0.532) total time=   2.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1, subsample=1.0;, score=(train=0.837, test=0.596) total time=   2.0s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.817, test=0.516) total time=   3.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.819, test=0.520) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.7;, score=(train=0.805, test=0.603) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.828, test=0.515) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.832, test=0.530) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=0.8;, score=(train=0.819, test=0.600) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.843, test=0.517) total time=   3.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.848, test=0.533) total time=   2.2s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=1.5, subsample=1.0;, score=(train=0.828, test=0.597) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.811, test=0.515) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.812, test=0.518) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.7;, score=(train=0.799, test=0.600) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.822, test=0.511) total time=   2.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.825, test=0.529) total time=   2.6s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=0.8;, score=(train=0.813, test=0.600) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.837, test=0.516) total time=   2.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.842, test=0.532) total time=   2.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=2, subsample=1.0;, score=(train=0.823, test=0.595) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.823, test=0.519) total time=   2.6s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.827, test=0.526) total time=   2.7s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.7;, score=(train=0.811, test=0.603) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.833, test=0.518) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.839, test=0.534) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=0.8;, score=(train=0.826, test=0.600) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.850, test=0.521) total time=   2.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.856, test=0.533) total time=   3.3s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=(train=0.838, test=0.597) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.816, test=0.516) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.819, test=0.521) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.7;, score=(train=0.805, test=0.604) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.828, test=0.514) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.832, test=0.532) total time=   3.4s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=0.8;, score=(train=0.819, test=0.599) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.842, test=0.515) total time=   1.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.848, test=0.534) total time=   2.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=1.5, subsample=1.0;, score=(train=0.830, test=0.593) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.811, test=0.515) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.812, test=0.520) total time=   3.2s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.7;, score=(train=0.799, test=0.602) total time=   2.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.823, test=0.511) total time=   1.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.823, test=0.525) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=0.8;, score=(train=0.812, test=0.599) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.837, test=0.516) total time=   1.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.843, test=0.529) total time=   2.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=2, subsample=1.0;, score=(train=0.823, test=0.596) total time=   2.4s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.820, test=0.518) total time=   1.8s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.823, test=0.527) total time=   1.9s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.7;, score=(train=0.809, test=0.604) total time=   1.8s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.830, test=0.519) total time=   2.0s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.836, test=0.534) total time=   3.8s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=0.8;, score=(train=0.824, test=0.598) total time=   2.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.847, test=0.523) total time=   1.9s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.849, test=0.530) total time=   2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x789ca51cf8b0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 637, in _next_wrapper\n",
            "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 550, in _handle_exception\n",
            "    return fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 637, in <lambda>\n",
            "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1416, in next\n",
            "    input_data(**self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 617, in input_data\n",
            "    new, cat_codes, feature_names, feature_types = _proxy_transform(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1459, in _proxy_transform\n",
            "    df, feature_names, feature_types = _transform_pandas_df(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 611, in _transform_pandas_df\n",
            "    arrays = pandas_transform_data(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 550, in pandas_transform_data\n",
            "    result.append(oth_type(data[col]))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 529, in oth_type\n",
            "    array = ser.to_numpy(dtype=np.float32, na_value=np.nan)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\", line 660, in to_numpy\n",
            "    values[np.asanyarray(isna(self))] = na_value\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/missing.py\", line 178, in isna\n",
            "    return _isna(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/missing.py\", line 218, in _isna\n",
            "    result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\", line 596, in __init__\n",
            "    if original_dtype is None and is_pandas_object and data_dtype == np.object_:\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1, subsample=1.0;, score=(train=0.835, test=0.593) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=nan, test=nan) total time=   0.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.816, test=0.522) total time=   2.5s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.7;, score=(train=0.803, test=0.603) total time=   4.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.825, test=0.514) total time=   3.5s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.828, test=0.529) total time=   2.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=0.8;, score=(train=0.817, test=0.598) total time=   1.9s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.840, test=0.519) total time=   2.1s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.845, test=0.529) total time=   5.1s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=1.5, subsample=1.0;, score=(train=0.829, test=0.597) total time=   2.2s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.808, test=0.513) total time=   2.2s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.810, test=0.519) total time=   2.0s\n",
            "[CV 3/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.7;, score=(train=0.797, test=0.600) total time=   2.6s\n",
            "[CV 1/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.820, test=0.512) total time=   4.7s\n",
            "[CV 2/3] END colsample_bytree=0.7, gamma=0, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=2, subsample=0.8;, score=(train=0.822, test=0.526) total time=   3.4s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-abc954cb0467>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Fit GridSearchCV on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Print the best parameters and score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1109\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    \"\"\"Compute ECFP fingerprints for molecules in the dataframe.\"\"\"\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue  # Skip molecules that can't be parsed\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame for easier manipulation\n",
        "    ecfp_features = np.array(ecfp_list)\n",
        "    ecfp_df = pd.DataFrame(ecfp_features, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])  # Drop rows with missing SMILES or target values\n",
        "\n",
        "# Compute ECFP Fingerprints\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)  #\n",
        "\n",
        "# Define the target variable\n",
        "y = regression_data['logBB'].loc[ecfp_df.index].reset_index(drop=True)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(ecfp_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Simple XGBoost Model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R² Score: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6KUaiu1A9PC",
        "outputId": "0b2ffff3-bc59-4ef6-fef5-4b2941fd1fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n",
            "Mean Squared Error: 0.30593707006953563\n",
            "R² Score: 0.46951777870651634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    \"\"\"Compute ECFP fingerprints for molecules in the dataframe.\"\"\"\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue  # Skip molecules that can't be parsed\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame for easier manipulation\n",
        "    ecfp_features = np.array(ecfp_list)\n",
        "    ecfp_df = pd.DataFrame(ecfp_features, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])  # Drop rows with missing SMILES or target values\n",
        "\n",
        "# Compute ECFP Fingerprints\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)  # Pass regression_data containing 'SMILES' column\n",
        "\n",
        "# Define the target variable\n",
        "# Filter the target variable by valid indices from the ECFP computation\n",
        "y = regression_data['logBB'].loc[ecfp_df.index].reset_index(drop=True)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(ecfp_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize RandomForestRegressor with default parameters\n",
        "regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance with MSE and R² score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RandomForestRegressor Performance :\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wLs05seCJqq",
        "outputId": "67ca0a11-0ef0-424d-84d6-1d40b651520f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n",
            "RandomForestRegressor Performance :\n",
            "Mean Squared Error: 0.29\n",
            "R² Score: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    \"\"\"Compute ECFP fingerprints for molecules in the dataframe.\"\"\"\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue  # Skip molecules that can't be parsed\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame for easier manipulation\n",
        "    ecfp_features = np.array(ecfp_list)\n",
        "    ecfp_df = pd.DataFrame(ecfp_features, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "# Define the Comprehensive Descriptor Computation Function\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    \"\"\"Compute all available RDKit descriptors for a molecule.\"\"\"\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Compute Descriptors for Each Molecule\n",
        "def compute_all_descriptors(df):\n",
        "    \"\"\"Compute descriptors for all molecules in the dataframe.\"\"\"\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "            descriptor_data.append({name: None for name, _ in Descriptors._descList})  # Fill with missing values for invalid SMILES\n",
        "\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    print(f\"Shape of Descriptor features: {descriptor_df.shape}\")\n",
        "    return descriptor_df\n",
        "\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])  # Drop rows with missing SMILES or target values\n",
        "\n",
        "# Compute ECFP Fingerprints and Descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)       # ECFP features\n",
        "descriptor_df = compute_all_descriptors(regression_data)   # All RDKit descriptors\n",
        "\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "X_combined = pd.concat([ecfp_df.loc[valid_indices].reset_index(drop=True),\n",
        "                        descriptor_df.loc[valid_indices].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Define the target variable, filtered by valid indices\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Simple XGBoost Model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R² Score: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzGH1WnNCWgB",
        "outputId": "e1119c59-6602-46be-bc23-9e5ff8cfa119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n",
            "Shape of Descriptor features: (1058, 210)\n",
            "Mean Squared Error: 0.31869099120046307\n",
            "R² Score: 0.44740300716151016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    \"\"\"Compute ECFP fingerprints for molecules in the dataframe.\"\"\"\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue  # Skip molecules that can't be parsed\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame for easier manipulation\n",
        "    ecfp_features = np.array(ecfp_list)\n",
        "    ecfp_df = pd.DataFrame(ecfp_features, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "# Define the Comprehensive Descriptor Computation Function\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    \"\"\"Compute all available RDKit descriptors for a molecule.\"\"\"\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Compute Descriptors for Each Molecule\n",
        "def compute_all_descriptors(df):\n",
        "    \"\"\"Compute descriptors for all molecules in the dataframe.\"\"\"\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "            descriptor_data.append({name: None for name, _ in Descriptors._descList})  # Fill with missing values for invalid SMILES\n",
        "\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    print(f\"Shape of Descriptor features: {descriptor_df.shape}\")\n",
        "    return descriptor_df\n",
        "\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])  # Drop rows with missing SMILES or target values\n",
        "\n",
        "# Compute ECFP Fingerprints and Descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)       # ECFP features\n",
        "descriptor_df = compute_all_descriptors(regression_data)   # All RDKit descriptors\n",
        "\n",
        "# Filter and Combine Features\n",
        "# Ensure we only keep rows with valid SMILES that have both ECFP and descriptor data\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "X_combined = pd.concat([ecfp_df.loc[valid_indices].reset_index(drop=True),\n",
        "                        descriptor_df.loc[valid_indices].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Define the target variable, filtered by valid indices\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "# Ensure all column names are strings in the combined feature set\n",
        "X_combined.columns = X_combined.columns.astype(str)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# initialize RandomForestRegressor with default parameters\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance with MSE and R² score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RandomForestRegressor Performance :\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRx25-erC9qH",
        "outputId": "b898b01c-0bc9-41ec-fd66-4d557247f4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n",
            "Shape of Descriptor features: (1058, 210)\n",
            "RandomForestRegressor Performance :\n",
            "Mean Squared Error: 0.27\n",
            "R² Score: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import optuna\n",
        "\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "    ecfp_df = pd.DataFrame(ecfp_list, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "# Define the Descriptor Computation Function\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Compute Descriptors for Each Molecule\n",
        "def compute_all_descriptors(df):\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "            descriptor_data.append({name: None for name, _ in Descriptors._descList})\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    print(f\"Shape of Descriptor features: {descriptor_df.shape}\")\n",
        "    return descriptor_df\n",
        "\n",
        "# Load your data\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])\n",
        "\n",
        "# Compute ECFP Fingerprints and Descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)\n",
        "descriptor_df = compute_all_descriptors(regression_data)\n",
        "\n",
        "# Combine Features and Define Target Variable\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "X_combined = pd.concat([ecfp_df.loc[valid_indices].reset_index(drop=True),\n",
        "                        descriptor_df.loc[valid_indices].reset_index(drop=True)], axis=1)\n",
        "X_combined.columns = X_combined.columns.astype(str)\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Objective Function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 30)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
        "    max_features = trial.suggest_categorical('max_features', [ 'sqrt', 'log2'])\n",
        "\n",
        "    # Create the model with the suggested hyperparameters\n",
        "    regressor = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score on validation data\n",
        "    y_pred = regressor.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return r2\n",
        "\n",
        "# Run Optuna Study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Output Top 10 Hyperparameters and Scores\n",
        "print(\"\\nTop 10 Hyperparameter Configurations:\")\n",
        "top_10_trials = sorted(study.trials, key=lambda x: x.value, reverse=True)[:10]\n",
        "for i, trial in enumerate(top_10_trials, 1):\n",
        "    print(f\"\\nTrial {i}\")\n",
        "    print(f\"Score (R²): {trial.value:.4f}\")\n",
        "    print(\"Hyperparameters:\", trial.params)\n",
        "\n",
        "# Train final model with the best parameters\n",
        "best_params = study.best_params\n",
        "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on Test Set\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nFinal Model Performance on Test Set:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuBGsnRgDspo",
        "outputId": "c4b7cb41-581f-4104-949a-7f7cdc89c1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 03:48:09,754] A new study created in memory with name: no-name-ed028111-b247-4863-a2cf-782b54b80537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Descriptor features: (1058, 210)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 03:48:10,549] Trial 0 finished with value: 0.3403529849939686 and parameters: {'n_estimators': 488, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 0 with value: 0.3403529849939686.\n",
            "[I 2024-10-27 03:48:11,328] Trial 1 finished with value: 0.3808212545462639 and parameters: {'n_estimators': 341, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.3808212545462639.\n",
            "[I 2024-10-27 03:48:11,823] Trial 2 finished with value: 0.42957992094194986 and parameters: {'n_estimators': 251, 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 2 with value: 0.42957992094194986.\n",
            "[I 2024-10-27 03:48:13,880] Trial 3 finished with value: 0.5380648697411954 and parameters: {'n_estimators': 494, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:14,341] Trial 4 finished with value: 0.4397193186397129 and parameters: {'n_estimators': 228, 'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:14,766] Trial 5 finished with value: 0.42153140045307025 and parameters: {'n_estimators': 210, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:15,339] Trial 6 finished with value: 0.3815508514326903 and parameters: {'n_estimators': 324, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:16,474] Trial 7 finished with value: 0.48501207528332313 and parameters: {'n_estimators': 375, 'max_depth': 27, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:16,809] Trial 8 finished with value: 0.4421866552336646 and parameters: {'n_estimators': 160, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:17,032] Trial 9 finished with value: 0.4787195364224619 and parameters: {'n_estimators': 63, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:18,336] Trial 10 finished with value: 0.4472540027576256 and parameters: {'n_estimators': 499, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:19,983] Trial 11 finished with value: 0.4956522137072412 and parameters: {'n_estimators': 399, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:22,078] Trial 12 finished with value: 0.49909133654963 and parameters: {'n_estimators': 417, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:23,714] Trial 13 finished with value: 0.4981290222686876 and parameters: {'n_estimators': 436, 'max_depth': 24, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:25,163] Trial 14 finished with value: 0.4980093153272508 and parameters: {'n_estimators': 444, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:26,712] Trial 15 finished with value: 0.5154920768134885 and parameters: {'n_estimators': 438, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:27,821] Trial 16 finished with value: 0.5167662844257515 and parameters: {'n_estimators': 308, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.5380648697411954.\n",
            "[I 2024-10-27 03:48:30,105] Trial 17 finished with value: 0.5550744318138197 and parameters: {'n_estimators': 299, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:30,681] Trial 18 finished with value: 0.5057677077453995 and parameters: {'n_estimators': 140, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:32,154] Trial 19 finished with value: 0.5426657462001209 and parameters: {'n_estimators': 283, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:34,108] Trial 20 finished with value: 0.54128380921291 and parameters: {'n_estimators': 273, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:36,385] Trial 21 finished with value: 0.5426657462001209 and parameters: {'n_estimators': 283, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:37,641] Trial 22 finished with value: 0.5291584825757607 and parameters: {'n_estimators': 295, 'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:39,114] Trial 23 finished with value: 0.553247874485161 and parameters: {'n_estimators': 188, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:40,624] Trial 24 finished with value: 0.5532169754834069 and parameters: {'n_estimators': 189, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:41,409] Trial 25 finished with value: 0.5313928273889396 and parameters: {'n_estimators': 175, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.5550744318138197.\n",
            "[I 2024-10-27 03:48:42,538] Trial 26 finished with value: 0.5563855818874026 and parameters: {'n_estimators': 117, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:43,243] Trial 27 finished with value: 0.5309357766685081 and parameters: {'n_estimators': 95, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:44,697] Trial 28 finished with value: 0.5483084365875612 and parameters: {'n_estimators': 120, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:44,981] Trial 29 finished with value: 0.37827378625504327 and parameters: {'n_estimators': 83, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:46,933] Trial 30 finished with value: 0.5493066522043242 and parameters: {'n_estimators': 231, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:49,433] Trial 31 finished with value: 0.5544961252161835 and parameters: {'n_estimators': 192, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:50,704] Trial 32 finished with value: 0.5557643045809058 and parameters: {'n_estimators': 118, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:51,401] Trial 33 finished with value: 0.5393497326155879 and parameters: {'n_estimators': 118, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:51,683] Trial 34 finished with value: 0.5206331691060297 and parameters: {'n_estimators': 53, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:52,391] Trial 35 finished with value: 0.5365198180522324 and parameters: {'n_estimators': 120, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:52,766] Trial 36 finished with value: 0.4452930034367807 and parameters: {'n_estimators': 152, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:54,001] Trial 37 finished with value: 0.5362024182367853 and parameters: {'n_estimators': 245, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:54,802] Trial 38 finished with value: 0.4493685447630027 and parameters: {'n_estimators': 364, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:55,765] Trial 39 finished with value: 0.5365677664275645 and parameters: {'n_estimators': 213, 'max_depth': 27, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:55,989] Trial 40 finished with value: 0.39898913046104634 and parameters: {'n_estimators': 92, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:57,520] Trial 41 finished with value: 0.5481548201396546 and parameters: {'n_estimators': 190, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:48:58,661] Trial 42 finished with value: 0.5521089620980293 and parameters: {'n_estimators': 141, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:00,112] Trial 43 finished with value: 0.5459850027922462 and parameters: {'n_estimators': 259, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:02,026] Trial 44 finished with value: 0.5316902589609003 and parameters: {'n_estimators': 333, 'max_depth': 24, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:04,084] Trial 45 finished with value: 0.5514403204011454 and parameters: {'n_estimators': 173, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:05,004] Trial 46 finished with value: 0.528571073462122 and parameters: {'n_estimators': 212, 'max_depth': 23, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:05,376] Trial 47 finished with value: 0.5432154884243836 and parameters: {'n_estimators': 74, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:05,703] Trial 48 finished with value: 0.4838655898965962 and parameters: {'n_estimators': 105, 'max_depth': 22, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:06,557] Trial 49 finished with value: 0.5309002176837153 and parameters: {'n_estimators': 196, 'max_depth': 26, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:09,355] Trial 50 finished with value: 0.5560102648859796 and parameters: {'n_estimators': 357, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:12,072] Trial 51 finished with value: 0.5520146219453834 and parameters: {'n_estimators': 352, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 26 with value: 0.5563855818874026.\n",
            "[I 2024-10-27 03:49:14,796] Trial 52 finished with value: 0.5576673434729869 and parameters: {'n_estimators': 319, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:18,830] Trial 53 finished with value: 0.5534978206130398 and parameters: {'n_estimators': 395, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:20,544] Trial 54 finished with value: 0.544102849386991 and parameters: {'n_estimators': 318, 'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:21,727] Trial 55 finished with value: 0.485002278732535 and parameters: {'n_estimators': 377, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:23,391] Trial 56 finished with value: 0.5452222802995015 and parameters: {'n_estimators': 307, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:24,826] Trial 57 finished with value: 0.527800413696921 and parameters: {'n_estimators': 345, 'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:27,125] Trial 58 finished with value: 0.5521602645527967 and parameters: {'n_estimators': 257, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:28,488] Trial 59 finished with value: 0.4660326042572086 and parameters: {'n_estimators': 325, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:30,649] Trial 60 finished with value: 0.5451579002902374 and parameters: {'n_estimators': 297, 'max_depth': 21, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:33,875] Trial 61 finished with value: 0.5529626214121559 and parameters: {'n_estimators': 407, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:37,755] Trial 62 finished with value: 0.5525994433595294 and parameters: {'n_estimators': 466, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:41,923] Trial 63 finished with value: 0.5520596613966774 and parameters: {'n_estimators': 377, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:44,752] Trial 64 finished with value: 0.5457446870303834 and parameters: {'n_estimators': 357, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:48,132] Trial 65 finished with value: 0.5520432224394135 and parameters: {'n_estimators': 421, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:50,296] Trial 66 finished with value: 0.5484797793844297 and parameters: {'n_estimators': 391, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:51,778] Trial 67 finished with value: 0.5167197709432862 and parameters: {'n_estimators': 267, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:52,664] Trial 68 finished with value: 0.5179186856168705 and parameters: {'n_estimators': 234, 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:53,984] Trial 69 finished with value: 0.49948630193792576 and parameters: {'n_estimators': 390, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:56,573] Trial 70 finished with value: 0.5435869639222264 and parameters: {'n_estimators': 337, 'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:58,158] Trial 71 finished with value: 0.5502772314734596 and parameters: {'n_estimators': 131, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:49:59,495] Trial 72 finished with value: 0.5520808486530944 and parameters: {'n_estimators': 162, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 52 with value: 0.5576673434729869.\n",
            "[I 2024-10-27 03:50:00,403] Trial 73 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:00,739] Trial 74 finished with value: 0.5251885179511038 and parameters: {'n_estimators': 50, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:01,407] Trial 75 finished with value: 0.5557577089629016 and parameters: {'n_estimators': 76, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:02,009] Trial 76 finished with value: 0.5469226460892889 and parameters: {'n_estimators': 69, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:02,479] Trial 77 finished with value: 0.5310742184989745 and parameters: {'n_estimators': 98, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:02,740] Trial 78 finished with value: 0.503387199005672 and parameters: {'n_estimators': 83, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:03,395] Trial 79 finished with value: 0.5401795749131548 and parameters: {'n_estimators': 107, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:04,522] Trial 80 finished with value: 0.5524548680890474 and parameters: {'n_estimators': 134, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:06,800] Trial 81 finished with value: 0.550780793860501 and parameters: {'n_estimators': 288, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:08,024] Trial 82 finished with value: 0.5461855731558263 and parameters: {'n_estimators': 152, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:09,386] Trial 83 finished with value: 0.557559907725002 and parameters: {'n_estimators': 112, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:10,445] Trial 84 finished with value: 0.5393137857058163 and parameters: {'n_estimators': 111, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:11,188] Trial 85 finished with value: 0.5411417061491854 and parameters: {'n_estimators': 62, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:12,247] Trial 86 finished with value: 0.5575889233614837 and parameters: {'n_estimators': 123, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:13,345] Trial 87 finished with value: 0.5574690684530068 and parameters: {'n_estimators': 124, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:13,793] Trial 88 finished with value: 0.530487959521224 and parameters: {'n_estimators': 87, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:14,893] Trial 89 finished with value: 0.5573382261119209 and parameters: {'n_estimators': 126, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:15,991] Trial 90 finished with value: 0.5574690684530068 and parameters: {'n_estimators': 124, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:17,065] Trial 91 finished with value: 0.557352468312089 and parameters: {'n_estimators': 125, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:18,127] Trial 92 finished with value: 0.5566893384233347 and parameters: {'n_estimators': 127, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:19,182] Trial 93 finished with value: 0.5558682316351164 and parameters: {'n_estimators': 128, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:20,414] Trial 94 finished with value: 0.5540907285304104 and parameters: {'n_estimators': 145, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:22,280] Trial 95 finished with value: 0.5517030166656232 and parameters: {'n_estimators': 168, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:23,425] Trial 96 finished with value: 0.5409574212070379 and parameters: {'n_estimators': 123, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:24,414] Trial 97 finished with value: 0.5363267937808471 and parameters: {'n_estimators': 99, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:25,701] Trial 98 finished with value: 0.5446771760556959 and parameters: {'n_estimators': 150, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n",
            "[I 2024-10-27 03:50:26,260] Trial 99 finished with value: 0.5420869778393971 and parameters: {'n_estimators': 112, 'max_depth': 27, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 73 with value: 0.5577100212896768.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Hyperparameter Configurations:\n",
            "\n",
            "Trial 1\n",
            "Score (R²): 0.5577\n",
            "Hyperparameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 2\n",
            "Score (R²): 0.5577\n",
            "Hyperparameters: {'n_estimators': 319, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 3\n",
            "Score (R²): 0.5576\n",
            "Hyperparameters: {'n_estimators': 123, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 4\n",
            "Score (R²): 0.5576\n",
            "Hyperparameters: {'n_estimators': 112, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 5\n",
            "Score (R²): 0.5575\n",
            "Hyperparameters: {'n_estimators': 124, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 6\n",
            "Score (R²): 0.5575\n",
            "Hyperparameters: {'n_estimators': 124, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 7\n",
            "Score (R²): 0.5574\n",
            "Hyperparameters: {'n_estimators': 125, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 8\n",
            "Score (R²): 0.5573\n",
            "Hyperparameters: {'n_estimators': 126, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 9\n",
            "Score (R²): 0.5567\n",
            "Hyperparameters: {'n_estimators': 127, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 10\n",
            "Score (R²): 0.5564\n",
            "Hyperparameters: {'n_estimators': 117, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Final Model Performance on Test Set:\n",
            "Mean Squared Error: 0.26\n",
            "R² Score: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import optuna\n",
        "\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "    ecfp_df = pd.DataFrame(ecfp_list, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "#  Define the Descriptor Computation Function\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Compute Descriptors for Each Molecule\n",
        "def compute_all_descriptors(df):\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "            descriptor_data.append({name: None for name, _ in Descriptors._descList})\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    print(f\"Shape of Descriptor features: {descriptor_df.shape}\")\n",
        "    return descriptor_df\n",
        "\n",
        "# Load your data\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])\n",
        "\n",
        "# Compute ECFP Fingerprints and Descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)\n",
        "descriptor_df = compute_all_descriptors(regression_data)\n",
        "\n",
        "# Combine Features and Define Target Variable\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "X_combined = pd.concat([ecfp_df.loc[valid_indices].reset_index(drop=True),\n",
        "                        descriptor_df.loc[valid_indices].reset_index(drop=True)], axis=1)\n",
        "X_combined.columns = X_combined.columns.astype(str)\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Objective Function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 30)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
        "    max_features = trial.suggest_categorical('max_features', [ 'sqrt', 'log2'])\n",
        "\n",
        "    # Create the model with the suggested hyperparameters\n",
        "    regressor = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate R² score on validation data\n",
        "    y_pred = regressor.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return r2\n",
        "\n",
        "# Run Optuna Study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "#  Output Top 10 Hyperparameters and Scores\n",
        "print(\"\\nTop 10 Hyperparameter Configurations:\")\n",
        "top_10_trials = sorted(study.trials, key=lambda x: x.value, reverse=True)[:10]\n",
        "for i, trial in enumerate(top_10_trials, 1):\n",
        "    print(f\"\\nTrial {i}\")\n",
        "    print(f\"Score (R²): {trial.value:.4f}\")\n",
        "    print(\"Hyperparameters:\", trial.params)\n",
        "\n",
        "# Train final model with the best parameters\n",
        "best_params = study.best_params\n",
        "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on Test Set\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nFinal Model Performance on Test Set:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgLSDNWUGaFP",
        "outputId": "1fd151c3-2171-400f-fc1c-ff98604c5bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 03:51:54,904] A new study created in memory with name: no-name-310e0bbc-572b-4aa8-af3b-8ff5a725c5ff\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Descriptor features: (1058, 210)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 03:51:55,461] Trial 0 finished with value: 0.5317574784511014 and parameters: {'n_estimators': 119, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.5317574784511014.\n",
            "[I 2024-10-27 03:51:55,969] Trial 1 finished with value: 0.5302288603789113 and parameters: {'n_estimators': 116, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.5317574784511014.\n",
            "[I 2024-10-27 03:51:56,606] Trial 2 finished with value: 0.5363223470086405 and parameters: {'n_estimators': 110, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.5363223470086405.\n",
            "[I 2024-10-27 03:51:57,224] Trial 3 finished with value: 0.5338014033762388 and parameters: {'n_estimators': 107, 'max_depth': 29, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.5363223470086405.\n",
            "[I 2024-10-27 03:51:57,841] Trial 4 finished with value: 0.536328294125376 and parameters: {'n_estimators': 108, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.536328294125376.\n",
            "[I 2024-10-27 03:51:58,572] Trial 5 finished with value: 0.5364839418385733 and parameters: {'n_estimators': 129, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.5364839418385733.\n",
            "[I 2024-10-27 03:51:59,098] Trial 6 finished with value: 0.5324073201430726 and parameters: {'n_estimators': 118, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.5364839418385733.\n",
            "[I 2024-10-27 03:51:59,609] Trial 7 finished with value: 0.5321584483801594 and parameters: {'n_estimators': 115, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.5364839418385733.\n",
            "[I 2024-10-27 03:52:00,384] Trial 8 finished with value: 0.5362011817771025 and parameters: {'n_estimators': 116, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.5364839418385733.\n",
            "[I 2024-10-27 03:52:01,365] Trial 9 finished with value: 0.53944158478478 and parameters: {'n_estimators': 116, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 9 with value: 0.53944158478478.\n",
            "[I 2024-10-27 03:52:02,927] Trial 10 finished with value: 0.5543332634939666 and parameters: {'n_estimators': 126, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5543332634939666.\n",
            "[I 2024-10-27 03:52:04,191] Trial 11 finished with value: 0.554364644141333 and parameters: {'n_estimators': 128, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.554364644141333.\n",
            "[I 2024-10-27 03:52:05,227] Trial 12 finished with value: 0.5538959730963418 and parameters: {'n_estimators': 129, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.554364644141333.\n",
            "[I 2024-10-27 03:52:06,223] Trial 13 finished with value: 0.5542967171537305 and parameters: {'n_estimators': 124, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.554364644141333.\n",
            "[I 2024-10-27 03:52:07,071] Trial 14 finished with value: 0.5555233253885185 and parameters: {'n_estimators': 100, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.5555233253885185.\n",
            "[I 2024-10-27 03:52:07,923] Trial 15 finished with value: 0.5568718566016083 and parameters: {'n_estimators': 101, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 15 with value: 0.5568718566016083.\n",
            "[I 2024-10-27 03:52:08,747] Trial 16 finished with value: 0.5571345871145434 and parameters: {'n_estimators': 100, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:09,215] Trial 17 finished with value: 0.5295859171623423 and parameters: {'n_estimators': 101, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:10,070] Trial 18 finished with value: 0.5562977291024003 and parameters: {'n_estimators': 104, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:10,933] Trial 19 finished with value: 0.5470645602509969 and parameters: {'n_estimators': 104, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:11,412] Trial 20 finished with value: 0.5300252804347994 and parameters: {'n_estimators': 103, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:12,261] Trial 21 finished with value: 0.5566928789651133 and parameters: {'n_estimators': 105, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:13,148] Trial 22 finished with value: 0.5505258326163556 and parameters: {'n_estimators': 110, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.5571345871145434.\n",
            "[I 2024-10-27 03:52:14,197] Trial 23 finished with value: 0.557455070185277 and parameters: {'n_estimators': 106, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.557455070185277.\n",
            "[I 2024-10-27 03:52:15,400] Trial 24 finished with value: 0.547480896521548 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.557455070185277.\n",
            "[I 2024-10-27 03:52:16,785] Trial 25 finished with value: 0.5493465498915004 and parameters: {'n_estimators': 112, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.557455070185277.\n",
            "[I 2024-10-27 03:52:17,745] Trial 26 finished with value: 0.5564179264266529 and parameters: {'n_estimators': 102, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.557455070185277.\n",
            "[I 2024-10-27 03:52:18,604] Trial 27 finished with value: 0.5483143666942532 and parameters: {'n_estimators': 106, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 23 with value: 0.557455070185277.\n",
            "[I 2024-10-27 03:52:19,495] Trial 28 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:20,025] Trial 29 finished with value: 0.5298625497146124 and parameters: {'n_estimators': 112, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:20,934] Trial 30 finished with value: 0.5555278954150574 and parameters: {'n_estimators': 108, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:21,763] Trial 31 finished with value: 0.5564179264266529 and parameters: {'n_estimators': 102, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:22,624] Trial 32 finished with value: 0.5566928789651133 and parameters: {'n_estimators': 105, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:23,532] Trial 33 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:24,456] Trial 34 finished with value: 0.5568683840518495 and parameters: {'n_estimators': 113, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:25,372] Trial 35 finished with value: 0.5499674648867694 and parameters: {'n_estimators': 109, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:26,297] Trial 36 finished with value: 0.5570617968535736 and parameters: {'n_estimators': 111, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:26,922] Trial 37 finished with value: 0.5395078073705413 and parameters: {'n_estimators': 107, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:27,582] Trial 38 finished with value: 0.5295737576080405 and parameters: {'n_estimators': 109, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:28,863] Trial 39 finished with value: 0.5571455115938561 and parameters: {'n_estimators': 106, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:29,893] Trial 40 finished with value: 0.533324545693538 and parameters: {'n_estimators': 114, 'max_depth': 29, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:31,182] Trial 41 finished with value: 0.5571277736840594 and parameters: {'n_estimators': 119, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:32,057] Trial 42 finished with value: 0.5571607004005071 and parameters: {'n_estimators': 107, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:32,936] Trial 43 finished with value: 0.548730084825994 and parameters: {'n_estimators': 107, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:33,834] Trial 44 finished with value: 0.5580969251443597 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:34,734] Trial 45 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:35,653] Trial 46 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:36,570] Trial 47 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:37,268] Trial 48 finished with value: 0.5393497326155879 and parameters: {'n_estimators': 118, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:38,216] Trial 49 finished with value: 0.5567550022174366 and parameters: {'n_estimators': 114, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:39,122] Trial 50 finished with value: 0.5580969251443597 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:40,040] Trial 51 finished with value: 0.5570617968535736 and parameters: {'n_estimators': 111, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:40,968] Trial 52 finished with value: 0.5499674648867694 and parameters: {'n_estimators': 109, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:42,329] Trial 53 finished with value: 0.5570617968535736 and parameters: {'n_estimators': 111, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:43,743] Trial 54 finished with value: 0.5566473079100673 and parameters: {'n_estimators': 115, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:45,104] Trial 55 finished with value: 0.5499226003911772 and parameters: {'n_estimators': 122, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:45,994] Trial 56 finished with value: 0.5573500791680491 and parameters: {'n_estimators': 108, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:46,902] Trial 57 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:47,835] Trial 58 finished with value: 0.5493465498915004 and parameters: {'n_estimators': 112, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:48,775] Trial 59 finished with value: 0.5567175185506692 and parameters: {'n_estimators': 113, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:49,290] Trial 60 finished with value: 0.5295443234465578 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:50,190] Trial 61 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:51,093] Trial 62 finished with value: 0.5573500791680491 and parameters: {'n_estimators': 108, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:52,021] Trial 63 finished with value: 0.5492169971680927 and parameters: {'n_estimators': 111, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:53,001] Trial 64 finished with value: 0.5567558890303874 and parameters: {'n_estimators': 116, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:54,073] Trial 65 finished with value: 0.5568683840518495 and parameters: {'n_estimators': 113, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:55,931] Trial 66 finished with value: 0.5573500791680491 and parameters: {'n_estimators': 108, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:52:58,202] Trial 67 finished with value: 0.5582074454890081 and parameters: {'n_estimators': 110, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:00,599] Trial 68 finished with value: 0.5566928789651133 and parameters: {'n_estimators': 105, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:03,353] Trial 69 finished with value: 0.557559907725002 and parameters: {'n_estimators': 112, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:04,781] Trial 70 finished with value: 0.5562977291024003 and parameters: {'n_estimators': 104, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:06,307] Trial 71 finished with value: 0.5577100212896768 and parameters: {'n_estimators': 110, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:08,225] Trial 72 finished with value: 0.5580969251443597 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:10,106] Trial 73 finished with value: 0.5573500791680491 and parameters: {'n_estimators': 108, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:12,946] Trial 74 finished with value: 0.5576596662611737 and parameters: {'n_estimators': 107, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:15,253] Trial 75 finished with value: 0.5580969251443597 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:17,377] Trial 76 finished with value: 0.5483143666942532 and parameters: {'n_estimators': 106, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:19,227] Trial 77 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:20,365] Trial 78 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:21,245] Trial 79 finished with value: 0.5487443779482335 and parameters: {'n_estimators': 109, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:21,760] Trial 80 finished with value: 0.5301540381326794 and parameters: {'n_estimators': 108, 'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:22,790] Trial 81 finished with value: 0.557655894015666 and parameters: {'n_estimators': 111, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:23,814] Trial 82 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:25,177] Trial 83 finished with value: 0.5576596662611737 and parameters: {'n_estimators': 107, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:26,488] Trial 84 finished with value: 0.557455070185277 and parameters: {'n_estimators': 106, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:27,641] Trial 85 finished with value: 0.5487443779482335 and parameters: {'n_estimators': 109, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:28,537] Trial 86 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:29,395] Trial 87 finished with value: 0.5566928789651133 and parameters: {'n_estimators': 105, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:30,308] Trial 88 finished with value: 0.5513733019227874 and parameters: {'n_estimators': 112, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:31,216] Trial 89 finished with value: 0.557655894015666 and parameters: {'n_estimators': 111, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:32,111] Trial 90 finished with value: 0.5576596662611737 and parameters: {'n_estimators': 107, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:33,027] Trial 91 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:34,316] Trial 92 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:35,695] Trial 93 finished with value: 0.5579202775608049 and parameters: {'n_estimators': 108, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:37,110] Trial 94 finished with value: 0.5576596662611737 and parameters: {'n_estimators': 107, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:38,745] Trial 95 finished with value: 0.5512553717031237 and parameters: {'n_estimators': 113, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:40,093] Trial 96 finished with value: 0.5586704200000433 and parameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:41,348] Trial 97 finished with value: 0.557655894015666 and parameters: {'n_estimators': 111, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:42,242] Trial 98 finished with value: 0.557455070185277 and parameters: {'n_estimators': 106, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n",
            "[I 2024-10-27 03:53:43,142] Trial 99 finished with value: 0.5579202775608049 and parameters: {'n_estimators': 108, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 28 with value: 0.5586704200000433.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Hyperparameter Configurations:\n",
            "\n",
            "Trial 1\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 2\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 3\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 4\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 5\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 6\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 7\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 8\n",
            "Score (R²): 0.5587\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 9\n",
            "Score (R²): 0.5582\n",
            "Hyperparameters: {'n_estimators': 110, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Trial 10\n",
            "Score (R²): 0.5581\n",
            "Hyperparameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
            "\n",
            "Final Model Performance on Test Set:\n",
            "Mean Squared Error: 0.25\n",
            "R² Score: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "    ecfp_df = pd.DataFrame(ecfp_list, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "# Define the Descriptor Computation Function\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Compute Descriptors for Each Molecule\n",
        "def compute_all_descriptors(df):\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "            descriptor_data.append({name: None for name, _ in Descriptors._descList})\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    print(f\"Shape of Descriptor features: {descriptor_df.shape}\")\n",
        "    return descriptor_df\n",
        "\n",
        "# Load your data\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])\n",
        "\n",
        "# Compute ECFP Fingerprints and Descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)\n",
        "descriptor_df = compute_all_descriptors(regression_data)\n",
        "\n",
        "# Combine Features and Define Target Variable\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "X_combined = pd.concat([ecfp_df.loc[valid_indices].reset_index(drop=True),\n",
        "                        descriptor_df.loc[valid_indices].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Drop rows with any NaN values in `X_combined`\n",
        "X_combined = X_combined.dropna()\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "y = y[X_combined.index]  # Align `y` with `X_combined`\n",
        "\n",
        "# Check for extremely large values in each column\n",
        "extreme_thresholds = X_combined.quantile(0.999)\n",
        "for column in X_combined.columns:\n",
        "    cap_value = extreme_thresholds[column]\n",
        "    X_combined[column] = np.where(X_combined[column] > cap_value, cap_value, X_combined[column])\n",
        "\n",
        "# Convert column names to strings to avoid issues with mixed types\n",
        "X_combined.columns = X_combined.columns.astype(str)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and Train the PLS Model\n",
        "n_components = 3  # Adjust the number of components if needed\n",
        "pls_model = PLSRegression(n_components=n_components)\n",
        "pls_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions and Evaluate the Model\n",
        "y_pred = pls_model.predict(X_test)\n",
        "r_squared = pls_model.score(X_test, y_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"R-Squared: {r_squared}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgy9rYcOGvfy",
        "outputId": "0f552d95-90c8-44ce-e1b3-a5f6210d7947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n",
            "Shape of Descriptor features: (1058, 210)\n",
            "R-Squared: 0.586363944161495\n",
            "Mean Squared Error: 0.22509164960276232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "# Define the range of components to evaluate\n",
        "n_components_range = range(1, 20)  # Adjust upper limit based on dataset size\n",
        "best_score = -np.inf\n",
        "best_n_components = None\n",
        "\n",
        "# Evaluate each number of components using cross-validation\n",
        "for n in n_components_range:\n",
        "    pls_model = PLSRegression(n_components=n, scale=True)\n",
        "    scores = cross_val_score(pls_model, X_train, y_train, cv=5, scoring='r2')\n",
        "    mean_score = scores.mean()\n",
        "    print(f\"Number of Components: {n}, Cross-validated R² Score: {mean_score:.4f}\")\n",
        "\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_n_components = n\n",
        "\n",
        "print(f\"Best n_components: {best_n_components}\")\n",
        "print(f\"Cross-validated R² Score: {best_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Siw-gaOKh54",
        "outputId": "20dece24-c3d7-4adb-fbf7-071a1ba180e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Components: 1, Cross-validated R² Score: 0.1437\n",
            "Number of Components: 2, Cross-validated R² Score: 0.4554\n",
            "Number of Components: 3, Cross-validated R² Score: 0.4507\n",
            "Number of Components: 4, Cross-validated R² Score: 0.4052\n",
            "Number of Components: 5, Cross-validated R² Score: 0.4038\n",
            "Number of Components: 6, Cross-validated R² Score: 0.3439\n",
            "Number of Components: 7, Cross-validated R² Score: 0.3217\n",
            "Number of Components: 8, Cross-validated R² Score: 0.0934\n",
            "Number of Components: 9, Cross-validated R² Score: -0.0870\n",
            "Number of Components: 10, Cross-validated R² Score: -0.4572\n",
            "Number of Components: 11, Cross-validated R² Score: -1.1854\n",
            "Number of Components: 12, Cross-validated R² Score: -1.8682\n",
            "Number of Components: 13, Cross-validated R² Score: -2.8282\n",
            "Number of Components: 14, Cross-validated R² Score: -3.9219\n",
            "Number of Components: 15, Cross-validated R² Score: -4.9011\n",
            "Number of Components: 16, Cross-validated R² Score: -6.0613\n",
            "Number of Components: 17, Cross-validated R² Score: -6.9319\n",
            "Number of Components: 18, Cross-validated R² Score: -7.7354\n",
            "Number of Components: 19, Cross-validated R² Score: -8.3918\n",
            "Best n_components: 2\n",
            "Cross-validated R² Score: 0.4554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#  Define the ECFP Computation Function\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        try:\n",
        "            ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "            ecfp_list.append(ecfp_feats)\n",
        "            valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping molecule at index {idx} due to error: {e}\")\n",
        "            continue\n",
        "    ecfp_df = pd.DataFrame(ecfp_list, index=valid_indices)\n",
        "    print(f\"Shape of ECFP features: {ecfp_df.shape}\")\n",
        "    return ecfp_df\n",
        "\n",
        "#  Define the Descriptor Computation Function\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "# Compute Descriptors for Each Molecule\n",
        "def compute_all_descriptors(df):\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            print(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "            descriptor_data.append({name: None for name, _ in Descriptors._descList})\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    print(f\"Shape of Descriptor features: {descriptor_df.shape}\")\n",
        "    return descriptor_df\n",
        "\n",
        "# Load your data\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\")\n",
        "regression_data = regression_data.dropna(subset=['SMILES', 'logBB'])\n",
        "\n",
        "#Compute ECFP Fingerprints and Descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)\n",
        "descriptor_df = compute_all_descriptors(regression_data)\n",
        "\n",
        "# Combine Features and Define Target Variable\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "X_combined = pd.concat([ecfp_df.loc[valid_indices].reset_index(drop=True),\n",
        "                        descriptor_df.loc[valid_indices].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Drop rows with any NaN values in `X_combined`\n",
        "X_combined = X_combined.dropna()\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "y = y[X_combined.index]  # Align `y` with `X_combined`\n",
        "\n",
        "# Check for extremely large values in each column\n",
        "extreme_thresholds = X_combined.quantile(0.999)\n",
        "for column in X_combined.columns:\n",
        "    cap_value = extreme_thresholds[column]\n",
        "    X_combined[column] = np.where(X_combined[column] > cap_value, cap_value, X_combined[column])\n",
        "\n",
        "# Convert column names to strings to avoid issues with mixed types\n",
        "X_combined.columns = X_combined.columns.astype(str)\n",
        "\n",
        "# Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and Train the PLS Model\n",
        "n_components = 2  # Adjust the number of components if needed\n",
        "pls_model = PLSRegression(n_components=n_components)\n",
        "pls_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions and Evaluate the Model\n",
        "y_pred = pls_model.predict(X_test)\n",
        "r_squared = pls_model.score(X_test, y_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"R-Squared: {r_squared}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soBEOM37LMH5",
        "outputId": "680b5eaf-4a2a-43b0-cb23-09912898bbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ECFP features: (1058, 2048)\n",
            "Shape of Descriptor features: (1058, 210)\n",
            "R-Squared: 0.5258665637246713\n",
            "Mean Squared Error: 0.2580129942654412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-2. MLP**"
      ],
      "metadata": {
        "id": "WVCdNO-iN-CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def compute_ecfp_fingerprints(df, radius=2, nBits=2048):\n",
        "    ecfp_list = []\n",
        "    valid_indices = []\n",
        "    for idx, smi in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        ecfp_feats = np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits))\n",
        "        ecfp_list.append(ecfp_feats)\n",
        "        valid_indices.append(idx)\n",
        "    ecfp_df = pd.DataFrame(ecfp_list, index=valid_indices)\n",
        "    return ecfp_df\n",
        "\n",
        "def getMolDescriptors(mol, missingVal=None):\n",
        "    descriptors = {}\n",
        "    for name, func in Descriptors._descList:\n",
        "        try:\n",
        "            descriptors[name] = func(mol)\n",
        "        except Exception:\n",
        "            descriptors[name] = missingVal\n",
        "    return descriptors\n",
        "\n",
        "def compute_all_descriptors(df):\n",
        "    descriptor_data = []\n",
        "    valid_indices = []\n",
        "    for idx, smiles in df['SMILES'].items():\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            descriptors = getMolDescriptors(mol)\n",
        "            descriptor_data.append(descriptors)\n",
        "            valid_indices.append(idx)\n",
        "    descriptor_df = pd.DataFrame(descriptor_data, index=valid_indices)\n",
        "    return descriptor_df\n",
        "\n",
        "# Load the data\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\").dropna(subset=['SMILES', 'logBB'])\n",
        "\n",
        "# Compute ECFP fingerprints and descriptors\n",
        "ecfp_df = compute_ecfp_fingerprints(regression_data)\n",
        "descriptor_df = compute_all_descriptors(regression_data)\n",
        "valid_indices = ecfp_df.index.intersection(descriptor_df.index)\n",
        "# Split ECFP and descriptor data\n",
        "ecfp_features = ecfp_df.loc[valid_indices].reset_index(drop=True)\n",
        "descriptor_features = descriptor_df.loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "# Apply MinMax scaling to descriptor features only\n",
        "scaler = MinMaxScaler()\n",
        "scaled_descriptor_features = scaler.fit_transform(descriptor_features.fillna(0))  # Handling any NaN values\n",
        "\n",
        "# Combine scaled descriptors and unscaled ECFP features\n",
        "X_combined = np.hstack([ecfp_features.values, scaled_descriptor_features])\n",
        "y = regression_data['logBB'].loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "# Convert combined data to tensors for PyTorch\n",
        "X = torch.tensor(X_combined, dtype=torch.float32)\n",
        "y = torch.tensor(y.values, dtype=torch.float32)\n",
        "\n",
        "# Define model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.layer3 = nn.Linear(hidden_dim, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "# Model setup\n",
        "input_dim = X.shape[1]\n",
        "model = SimpleMLP(input_dim)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Data loading\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training setup\n",
        "loss_module = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 100  # Number of epochs for training\n",
        "\n",
        "# Training function with validation tracking\n",
        "def train_model(model, optimizer, train_loader, validation_loader, loss_module, num_epochs=100):\n",
        "    model.train()\n",
        "    train_loss_list = []\n",
        "    validation_loss_list = []\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for data_inputs, data_labels in train_loader:\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            preds = model(data_inputs).squeeze()\n",
        "            loss = loss_module(preds, data_labels)\n",
        "            train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data_inputs, data_labels in validation_loader:\n",
        "                data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                preds = model(data_inputs).squeeze()\n",
        "                loss = loss_module(preds, data_labels)\n",
        "                val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "        val_loss /= len(validation_loader)\n",
        "        validation_loss_list.append(val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Epoch: {epoch+1:03d}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_loss_list, validation_loss_list\n",
        "\n",
        "# Training and validation\n",
        "train_loss_list, validation_loss_list = train_model(\n",
        "    model, optimizer, train_loader, validation_loader, loss_module, num_epochs=num_epochs\n",
        ")\n",
        "\n",
        "# Final evaluation on test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_preds = model(X_test.to(device)).squeeze()\n",
        "    test_loss = loss_module(test_preds, y_test.to(device)).item()\n",
        "    r2 = r2_score(y_test.cpu().numpy(), test_preds.cpu().numpy())\n",
        "\n",
        "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
        "print(f\"Test R² Score: {r2:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtbRdQdeMU1d",
        "outputId": "c7938bb8-c1f9-46fa-e4b6-b826dcdfa477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 2/100 [00:00<00:05, 16.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/100, Training Loss: 0.4725, Validation Loss: 0.3561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 8/100 [00:00<00:05, 15.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 006/100, Training Loss: 0.0504, Validation Loss: 0.3102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 14/100 [00:00<00:05, 15.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 011/100, Training Loss: 0.0164, Validation Loss: 0.3154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 18/100 [00:01<00:05, 16.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 016/100, Training Loss: 0.0119, Validation Loss: 0.3226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 24/100 [00:01<00:04, 16.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 021/100, Training Loss: 0.0084, Validation Loss: 0.3226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 28/100 [00:01<00:04, 16.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 026/100, Training Loss: 0.0070, Validation Loss: 0.3212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 34/100 [00:02<00:04, 16.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 031/100, Training Loss: 0.0080, Validation Loss: 0.3211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 38/100 [00:02<00:03, 16.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 036/100, Training Loss: 0.0060, Validation Loss: 0.3104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 44/100 [00:02<00:03, 16.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 041/100, Training Loss: 0.0071, Validation Loss: 0.3147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 48/100 [00:02<00:03, 16.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 046/100, Training Loss: 0.0068, Validation Loss: 0.3212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 52/100 [00:03<00:03, 15.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 051/100, Training Loss: 0.0077, Validation Loss: 0.3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 58/100 [00:03<00:03, 13.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 056/100, Training Loss: 0.0055, Validation Loss: 0.3111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 62/100 [00:04<00:02, 13.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 061/100, Training Loss: 0.0051, Validation Loss: 0.3120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 68/100 [00:04<00:02, 13.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 066/100, Training Loss: 0.0052, Validation Loss: 0.3156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 72/100 [00:04<00:02, 12.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 071/100, Training Loss: 0.0052, Validation Loss: 0.3159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 78/100 [00:05<00:01, 12.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 076/100, Training Loss: 0.0045, Validation Loss: 0.3166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 82/100 [00:05<00:02,  8.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 081/100, Training Loss: 0.0055, Validation Loss: 0.3151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 88/100 [00:06<00:00, 12.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 086/100, Training Loss: 0.0053, Validation Loss: 0.3130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 94/100 [00:06<00:00, 14.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 091/100, Training Loss: 0.0056, Validation Loss: 0.3103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 98/100 [00:06<00:00, 15.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 096/100, Training Loss: 0.0061, Validation Loss: 0.3079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:07<00:00, 14.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (MSE): 0.2987\n",
            "Test R² Score: 0.4821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        # First hidden layer\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "        # Additional hidden layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(hidden_dim, 1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256, 512])\n",
        "    num_layers = trial.suggest_int(\"num_layers\",  1, 3)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)  # Updated syntax\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_data, valid_data, train_targets, valid_targets = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    train_loader = DataLoader(TensorDataset(train_data, train_targets), batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(TensorDataset(valid_data, valid_targets), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize the model, loss, and optimizer\n",
        "    input_dim = X.shape[1]\n",
        "    model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
        "    loss_module = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 100\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    best_r2 = -float(\"inf\")\n",
        "    patience, patience_counter = 10, 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for data_inputs, data_labels in train_loader:\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(data_inputs).squeeze()\n",
        "            loss = loss_module(preds, data_labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        preds_list, labels_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for data_inputs, data_labels in valid_loader:\n",
        "                data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "                preds = model(data_inputs).squeeze()\n",
        "                preds_list.extend(preds.cpu().numpy())\n",
        "                labels_list.extend(data_labels.cpu().numpy())\n",
        "                loss = loss_module(preds, data_labels.float())\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "        # Calculate R² for validation\n",
        "        valid_loss /= len(valid_loader)\n",
        "        preds_array, labels_array = np.array(preds_list), np.array(labels_list)\n",
        "        valid_r2 = r2_score(labels_array, preds_array)\n",
        "\n",
        "        # Early stopping\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_r2 = valid_r2\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    trial.set_user_attr(\"valid_r2\", best_r2)\n",
        "    return best_valid_loss\n",
        "\n",
        "# Set up and run the Optuna study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Print the best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value (Loss): {trial.value}\")\n",
        "print(f\"  Params: {trial.params}\")\n",
        "print(f\"  Validation R²: {trial.user_attrs['valid_r2']}\")\n",
        "\n",
        "# Output top 10 trials\n",
        "print(\"\\nTop 10 trials:\")\n",
        "top_10_trials = sorted(study.trials, key=lambda x: x.value)[:10]\n",
        "for i, t in enumerate(top_10_trials, 1):\n",
        "    print(f\"Trial {i} - Loss: {t.value:.4f}, Params: {t.params}, Validation R²: {t.user_attrs['valid_r2']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy7aNN76P3ii",
        "outputId": "11d2161b-2cd4-4c5d-dd34-72fbe6c86f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 04:39:12,256] A new study created in memory with name: no-name-87bd65bf-eff3-4559-ad39-00721dcde875\n",
            "[I 2024-10-27 04:39:13,809] Trial 0 finished with value: 0.310543667525053 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout_rate': 0.33882396260247816, 'learning_rate': 0.0015629395679993782, 'batch_size': 64}. Best is trial 0 with value: 0.310543667525053.\n",
            "[I 2024-10-27 04:39:16,590] Trial 1 finished with value: 0.29004940603460583 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout_rate': 0.4624977170540201, 'learning_rate': 0.007473227776834686, 'batch_size': 32}. Best is trial 1 with value: 0.29004940603460583.\n",
            "[I 2024-10-27 04:39:21,900] Trial 2 finished with value: 0.2723926209977695 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.3157666146072594, 'learning_rate': 0.00013176690290753003, 'batch_size': 16}. Best is trial 2 with value: 0.2723926209977695.\n",
            "[I 2024-10-27 04:39:23,237] Trial 3 finished with value: 0.2815992832183838 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.1041759805663368, 'learning_rate': 0.00017619244433475674, 'batch_size': 32}. Best is trial 2 with value: 0.2723926209977695.\n",
            "[I 2024-10-27 04:39:24,583] Trial 4 finished with value: 0.2786022275686264 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout_rate': 0.49704609451568804, 'learning_rate': 0.0009126198623215925, 'batch_size': 32}. Best is trial 2 with value: 0.2723926209977695.\n",
            "[I 2024-10-27 04:39:26,498] Trial 5 finished with value: 0.275417087333543 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'dropout_rate': 0.44991893197821686, 'learning_rate': 0.0006834034972062054, 'batch_size': 32}. Best is trial 2 with value: 0.2723926209977695.\n",
            "[I 2024-10-27 04:39:29,212] Trial 6 finished with value: 0.269836072410856 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'dropout_rate': 0.4619313142953697, 'learning_rate': 0.00013346455248303263, 'batch_size': 32}. Best is trial 6 with value: 0.269836072410856.\n",
            "[I 2024-10-27 04:39:31,048] Trial 7 finished with value: 0.27420805394649506 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout_rate': 0.36587171856071987, 'learning_rate': 0.000294144950264673, 'batch_size': 32}. Best is trial 6 with value: 0.269836072410856.\n",
            "[I 2024-10-27 04:39:32,818] Trial 8 finished with value: 0.28177931532263756 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.23590734296206473, 'learning_rate': 0.004314309313273187, 'batch_size': 64}. Best is trial 6 with value: 0.269836072410856.\n",
            "[I 2024-10-27 04:39:33,962] Trial 9 finished with value: 0.2965392395853996 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout_rate': 0.4102654106492517, 'learning_rate': 0.0002908872942506445, 'batch_size': 64}. Best is trial 6 with value: 0.269836072410856.\n",
            "[I 2024-10-27 04:39:38,239] Trial 10 finished with value: 0.26922064274549484 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.2300106774035763, 'learning_rate': 0.0030982993553195126, 'batch_size': 16}. Best is trial 10 with value: 0.26922064274549484.\n",
            "[I 2024-10-27 04:39:42,891] Trial 11 finished with value: 0.2727447295827525 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.22542956023941918, 'learning_rate': 0.002435610489320595, 'batch_size': 16}. Best is trial 10 with value: 0.26922064274549484.\n",
            "[I 2024-10-27 04:39:46,210] Trial 12 finished with value: 0.2681117483547756 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.2327477773101167, 'learning_rate': 0.0029315329027199953, 'batch_size': 16}. Best is trial 12 with value: 0.2681117483547756.\n",
            "[I 2024-10-27 04:39:49,630] Trial 13 finished with value: 0.27314842279468265 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.21809418334981037, 'learning_rate': 0.003532778371298453, 'batch_size': 16}. Best is trial 12 with value: 0.2681117483547756.\n",
            "[I 2024-10-27 04:39:52,636] Trial 14 finished with value: 0.29503582205091206 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout_rate': 0.15244790635910827, 'learning_rate': 0.007914239836114825, 'batch_size': 16}. Best is trial 12 with value: 0.2681117483547756.\n",
            "[I 2024-10-27 04:39:54,888] Trial 15 finished with value: 0.2709551975131035 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.2640953664207593, 'learning_rate': 0.002024626301363329, 'batch_size': 16}. Best is trial 12 with value: 0.2681117483547756.\n",
            "[I 2024-10-27 04:39:57,847] Trial 16 finished with value: 0.2804088890552521 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.1735765376991445, 'learning_rate': 0.004143953121567431, 'batch_size': 16}. Best is trial 12 with value: 0.2681117483547756.\n",
            "[I 2024-10-27 04:40:01,382] Trial 17 finished with value: 0.26289520412683487 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.2657554225818198, 'learning_rate': 0.0013205140937103549, 'batch_size': 16}. Best is trial 17 with value: 0.26289520412683487.\n",
            "[I 2024-10-27 04:40:04,357] Trial 18 finished with value: 0.27102178973811014 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.2881583550924147, 'learning_rate': 0.001301805140577366, 'batch_size': 16}. Best is trial 17 with value: 0.26289520412683487.\n",
            "[I 2024-10-27 04:40:06,070] Trial 19 finished with value: 0.2778497636318207 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout_rate': 0.1808820037809326, 'learning_rate': 0.0005712503584825051, 'batch_size': 16}. Best is trial 17 with value: 0.26289520412683487.\n",
            "[I 2024-10-27 04:40:09,419] Trial 20 finished with value: 0.26335486343928743 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.36443391097362265, 'learning_rate': 0.00043792642541156177, 'batch_size': 16}. Best is trial 17 with value: 0.26289520412683487.\n",
            "[I 2024-10-27 04:40:13,074] Trial 21 finished with value: 0.26742990421397345 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.3777154698956873, 'learning_rate': 0.000386903767323396, 'batch_size': 16}. Best is trial 17 with value: 0.26289520412683487.\n",
            "[I 2024-10-27 04:40:16,748] Trial 22 finished with value: 0.26035691265548977 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.3739456915683935, 'learning_rate': 0.00040406650663842584, 'batch_size': 16}. Best is trial 22 with value: 0.26035691265548977.\n",
            "[I 2024-10-27 04:40:18,746] Trial 23 finished with value: 0.26864949081625256 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.38624716933677933, 'learning_rate': 0.00047372525176508294, 'batch_size': 16}. Best is trial 22 with value: 0.26035691265548977.\n",
            "[I 2024-10-27 04:40:22,216] Trial 24 finished with value: 0.26583374930279596 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.33712428833360286, 'learning_rate': 0.0008403557691668311, 'batch_size': 16}. Best is trial 22 with value: 0.26035691265548977.\n",
            "[I 2024-10-27 04:40:23,354] Trial 25 finished with value: 0.30684585124254227 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'dropout_rate': 0.4221478534457112, 'learning_rate': 0.0002004379666243647, 'batch_size': 64}. Best is trial 22 with value: 0.26035691265548977.\n",
            "[I 2024-10-27 04:40:27,757] Trial 26 finished with value: 0.27258027344942093 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout_rate': 0.28686491075786735, 'learning_rate': 0.0011963706498340828, 'batch_size': 16}. Best is trial 22 with value: 0.26035691265548977.\n",
            "[I 2024-10-27 04:40:29,860] Trial 27 finished with value: 0.25782763000045505 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.3432544875463592, 'learning_rate': 0.00041024730021310625, 'batch_size': 16}. Best is trial 27 with value: 0.25782763000045505.\n",
            "[I 2024-10-27 04:40:32,529] Trial 28 finished with value: 0.25486054590770174 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.33505494020182836, 'learning_rate': 0.00028301015452717544, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:33,187] Trial 29 finished with value: 0.2965852804481983 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.3320082770728976, 'learning_rate': 0.0002760413728581065, 'batch_size': 64}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:35,049] Trial 30 finished with value: 0.2675142384001187 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.4029811152929301, 'learning_rate': 0.00010073865302011142, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:36,904] Trial 31 finished with value: 0.26138404650347574 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.34949623926939405, 'learning_rate': 0.0006972401949237966, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:38,990] Trial 32 finished with value: 0.2682053191321237 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.3139507149497371, 'learning_rate': 0.0007235442826940362, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:42,419] Trial 33 finished with value: 0.26938911633832113 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.35321434972161786, 'learning_rate': 0.0003657166117880648, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:46,157] Trial 34 finished with value: 0.2672622267689024 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.3265643558934681, 'learning_rate': 0.0002133549272479682, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:49,962] Trial 35 finished with value: 0.27545772599322454 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.35123677825683863, 'learning_rate': 0.0005701159131622635, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:50,943] Trial 36 finished with value: 0.2997807413339615 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.3047742127597803, 'learning_rate': 0.0005553469890561657, 'batch_size': 64}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:53,146] Trial 37 finished with value: 0.2771881946495601 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.4360884549893424, 'learning_rate': 0.0002373523393368835, 'batch_size': 32}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:55,553] Trial 38 finished with value: 0.26817374250718523 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout_rate': 0.389138079316595, 'learning_rate': 0.00034592127170597967, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:40:56,867] Trial 39 finished with value: 0.2707871219941548 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.4772616132873474, 'learning_rate': 0.00013347048026726832, 'batch_size': 32}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:41:00,672] Trial 40 finished with value: 0.26902334285633905 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout_rate': 0.4284146370775882, 'learning_rate': 0.0007180092108676442, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:41:03,027] Trial 41 finished with value: 0.2757593398647649 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.2587049659185563, 'learning_rate': 0.0016476459585058664, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:41:06,645] Trial 42 finished with value: 0.2751068254666669 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.2703894945602879, 'learning_rate': 0.0011578076033339501, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:41:08,874] Trial 43 finished with value: 0.2684394839618887 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout_rate': 0.2989299411509118, 'learning_rate': 0.0009601596854115123, 'batch_size': 16}. Best is trial 28 with value: 0.25486054590770174.\n",
            "[I 2024-10-27 04:41:11,634] Trial 44 finished with value: 0.24743948663984025 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.348829473628014, 'learning_rate': 0.0001612887827749467, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:13,118] Trial 45 finished with value: 0.28093720972537994 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.3521737169418144, 'learning_rate': 0.00018892498987256362, 'batch_size': 32}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:14,744] Trial 46 finished with value: 0.29162679240107536 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4013915551980013, 'learning_rate': 0.0001628983197047185, 'batch_size': 64}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:16,095] Trial 47 finished with value: 0.26165577130658285 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.3690834267994175, 'learning_rate': 0.0002543519031462362, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:18,162] Trial 48 finished with value: 0.2650570177606174 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.32220196658223943, 'learning_rate': 0.00031462674518058817, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:23,336] Trial 49 finished with value: 0.2627667247184685 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout_rate': 0.34909275355015273, 'learning_rate': 0.00015500024115309676, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:25,533] Trial 50 finished with value: 0.2755778112581798 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.3816371491355705, 'learning_rate': 0.000456431462139155, 'batch_size': 32}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:27,088] Trial 51 finished with value: 0.2632738724350929 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.3759530753083373, 'learning_rate': 0.00027779562694020955, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:29,093] Trial 52 finished with value: 0.2736222594976425 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.366419941016972, 'learning_rate': 0.0002402188948102963, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:31,155] Trial 53 finished with value: 0.26540222444704603 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.3410209879087614, 'learning_rate': 0.0001036735473327036, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:32,542] Trial 54 finished with value: 0.26117496511765886 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout_rate': 0.41410223851598144, 'learning_rate': 0.00040342829869512945, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:35,122] Trial 55 finished with value: 0.2705740449684007 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout_rate': 0.4106548242164298, 'learning_rate': 0.00041207082181168213, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:39,210] Trial 56 finished with value: 0.26801664914403645 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.3924781257014487, 'learning_rate': 0.0005216242869804398, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:40,689] Trial 57 finished with value: 0.27929369466645376 and parameters: {'hidden_dim': 256, 'num_layers': 1, 'dropout_rate': 0.30933597885067554, 'learning_rate': 0.0006300802660323408, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:42,746] Trial 58 finished with value: 0.25751887474741253 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4603024064625163, 'learning_rate': 0.00030832795451845365, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:44,689] Trial 59 finished with value: 0.33691598049231936 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.49968618021415423, 'learning_rate': 0.005770263912393913, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:45,417] Trial 60 finished with value: 0.30215856432914734 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.45415823747969825, 'learning_rate': 0.0003203193253581846, 'batch_size': 64}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:50,706] Trial 61 finished with value: 0.24920496344566345 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48039852572629405, 'learning_rate': 0.0003868468783532605, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:41:57,251] Trial 62 finished with value: 0.2602701910904476 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.47466327789714907, 'learning_rate': 0.00038207365949535814, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:03,401] Trial 63 finished with value: 0.25864565478903906 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48071874036262, 'learning_rate': 0.00014229883402763302, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:06,159] Trial 64 finished with value: 0.25826726321663174 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48179474771082853, 'learning_rate': 0.00014347254356992148, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:08,382] Trial 65 finished with value: 0.2641014882496425 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48035911214633203, 'learning_rate': 0.00013315582187360296, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:13,118] Trial 66 finished with value: 0.2722586765885353 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4419067809500824, 'learning_rate': 0.0001207710650275702, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:19,315] Trial 67 finished with value: 0.25565255007573534 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48791822334002816, 'learning_rate': 0.00017779524995228341, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:22,979] Trial 68 finished with value: 0.26510219861354145 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4646370347579383, 'learning_rate': 0.00017449712509409903, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:25,073] Trial 69 finished with value: 0.2644777170249394 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4884140406382847, 'learning_rate': 0.0002133879727957753, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:28,064] Trial 70 finished with value: 0.2613377943634987 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4582877501672055, 'learning_rate': 0.00011088486118766602, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:32,604] Trial 71 finished with value: 0.2685574245240007 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4882897941415083, 'learning_rate': 0.00014955045299985757, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:36,595] Trial 72 finished with value: 0.26229055332286016 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.46722092910448737, 'learning_rate': 0.00019096583342744349, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:39,502] Trial 73 finished with value: 0.2558209959949766 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4472835209762094, 'learning_rate': 0.00014693615565338465, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:43,240] Trial 74 finished with value: 0.2614347072584288 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4469518583530293, 'learning_rate': 0.00017691029349784526, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:45,811] Trial 75 finished with value: 0.2614303327032498 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout_rate': 0.4331513696638948, 'learning_rate': 0.0002228593817279551, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:47,904] Trial 76 finished with value: 0.34032287129334043 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.44714971640520973, 'learning_rate': 0.009920976451239473, 'batch_size': 32}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:48,791] Trial 77 finished with value: 0.3031027913093567 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4705211670036946, 'learning_rate': 0.00011847433883977331, 'batch_size': 64}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:50,796] Trial 78 finished with value: 0.2635051544223513 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout_rate': 0.2040916823082236, 'learning_rate': 0.00026876911928619586, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:42:58,301] Trial 79 finished with value: 0.26995265058108736 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.12338512549466712, 'learning_rate': 0.00031838266480894767, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:02,193] Trial 80 finished with value: 0.27020462655595373 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48883972419695954, 'learning_rate': 0.00016376888361488464, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:04,973] Trial 81 finished with value: 0.2621371384177889 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48217890190533164, 'learning_rate': 0.00014044713639351357, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:07,590] Trial 82 finished with value: 0.2678070994360106 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.49929666923562604, 'learning_rate': 0.0002003908505245231, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:11,373] Trial 83 finished with value: 0.263279632798263 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.42208927530309054, 'learning_rate': 0.0002392981778020772, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:14,423] Trial 84 finished with value: 0.260685531688588 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.45912691196304317, 'learning_rate': 0.0001224619912248484, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:17,005] Trial 85 finished with value: 0.26304751953908373 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.46870875606692347, 'learning_rate': 0.00014765834689381066, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:20,654] Trial 86 finished with value: 0.26362732531768934 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.45370333507558697, 'learning_rate': 0.00017473693783811456, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:23,280] Trial 87 finished with value: 0.2594051052417074 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4923697457060038, 'learning_rate': 0.00029860712245767345, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:24,591] Trial 88 finished with value: 0.274514513356345 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout_rate': 0.2889420286486082, 'learning_rate': 0.0003416515190145261, 'batch_size': 32}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:29,048] Trial 89 finished with value: 0.27192944660782814 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4802233658495908, 'learning_rate': 0.0004928604219778298, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:32,017] Trial 90 finished with value: 0.2595472734953676 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.44271109679820525, 'learning_rate': 0.00021793019888506152, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:35,417] Trial 91 finished with value: 0.26063662501318113 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4885994091601345, 'learning_rate': 0.0002971328393585033, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:38,705] Trial 92 finished with value: 0.2730119808443955 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.49236922318993237, 'learning_rate': 0.00026012192817889484, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:42,123] Trial 93 finished with value: 0.26734270261866705 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.321309611384155, 'learning_rate': 0.00013035952177309676, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:45,586] Trial 94 finished with value: 0.2592915807451521 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4748729050686795, 'learning_rate': 0.000191766864641593, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:46,779] Trial 95 finished with value: 0.30896683037281036 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4639224481225491, 'learning_rate': 0.00016067143336854153, 'batch_size': 64}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:50,752] Trial 96 finished with value: 0.2627213852746146 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout_rate': 0.4749729488429986, 'learning_rate': 0.00018597722339435576, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:53,627] Trial 97 finished with value: 0.25881533856902805 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4258820877546219, 'learning_rate': 0.00019854756662398664, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:56,253] Trial 98 finished with value: 0.2610186008470399 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4269999211514024, 'learning_rate': 0.00014774751686713115, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n",
            "[I 2024-10-27 04:43:58,155] Trial 99 finished with value: 0.26393621202026096 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.4378053487409488, 'learning_rate': 0.00022938403131127325, 'batch_size': 16}. Best is trial 44 with value: 0.24743948663984025.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Value (Loss): 0.24743948663984025\n",
            "  Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.348829473628014, 'learning_rate': 0.0001612887827749467, 'batch_size': 16}\n",
            "  Validation R²: 0.5762923955917358\n",
            "\n",
            "Top 10 trials:\n",
            "Trial 1 - Loss: 0.2474, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.348829473628014, 'learning_rate': 0.0001612887827749467, 'batch_size': 16}, Validation R²: 0.5763\n",
            "Trial 2 - Loss: 0.2492, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48039852572629405, 'learning_rate': 0.0003868468783532605, 'batch_size': 16}, Validation R²: 0.5718\n",
            "Trial 3 - Loss: 0.2549, Params: {'hidden_dim': 512, 'num_layers': 2, 'dropout_rate': 0.33505494020182836, 'learning_rate': 0.00028301015452717544, 'batch_size': 16}, Validation R²: 0.5666\n",
            "Trial 4 - Loss: 0.2557, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48791822334002816, 'learning_rate': 0.00017779524995228341, 'batch_size': 16}, Validation R²: 0.5639\n",
            "Trial 5 - Loss: 0.2558, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4472835209762094, 'learning_rate': 0.00014693615565338465, 'batch_size': 16}, Validation R²: 0.5546\n",
            "Trial 6 - Loss: 0.2575, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4603024064625163, 'learning_rate': 0.00030832795451845365, 'batch_size': 16}, Validation R²: 0.5554\n",
            "Trial 7 - Loss: 0.2578, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.3432544875463592, 'learning_rate': 0.00041024730021310625, 'batch_size': 16}, Validation R²: 0.5504\n",
            "Trial 8 - Loss: 0.2583, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48179474771082853, 'learning_rate': 0.00014347254356992148, 'batch_size': 16}, Validation R²: 0.5625\n",
            "Trial 9 - Loss: 0.2586, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.48071874036262, 'learning_rate': 0.00014229883402763302, 'batch_size': 16}, Validation R²: 0.5566\n",
            "Trial 10 - Loss: 0.2588, Params: {'hidden_dim': 512, 'num_layers': 3, 'dropout_rate': 0.4258820877546219, 'learning_rate': 0.00019854756662398664, 'batch_size': 16}, Validation R²: 0.5564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-3. GNN**"
      ],
      "metadata": {
        "id": "kEgaqycFSuDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irs6HUf9S_xw",
        "outputId": "eecd3996-141e-4ae3-9983-05addee02039"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# Initialize valid_atoms as an empty dictionary to dynamically add atoms as they are encountered\n",
        "# Set the fixed length for node attributes\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from tqdm import tqdm\n",
        "\n",
        "FIXED_NODE_ATTR_LENGTH = 30\n",
        "valid_atoms = {'H': 0, 'B': 1, 'C': 2, 'N': 3, 'O': 4, 'F': 5, 'P': 6, 'S': 7, 'Cl': 8}\n",
        "\n",
        "def convert_mol_to_graph(mol, use_pos=False):\n",
        "    mol2 = Chem.RemoveHs(mol)\n",
        "    n_bonds = len(mol2.GetBonds())\n",
        "    n_atoms = len(mol2.GetAtoms())\n",
        "\n",
        "    edge_index_list = []\n",
        "    edge_attr_list = []\n",
        "    edge_weight_list = []\n",
        "\n",
        "    # Edge attribute and weight calculation\n",
        "    for edge_idx in range(n_bonds):\n",
        "        bond = mol2.GetBondWithIdx(edge_idx)\n",
        "        begin_idx, end_idx = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index_list.extend([[begin_idx, end_idx], [end_idx, begin_idx]])\n",
        "\n",
        "        bond_type_map = {\n",
        "            Chem.rdchem.BondType.SINGLE: [1, 0, 0, 0],\n",
        "            Chem.rdchem.BondType.AROMATIC: [0, 1, 0, 0],\n",
        "            Chem.rdchem.BondType.DOUBLE: [0, 0, 1, 0],\n",
        "            Chem.rdchem.BondType.TRIPLE: [0, 0, 0, 1]\n",
        "        }\n",
        "        bond_one_hot = bond_type_map.get(bond.GetBondType(), [0, 0, 0, 0])\n",
        "        edge_weight_list.extend([1.0, 1.0])  # Adjusted default weights\n",
        "\n",
        "        stereo_map = {\n",
        "            Chem.rdchem.BondStereo.STEREOANY: [1, 0, 0, 0, 0, 0],\n",
        "            Chem.rdchem.BondStereo.STEREOCIS: [0, 1, 0, 0, 0, 0],\n",
        "            Chem.rdchem.BondStereo.STEREOE: [0, 0, 1, 0, 0, 0],\n",
        "            Chem.rdchem.BondStereo.STEREONONE: [0, 0, 0, 1, 0, 0],\n",
        "            Chem.rdchem.BondStereo.STEREOTRANS: [0, 0, 0, 0, 1, 0],\n",
        "            Chem.rdchem.BondStereo.STEREOZ: [0, 0, 0, 0, 0, 1]\n",
        "        }\n",
        "        stereo_one_hot = stereo_map.get(bond.GetStereo(), [0, 0, 0, 0, 0, 0])\n",
        "\n",
        "        ring_bond = int(bond.IsInRing())\n",
        "        conjugate = int(bond.GetIsConjugated())\n",
        "\n",
        "        attr = bond_one_hot + stereo_one_hot + [ring_bond, conjugate]\n",
        "        edge_attr_list.extend([attr, attr])\n",
        "\n",
        "    # Node attribute calculation\n",
        "    node_attr_list = []\n",
        "    for atm_id in range(n_atoms):\n",
        "        atm = mol2.GetAtomWithIdx(atm_id)\n",
        "        sym = atm.GetSymbol()\n",
        "\n",
        "        if sym not in valid_atoms:\n",
        "            valid_atoms[sym] = len(valid_atoms)\n",
        "\n",
        "        atm_one_hot = [0] * len(valid_atoms)\n",
        "        atm_one_hot[valid_atoms[sym]] = 1\n",
        "\n",
        "        hybrid_map = {\n",
        "            Chem.HybridizationType.SP3: [1, 0, 0, 0, 0, 0, 0],\n",
        "            Chem.HybridizationType.SP2: [0, 1, 0, 0, 0, 0, 0],\n",
        "            Chem.HybridizationType.SP: [0, 0, 1, 0, 0, 0, 0],\n",
        "            Chem.HybridizationType.S: [0, 0, 0, 1, 0, 0, 0],\n",
        "            Chem.HybridizationType.SP3D: [0, 0, 0, 0, 1, 0, 0],\n",
        "            Chem.HybridizationType.SP3D2: [0, 0, 0, 0, 0, 1, 0]\n",
        "        }\n",
        "        hybrid_one_hot = hybrid_map.get(atm.GetHybridization(), [0, 0, 0, 0, 0, 0, 1])\n",
        "\n",
        "        arom = int(atm.GetIsAromatic())\n",
        "        ring_flag = int(atm.IsInRing())\n",
        "        degree_one_hot = [0] * 6\n",
        "        degree = atm.GetTotalDegree()\n",
        "        degree_one_hot[min(degree, 5)] = 1\n",
        "\n",
        "        hydrogen_one_hot = [0] * 5\n",
        "        num_h = atm.GetTotalNumHs()\n",
        "        hydrogen_one_hot[min(num_h, 4)] = 1\n",
        "\n",
        "        chiral_map = {\n",
        "            Chem.rdchem.ChiralType.CHI_OTHER: [1, 0, 0, 0],\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW: [0, 1, 0, 0],\n",
        "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW: [0, 0, 1, 0],\n",
        "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED: [0, 0, 0, 1]\n",
        "        }\n",
        "        chiral_one_hot = chiral_map.get(atm.GetChiralTag(), [0, 0, 0, 1])\n",
        "\n",
        "        attr = atm_one_hot + hybrid_one_hot + degree_one_hot + hydrogen_one_hot + chiral_one_hot + \\\n",
        "               [arom, ring_flag, atm.GetFormalCharge(), atm.GetNumRadicalElectrons()]\n",
        "\n",
        "        attr = (attr + [0] * FIXED_NODE_ATTR_LENGTH)[:FIXED_NODE_ATTR_LENGTH]\n",
        "        node_attr_list.append(attr)\n",
        "\n",
        "    # Convert to tensors\n",
        "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
        "    node_attr = torch.tensor(node_attr_list, dtype=torch.float)\n",
        "\n",
        "    if edge_index.numel() > 0:\n",
        "        # Ensure `edge_weight` matches `edge_index`\n",
        "        if len(edge_weight_list) != edge_index.size(1):\n",
        "            print(f\"[Warning] Edge weight and edge index size mismatch: {len(edge_weight_list)} vs {edge_index.size(1)}. Resetting edge_weight.\")\n",
        "            edge_weight = torch.ones(edge_index.size(1))\n",
        "        else:\n",
        "            edge_weight = torch.tensor(edge_weight_list, dtype=torch.float)\n",
        "    else:\n",
        "        edge_weight = torch.tensor([])  # Empty tensor if no edges are present\n",
        "\n",
        "    pos = None\n",
        "    if use_pos and AllChem.EmbedMolecule(mol2) == 0:\n",
        "        pos = torch.tensor([[atm_pos.x, atm_pos.y, atm_pos.z]\n",
        "                            for atm_pos in [mol2.GetConformer(0).GetAtomPosition(i) for i in range(n_atoms)]], dtype=torch.float)\n",
        "\n",
        "    return edge_index, node_attr, edge_attr, pos, edge_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MolecularGraphDataset(Dataset):\n",
        "    def __init__(self, smiles_list, targets, use_pos=False):\n",
        "        self.smiles_list = smiles_list\n",
        "        self.targets = targets\n",
        "        self.use_pos = use_pos\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles_list[idx]\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        if mol is None:\n",
        "            raise ValueError(f\"Invalid SMILES string at index {idx}: {smiles}\")\n",
        "\n",
        "        edge_index, node_attr, edge_attr, pos, edge_weight = convert_mol_to_graph(mol, use_pos=self.use_pos)\n",
        "\n",
        "        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "        return edge_index, node_attr, edge_attr, pos, edge_weight, target\n"
      ],
      "metadata": {
        "id": "e0kj6CSuS4nU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hndnj7OKYJgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader  # Updated import\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "regression_data = pd.read_csv(\"B3DB_regression.tsv\", sep=\"\\t\").dropna(subset=['SMILES', 'logBB'])\n",
        "objective = [(Chem.MolFromSmiles(smiles), logBB) for smiles, logBB in zip(regression_data['SMILES'], regression_data['logBB'])]\n",
        "\n",
        "# Convert each molecule to a graph and store in Data objects\n",
        "data_list = []\n",
        "for mol, score in tqdm(objective):\n",
        "    result = convert_mol_to_graph(mol)\n",
        "    if result is None:\n",
        "        continue\n",
        "\n",
        "    # Extract graph components\n",
        "    edge_index, node_attr, edge_attr, pos, edge_weight = result\n",
        "    y = torch.tensor([[score]], dtype=torch.float)\n",
        "\n",
        "    # Create a PyTorch Geometric Data object\n",
        "    data_obj = Data(x=node_attr, edge_index=edge_index, edge_attr=edge_attr, pos=pos, edge_weight=edge_weight, y=y)\n",
        "    data_list.append(data_obj)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "n_data = len(data_list)\n",
        "n_train = int(n_data * 0.8)\n",
        "n_test = int(n_data * 0.1)\n",
        "\n",
        "train_set = data_list[:n_train]\n",
        "val_set = data_list[n_train:n_train + n_test]\n",
        "test_set = data_list[n_train + n_test:]\n",
        "\n",
        "print(f\"Number of training set: {len(train_set)}\")\n",
        "print(f\"Number of validation set: {len(val_set)}\")\n",
        "print(f\"Number of test set: {len(test_set)}\")\n",
        "\n",
        "# Create PyTorch Geometric DataLoaders\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn3 = BatchNorm(hidden_dim)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn_fc = BatchNorm(hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        # Convolutional Layer 1\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Convolutional Layer 2\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Convolutional Layer 3\n",
        "        x = self.conv3(x, edge_index, edge_weight)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # shape: [batch_size, hidden_dim]\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)  # shape: [batch_size, 1]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Model setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "node_input_dim = 30  # Assuming 35-dimensional node features as per your `convert_mol_to_graph` function\n",
        "hidden_dim = 64      # Example hidden dimension, can be tuned\n",
        "model = GNN(input_dim = node_input_dim, hidden_dim=hidden_dim).to(device)\n",
        "\n",
        "# Training configuration\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "from torch_geometric.data import DataLoader\n",
        "def train_gnn_model(model, train_loader, validation_loader, optimizer, loss_fn, num_epochs=100, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    train_loss_list = []\n",
        "    validation_loss_list = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            if data.edge_index.size(1) != data.edge_weight.size(0):\n",
        "                print(f\"[Warning] Edge index and weight mismatch in batch. Adjusting edge_weight.\")\n",
        "                data.edge_weight = torch.ones(data.edge_index.size(1), device=device)  # Fallback\n",
        "\n",
        "            print(f\"\\n[DEBUG] Training batch - data object properties:\")\n",
        "            print(f\"x shape: {data.x.shape}\")\n",
        "            print(f\"edge_index shape: {data.edge_index.shape}\")\n",
        "            print(f\"edge_attr shape: {data.edge_attr.shape}\")\n",
        "            print(f\"edge_weight shape: {data.edge_weight.shape}\")\n",
        "            print(f\"y shape: {data.y.shape}\")\n",
        "\n",
        "            # Verify edge consistency\n",
        "            num_edges = data.edge_index.shape[1]\n",
        "            num_edge_weights = data.edge_weight.shape[0]\n",
        "            if num_edges != num_edge_weights:\n",
        "                print(f\"[ERROR] Edge weight count {num_edge_weights} does not match edge index count {num_edges}\")\n",
        "                continue  # Skip this batch to prevent runtime error\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "\n",
        "            # Check output consistency\n",
        "            if output.shape[0] != data.y.shape[0]:\n",
        "                print(f\"[ERROR] Output shape {output.shape} does not match target shape {data.y.shape}\")\n",
        "                continue  # Skip this batch if there's a mismatch\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(output, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in validation_loader:\n",
        "                data = data.to(device)\n",
        "                output = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "                loss = loss_fn(output, data.y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(validation_loader)\n",
        "        validation_loss_list.append(avg_val_loss)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_without_improvement = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"Early stopping on epoch {epoch+1}\")\n",
        "                # Load the best model\n",
        "                model.load_state_dict(torch.load('best_model.pt'))\n",
        "                break\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"[DEBUG] Epoch: {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Load the best model at the end (if not already loaded)\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return train_loss_list, validation_loss_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_loss_list, validation_loss_list = train_gnn_model(model, train_loader, test_loader, optimizer, loss_fn, num_epochs=100)\n",
        "\n",
        "# Final test evaluation\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        preds = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(data.y.cpu().numpy())\n",
        "        test_loss += loss_fn(preds, data.y).item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
        "print(f\"Test R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atCSkGvrS98_",
        "outputId": "b25bee3e-ed99-45c8-ba38-d09d885ff818"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1058/1058 [00:00<00:00, 1272.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "edge_attr shape: torch.Size([1600, 12])\n",
            "edge_weight shape: torch.Size([1600])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1644])\n",
            "edge_attr shape: torch.Size([1644, 12])\n",
            "edge_weight shape: torch.Size([1644])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([783, 30])\n",
            "edge_index shape: torch.Size([2, 1698])\n",
            "edge_attr shape: torch.Size([1698, 12])\n",
            "edge_weight shape: torch.Size([1698])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([698, 30])\n",
            "edge_index shape: torch.Size([2, 1486])\n",
            "edge_attr shape: torch.Size([1486, 12])\n",
            "edge_weight shape: torch.Size([1486])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([737, 30])\n",
            "edge_index shape: torch.Size([2, 1600])\n",
            "edge_attr shape: torch.Size([1600, 12])\n",
            "edge_weight shape: torch.Size([1600])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([706, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([721, 30])\n",
            "edge_index shape: torch.Size([2, 1576])\n",
            "edge_attr shape: torch.Size([1576, 12])\n",
            "edge_weight shape: torch.Size([1576])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([701, 30])\n",
            "edge_index shape: torch.Size([2, 1502])\n",
            "edge_attr shape: torch.Size([1502, 12])\n",
            "edge_weight shape: torch.Size([1502])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([717, 30])\n",
            "edge_index shape: torch.Size([2, 1552])\n",
            "edge_attr shape: torch.Size([1552, 12])\n",
            "edge_weight shape: torch.Size([1552])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([783, 30])\n",
            "edge_index shape: torch.Size([2, 1692])\n",
            "edge_attr shape: torch.Size([1692, 12])\n",
            "edge_weight shape: torch.Size([1692])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([620, 30])\n",
            "edge_index shape: torch.Size([2, 1320])\n",
            "edge_attr shape: torch.Size([1320, 12])\n",
            "edge_weight shape: torch.Size([1320])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([293, 30])\n",
            "edge_index shape: torch.Size([2, 628])\n",
            "edge_attr shape: torch.Size([628, 12])\n",
            "edge_weight shape: torch.Size([628])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([685, 30])\n",
            "edge_index shape: torch.Size([2, 1474])\n",
            "edge_attr shape: torch.Size([1474, 12])\n",
            "edge_weight shape: torch.Size([1474])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([743, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([799, 30])\n",
            "edge_index shape: torch.Size([2, 1758])\n",
            "edge_attr shape: torch.Size([1758, 12])\n",
            "edge_weight shape: torch.Size([1758])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([824, 30])\n",
            "edge_index shape: torch.Size([2, 1770])\n",
            "edge_attr shape: torch.Size([1770, 12])\n",
            "edge_weight shape: torch.Size([1770])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1668])\n",
            "edge_attr shape: torch.Size([1668, 12])\n",
            "edge_weight shape: torch.Size([1668])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([737, 30])\n",
            "edge_index shape: torch.Size([2, 1606])\n",
            "edge_attr shape: torch.Size([1606, 12])\n",
            "edge_weight shape: torch.Size([1606])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([735, 30])\n",
            "edge_index shape: torch.Size([2, 1608])\n",
            "edge_attr shape: torch.Size([1608, 12])\n",
            "edge_weight shape: torch.Size([1608])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([725, 30])\n",
            "edge_index shape: torch.Size([2, 1572])\n",
            "edge_attr shape: torch.Size([1572, 12])\n",
            "edge_weight shape: torch.Size([1572])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([700, 30])\n",
            "edge_index shape: torch.Size([2, 1504])\n",
            "edge_attr shape: torch.Size([1504, 12])\n",
            "edge_weight shape: torch.Size([1504])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([723, 30])\n",
            "edge_index shape: torch.Size([2, 1574])\n",
            "edge_attr shape: torch.Size([1574, 12])\n",
            "edge_weight shape: torch.Size([1574])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([803, 30])\n",
            "edge_index shape: torch.Size([2, 1734])\n",
            "edge_attr shape: torch.Size([1734, 12])\n",
            "edge_weight shape: torch.Size([1734])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([754, 30])\n",
            "edge_index shape: torch.Size([2, 1602])\n",
            "edge_attr shape: torch.Size([1602, 12])\n",
            "edge_weight shape: torch.Size([1602])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([714, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([613, 30])\n",
            "edge_index shape: torch.Size([2, 1320])\n",
            "edge_attr shape: torch.Size([1320, 12])\n",
            "edge_weight shape: torch.Size([1320])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([710, 30])\n",
            "edge_index shape: torch.Size([2, 1516])\n",
            "edge_attr shape: torch.Size([1516, 12])\n",
            "edge_weight shape: torch.Size([1516])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([723, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([795, 30])\n",
            "edge_index shape: torch.Size([2, 1684])\n",
            "edge_attr shape: torch.Size([1684, 12])\n",
            "edge_weight shape: torch.Size([1684])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([747, 30])\n",
            "edge_index shape: torch.Size([2, 1634])\n",
            "edge_attr shape: torch.Size([1634, 12])\n",
            "edge_weight shape: torch.Size([1634])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([636, 30])\n",
            "edge_index shape: torch.Size([2, 1356])\n",
            "edge_attr shape: torch.Size([1356, 12])\n",
            "edge_weight shape: torch.Size([1356])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([688, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([797, 30])\n",
            "edge_index shape: torch.Size([2, 1706])\n",
            "edge_attr shape: torch.Size([1706, 12])\n",
            "edge_weight shape: torch.Size([1706])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([1007, 30])\n",
            "edge_index shape: torch.Size([2, 2170])\n",
            "edge_attr shape: torch.Size([2170, 12])\n",
            "edge_weight shape: torch.Size([2170])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([758, 30])\n",
            "edge_index shape: torch.Size([2, 1656])\n",
            "edge_attr shape: torch.Size([1656, 12])\n",
            "edge_weight shape: torch.Size([1656])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([701, 30])\n",
            "edge_index shape: torch.Size([2, 1500])\n",
            "edge_attr shape: torch.Size([1500, 12])\n",
            "edge_weight shape: torch.Size([1500])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([364, 30])\n",
            "edge_index shape: torch.Size([2, 786])\n",
            "edge_attr shape: torch.Size([786, 12])\n",
            "edge_weight shape: torch.Size([786])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([789, 30])\n",
            "edge_index shape: torch.Size([2, 1680])\n",
            "edge_attr shape: torch.Size([1680, 12])\n",
            "edge_weight shape: torch.Size([1680])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([727, 30])\n",
            "edge_index shape: torch.Size([2, 1556])\n",
            "edge_attr shape: torch.Size([1556, 12])\n",
            "edge_weight shape: torch.Size([1556])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([639, 30])\n",
            "edge_index shape: torch.Size([2, 1386])\n",
            "edge_attr shape: torch.Size([1386, 12])\n",
            "edge_weight shape: torch.Size([1386])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([722, 30])\n",
            "edge_index shape: torch.Size([2, 1562])\n",
            "edge_attr shape: torch.Size([1562, 12])\n",
            "edge_weight shape: torch.Size([1562])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([746, 30])\n",
            "edge_index shape: torch.Size([2, 1606])\n",
            "edge_attr shape: torch.Size([1606, 12])\n",
            "edge_weight shape: torch.Size([1606])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([720, 30])\n",
            "edge_index shape: torch.Size([2, 1554])\n",
            "edge_attr shape: torch.Size([1554, 12])\n",
            "edge_weight shape: torch.Size([1554])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([690, 30])\n",
            "edge_index shape: torch.Size([2, 1474])\n",
            "edge_attr shape: torch.Size([1474, 12])\n",
            "edge_weight shape: torch.Size([1474])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([746, 30])\n",
            "edge_index shape: torch.Size([2, 1618])\n",
            "edge_attr shape: torch.Size([1618, 12])\n",
            "edge_weight shape: torch.Size([1618])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([782, 30])\n",
            "edge_index shape: torch.Size([2, 1694])\n",
            "edge_attr shape: torch.Size([1694, 12])\n",
            "edge_weight shape: torch.Size([1694])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([652, 30])\n",
            "edge_index shape: torch.Size([2, 1390])\n",
            "edge_attr shape: torch.Size([1390, 12])\n",
            "edge_weight shape: torch.Size([1390])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([812, 30])\n",
            "edge_index shape: torch.Size([2, 1740])\n",
            "edge_attr shape: torch.Size([1740, 12])\n",
            "edge_weight shape: torch.Size([1740])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([911, 30])\n",
            "edge_index shape: torch.Size([2, 1998])\n",
            "edge_attr shape: torch.Size([1998, 12])\n",
            "edge_weight shape: torch.Size([1998])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([706, 30])\n",
            "edge_index shape: torch.Size([2, 1528])\n",
            "edge_attr shape: torch.Size([1528, 12])\n",
            "edge_weight shape: torch.Size([1528])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1646])\n",
            "edge_attr shape: torch.Size([1646, 12])\n",
            "edge_weight shape: torch.Size([1646])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([728, 30])\n",
            "edge_index shape: torch.Size([2, 1574])\n",
            "edge_attr shape: torch.Size([1574, 12])\n",
            "edge_weight shape: torch.Size([1574])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([772, 30])\n",
            "edge_index shape: torch.Size([2, 1688])\n",
            "edge_attr shape: torch.Size([1688, 12])\n",
            "edge_weight shape: torch.Size([1688])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([792, 30])\n",
            "edge_index shape: torch.Size([2, 1716])\n",
            "edge_attr shape: torch.Size([1716, 12])\n",
            "edge_weight shape: torch.Size([1716])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([726, 30])\n",
            "edge_index shape: torch.Size([2, 1574])\n",
            "edge_attr shape: torch.Size([1574, 12])\n",
            "edge_weight shape: torch.Size([1574])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([679, 30])\n",
            "edge_index shape: torch.Size([2, 1462])\n",
            "edge_attr shape: torch.Size([1462, 12])\n",
            "edge_weight shape: torch.Size([1462])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([737, 30])\n",
            "edge_index shape: torch.Size([2, 1592])\n",
            "edge_attr shape: torch.Size([1592, 12])\n",
            "edge_weight shape: torch.Size([1592])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([820, 30])\n",
            "edge_index shape: torch.Size([2, 1756])\n",
            "edge_attr shape: torch.Size([1756, 12])\n",
            "edge_weight shape: torch.Size([1756])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([734, 30])\n",
            "edge_index shape: torch.Size([2, 1578])\n",
            "edge_attr shape: torch.Size([1578, 12])\n",
            "edge_weight shape: torch.Size([1578])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([969, 30])\n",
            "edge_index shape: torch.Size([2, 2078])\n",
            "edge_attr shape: torch.Size([2078, 12])\n",
            "edge_weight shape: torch.Size([2078])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([641, 30])\n",
            "edge_index shape: torch.Size([2, 1370])\n",
            "edge_attr shape: torch.Size([1370, 12])\n",
            "edge_weight shape: torch.Size([1370])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([744, 30])\n",
            "edge_index shape: torch.Size([2, 1630])\n",
            "edge_attr shape: torch.Size([1630, 12])\n",
            "edge_weight shape: torch.Size([1630])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([717, 30])\n",
            "edge_index shape: torch.Size([2, 1544])\n",
            "edge_attr shape: torch.Size([1544, 12])\n",
            "edge_weight shape: torch.Size([1544])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([249, 30])\n",
            "edge_index shape: torch.Size([2, 538])\n",
            "edge_attr shape: torch.Size([538, 12])\n",
            "edge_weight shape: torch.Size([538])\n",
            "y shape: torch.Size([14, 1])\n",
            "[DEBUG] Epoch: 31/100, Training Loss: 0.3919, Validation Loss: 0.3616\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([686, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([741, 30])\n",
            "edge_index shape: torch.Size([2, 1614])\n",
            "edge_attr shape: torch.Size([1614, 12])\n",
            "edge_weight shape: torch.Size([1614])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1632])\n",
            "edge_attr shape: torch.Size([1632, 12])\n",
            "edge_weight shape: torch.Size([1632])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([842, 30])\n",
            "edge_index shape: torch.Size([2, 1832])\n",
            "edge_attr shape: torch.Size([1832, 12])\n",
            "edge_weight shape: torch.Size([1832])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([657, 30])\n",
            "edge_index shape: torch.Size([2, 1408])\n",
            "edge_attr shape: torch.Size([1408, 12])\n",
            "edge_weight shape: torch.Size([1408])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([840, 30])\n",
            "edge_index shape: torch.Size([2, 1770])\n",
            "edge_attr shape: torch.Size([1770, 12])\n",
            "edge_weight shape: torch.Size([1770])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([793, 30])\n",
            "edge_index shape: torch.Size([2, 1704])\n",
            "edge_attr shape: torch.Size([1704, 12])\n",
            "edge_weight shape: torch.Size([1704])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([709, 30])\n",
            "edge_index shape: torch.Size([2, 1536])\n",
            "edge_attr shape: torch.Size([1536, 12])\n",
            "edge_weight shape: torch.Size([1536])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([867, 30])\n",
            "edge_index shape: torch.Size([2, 1884])\n",
            "edge_attr shape: torch.Size([1884, 12])\n",
            "edge_weight shape: torch.Size([1884])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([692, 30])\n",
            "edge_index shape: torch.Size([2, 1500])\n",
            "edge_attr shape: torch.Size([1500, 12])\n",
            "edge_weight shape: torch.Size([1500])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([605, 30])\n",
            "edge_index shape: torch.Size([2, 1296])\n",
            "edge_attr shape: torch.Size([1296, 12])\n",
            "edge_weight shape: torch.Size([1296])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([818, 30])\n",
            "edge_index shape: torch.Size([2, 1748])\n",
            "edge_attr shape: torch.Size([1748, 12])\n",
            "edge_weight shape: torch.Size([1748])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([663, 30])\n",
            "edge_index shape: torch.Size([2, 1436])\n",
            "edge_attr shape: torch.Size([1436, 12])\n",
            "edge_weight shape: torch.Size([1436])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([697, 30])\n",
            "edge_index shape: torch.Size([2, 1502])\n",
            "edge_attr shape: torch.Size([1502, 12])\n",
            "edge_weight shape: torch.Size([1502])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([858, 30])\n",
            "edge_index shape: torch.Size([2, 1878])\n",
            "edge_attr shape: torch.Size([1878, 12])\n",
            "edge_weight shape: torch.Size([1878])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([668, 30])\n",
            "edge_index shape: torch.Size([2, 1434])\n",
            "edge_attr shape: torch.Size([1434, 12])\n",
            "edge_weight shape: torch.Size([1434])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([724, 30])\n",
            "edge_index shape: torch.Size([2, 1578])\n",
            "edge_attr shape: torch.Size([1578, 12])\n",
            "edge_weight shape: torch.Size([1578])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([682, 30])\n",
            "edge_index shape: torch.Size([2, 1454])\n",
            "edge_attr shape: torch.Size([1454, 12])\n",
            "edge_weight shape: torch.Size([1454])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([700, 30])\n",
            "edge_index shape: torch.Size([2, 1496])\n",
            "edge_attr shape: torch.Size([1496, 12])\n",
            "edge_weight shape: torch.Size([1496])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([793, 30])\n",
            "edge_index shape: torch.Size([2, 1716])\n",
            "edge_attr shape: torch.Size([1716, 12])\n",
            "edge_weight shape: torch.Size([1716])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([750, 30])\n",
            "edge_index shape: torch.Size([2, 1624])\n",
            "edge_attr shape: torch.Size([1624, 12])\n",
            "edge_weight shape: torch.Size([1624])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([823, 30])\n",
            "edge_index shape: torch.Size([2, 1770])\n",
            "edge_attr shape: torch.Size([1770, 12])\n",
            "edge_weight shape: torch.Size([1770])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([796, 30])\n",
            "edge_index shape: torch.Size([2, 1744])\n",
            "edge_attr shape: torch.Size([1744, 12])\n",
            "edge_weight shape: torch.Size([1744])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([683, 30])\n",
            "edge_index shape: torch.Size([2, 1456])\n",
            "edge_attr shape: torch.Size([1456, 12])\n",
            "edge_weight shape: torch.Size([1456])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([835, 30])\n",
            "edge_index shape: torch.Size([2, 1804])\n",
            "edge_attr shape: torch.Size([1804, 12])\n",
            "edge_weight shape: torch.Size([1804])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([320, 30])\n",
            "edge_index shape: torch.Size([2, 704])\n",
            "edge_attr shape: torch.Size([704, 12])\n",
            "edge_weight shape: torch.Size([704])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([700, 30])\n",
            "edge_index shape: torch.Size([2, 1514])\n",
            "edge_attr shape: torch.Size([1514, 12])\n",
            "edge_weight shape: torch.Size([1514])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([706, 30])\n",
            "edge_index shape: torch.Size([2, 1534])\n",
            "edge_attr shape: torch.Size([1534, 12])\n",
            "edge_weight shape: torch.Size([1534])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([702, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([733, 30])\n",
            "edge_index shape: torch.Size([2, 1592])\n",
            "edge_attr shape: torch.Size([1592, 12])\n",
            "edge_weight shape: torch.Size([1592])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([746, 30])\n",
            "edge_index shape: torch.Size([2, 1606])\n",
            "edge_attr shape: torch.Size([1606, 12])\n",
            "edge_weight shape: torch.Size([1606])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([725, 30])\n",
            "edge_index shape: torch.Size([2, 1574])\n",
            "edge_attr shape: torch.Size([1574, 12])\n",
            "edge_weight shape: torch.Size([1574])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([820, 30])\n",
            "edge_index shape: torch.Size([2, 1766])\n",
            "edge_attr shape: torch.Size([1766, 12])\n",
            "edge_weight shape: torch.Size([1766])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([732, 30])\n",
            "edge_index shape: torch.Size([2, 1576])\n",
            "edge_attr shape: torch.Size([1576, 12])\n",
            "edge_weight shape: torch.Size([1576])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([900, 30])\n",
            "edge_index shape: torch.Size([2, 1958])\n",
            "edge_attr shape: torch.Size([1958, 12])\n",
            "edge_weight shape: torch.Size([1958])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1632])\n",
            "edge_attr shape: torch.Size([1632, 12])\n",
            "edge_weight shape: torch.Size([1632])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([765, 30])\n",
            "edge_index shape: torch.Size([2, 1638])\n",
            "edge_attr shape: torch.Size([1638, 12])\n",
            "edge_weight shape: torch.Size([1638])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([687, 30])\n",
            "edge_index shape: torch.Size([2, 1486])\n",
            "edge_attr shape: torch.Size([1486, 12])\n",
            "edge_weight shape: torch.Size([1486])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([681, 30])\n",
            "edge_index shape: torch.Size([2, 1468])\n",
            "edge_attr shape: torch.Size([1468, 12])\n",
            "edge_weight shape: torch.Size([1468])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1610])\n",
            "edge_attr shape: torch.Size([1610, 12])\n",
            "edge_weight shape: torch.Size([1610])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([855, 30])\n",
            "edge_index shape: torch.Size([2, 1870])\n",
            "edge_attr shape: torch.Size([1870, 12])\n",
            "edge_weight shape: torch.Size([1870])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([771, 30])\n",
            "edge_index shape: torch.Size([2, 1684])\n",
            "edge_attr shape: torch.Size([1684, 12])\n",
            "edge_weight shape: torch.Size([1684])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([679, 30])\n",
            "edge_index shape: torch.Size([2, 1466])\n",
            "edge_attr shape: torch.Size([1466, 12])\n",
            "edge_weight shape: torch.Size([1466])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([756, 30])\n",
            "edge_index shape: torch.Size([2, 1644])\n",
            "edge_attr shape: torch.Size([1644, 12])\n",
            "edge_weight shape: torch.Size([1644])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([774, 30])\n",
            "edge_index shape: torch.Size([2, 1696])\n",
            "edge_attr shape: torch.Size([1696, 12])\n",
            "edge_weight shape: torch.Size([1696])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([750, 30])\n",
            "edge_index shape: torch.Size([2, 1608])\n",
            "edge_attr shape: torch.Size([1608, 12])\n",
            "edge_weight shape: torch.Size([1608])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([732, 30])\n",
            "edge_index shape: torch.Size([2, 1550])\n",
            "edge_attr shape: torch.Size([1550, 12])\n",
            "edge_weight shape: torch.Size([1550])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([647, 30])\n",
            "edge_index shape: torch.Size([2, 1366])\n",
            "edge_attr shape: torch.Size([1366, 12])\n",
            "edge_weight shape: torch.Size([1366])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([637, 30])\n",
            "edge_index shape: torch.Size([2, 1350])\n",
            "edge_attr shape: torch.Size([1350, 12])\n",
            "edge_weight shape: torch.Size([1350])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([920, 30])\n",
            "edge_index shape: torch.Size([2, 1974])\n",
            "edge_attr shape: torch.Size([1974, 12])\n",
            "edge_weight shape: torch.Size([1974])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([729, 30])\n",
            "edge_index shape: torch.Size([2, 1574])\n",
            "edge_attr shape: torch.Size([1574, 12])\n",
            "edge_weight shape: torch.Size([1574])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([739, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([316, 30])\n",
            "edge_index shape: torch.Size([2, 696])\n",
            "edge_attr shape: torch.Size([696, 12])\n",
            "edge_weight shape: torch.Size([696])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([674, 30])\n",
            "edge_index shape: torch.Size([2, 1472])\n",
            "edge_attr shape: torch.Size([1472, 12])\n",
            "edge_weight shape: torch.Size([1472])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([754, 30])\n",
            "edge_index shape: torch.Size([2, 1626])\n",
            "edge_attr shape: torch.Size([1626, 12])\n",
            "edge_weight shape: torch.Size([1626])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([642, 30])\n",
            "edge_index shape: torch.Size([2, 1376])\n",
            "edge_attr shape: torch.Size([1376, 12])\n",
            "edge_weight shape: torch.Size([1376])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([741, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([677, 30])\n",
            "edge_index shape: torch.Size([2, 1460])\n",
            "edge_attr shape: torch.Size([1460, 12])\n",
            "edge_weight shape: torch.Size([1460])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([765, 30])\n",
            "edge_index shape: torch.Size([2, 1666])\n",
            "edge_attr shape: torch.Size([1666, 12])\n",
            "edge_weight shape: torch.Size([1666])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([835, 30])\n",
            "edge_index shape: torch.Size([2, 1836])\n",
            "edge_attr shape: torch.Size([1836, 12])\n",
            "edge_weight shape: torch.Size([1836])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([716, 30])\n",
            "edge_index shape: torch.Size([2, 1528])\n",
            "edge_attr shape: torch.Size([1528, 12])\n",
            "edge_weight shape: torch.Size([1528])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([887, 30])\n",
            "edge_index shape: torch.Size([2, 1920])\n",
            "edge_attr shape: torch.Size([1920, 12])\n",
            "edge_weight shape: torch.Size([1920])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([921, 30])\n",
            "edge_index shape: torch.Size([2, 1976])\n",
            "edge_attr shape: torch.Size([1976, 12])\n",
            "edge_weight shape: torch.Size([1976])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([754, 30])\n",
            "edge_index shape: torch.Size([2, 1622])\n",
            "edge_attr shape: torch.Size([1622, 12])\n",
            "edge_weight shape: torch.Size([1622])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([637, 30])\n",
            "edge_index shape: torch.Size([2, 1366])\n",
            "edge_attr shape: torch.Size([1366, 12])\n",
            "edge_weight shape: torch.Size([1366])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([691, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1654])\n",
            "edge_attr shape: torch.Size([1654, 12])\n",
            "edge_weight shape: torch.Size([1654])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([656, 30])\n",
            "edge_index shape: torch.Size([2, 1406])\n",
            "edge_attr shape: torch.Size([1406, 12])\n",
            "edge_weight shape: torch.Size([1406])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([801, 30])\n",
            "edge_index shape: torch.Size([2, 1728])\n",
            "edge_attr shape: torch.Size([1728, 12])\n",
            "edge_weight shape: torch.Size([1728])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([697, 30])\n",
            "edge_index shape: torch.Size([2, 1496])\n",
            "edge_attr shape: torch.Size([1496, 12])\n",
            "edge_weight shape: torch.Size([1496])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([662, 30])\n",
            "edge_index shape: torch.Size([2, 1406])\n",
            "edge_attr shape: torch.Size([1406, 12])\n",
            "edge_weight shape: torch.Size([1406])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1652])\n",
            "edge_attr shape: torch.Size([1652, 12])\n",
            "edge_weight shape: torch.Size([1652])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([712, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([760, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1616])\n",
            "edge_attr shape: torch.Size([1616, 12])\n",
            "edge_weight shape: torch.Size([1616])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([734, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([796, 30])\n",
            "edge_index shape: torch.Size([2, 1714])\n",
            "edge_attr shape: torch.Size([1714, 12])\n",
            "edge_weight shape: torch.Size([1714])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([801, 30])\n",
            "edge_index shape: torch.Size([2, 1756])\n",
            "edge_attr shape: torch.Size([1756, 12])\n",
            "edge_weight shape: torch.Size([1756])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([681, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([442, 30])\n",
            "edge_index shape: torch.Size([2, 962])\n",
            "edge_attr shape: torch.Size([962, 12])\n",
            "edge_weight shape: torch.Size([962])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([740, 30])\n",
            "edge_index shape: torch.Size([2, 1616])\n",
            "edge_attr shape: torch.Size([1616, 12])\n",
            "edge_weight shape: torch.Size([1616])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1662])\n",
            "edge_attr shape: torch.Size([1662, 12])\n",
            "edge_weight shape: torch.Size([1662])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([809, 30])\n",
            "edge_index shape: torch.Size([2, 1736])\n",
            "edge_attr shape: torch.Size([1736, 12])\n",
            "edge_weight shape: torch.Size([1736])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([837, 30])\n",
            "edge_index shape: torch.Size([2, 1780])\n",
            "edge_attr shape: torch.Size([1780, 12])\n",
            "edge_weight shape: torch.Size([1780])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1544])\n",
            "edge_attr shape: torch.Size([1544, 12])\n",
            "edge_weight shape: torch.Size([1544])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([785, 30])\n",
            "edge_index shape: torch.Size([2, 1694])\n",
            "edge_attr shape: torch.Size([1694, 12])\n",
            "edge_weight shape: torch.Size([1694])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([875, 30])\n",
            "edge_index shape: torch.Size([2, 1918])\n",
            "edge_attr shape: torch.Size([1918, 12])\n",
            "edge_weight shape: torch.Size([1918])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([706, 30])\n",
            "edge_index shape: torch.Size([2, 1516])\n",
            "edge_attr shape: torch.Size([1516, 12])\n",
            "edge_weight shape: torch.Size([1516])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([748, 30])\n",
            "edge_index shape: torch.Size([2, 1594])\n",
            "edge_attr shape: torch.Size([1594, 12])\n",
            "edge_weight shape: torch.Size([1594])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([655, 30])\n",
            "edge_index shape: torch.Size([2, 1420])\n",
            "edge_attr shape: torch.Size([1420, 12])\n",
            "edge_weight shape: torch.Size([1420])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([736, 30])\n",
            "edge_index shape: torch.Size([2, 1570])\n",
            "edge_attr shape: torch.Size([1570, 12])\n",
            "edge_weight shape: torch.Size([1570])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([832, 30])\n",
            "edge_index shape: torch.Size([2, 1788])\n",
            "edge_attr shape: torch.Size([1788, 12])\n",
            "edge_weight shape: torch.Size([1788])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([666, 30])\n",
            "edge_index shape: torch.Size([2, 1428])\n",
            "edge_attr shape: torch.Size([1428, 12])\n",
            "edge_weight shape: torch.Size([1428])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([785, 30])\n",
            "edge_index shape: torch.Size([2, 1700])\n",
            "edge_attr shape: torch.Size([1700, 12])\n",
            "edge_weight shape: torch.Size([1700])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([711, 30])\n",
            "edge_index shape: torch.Size([2, 1548])\n",
            "edge_attr shape: torch.Size([1548, 12])\n",
            "edge_weight shape: torch.Size([1548])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([702, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([673, 30])\n",
            "edge_index shape: torch.Size([2, 1452])\n",
            "edge_attr shape: torch.Size([1452, 12])\n",
            "edge_weight shape: torch.Size([1452])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([699, 30])\n",
            "edge_index shape: torch.Size([2, 1518])\n",
            "edge_attr shape: torch.Size([1518, 12])\n",
            "edge_weight shape: torch.Size([1518])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1516])\n",
            "edge_attr shape: torch.Size([1516, 12])\n",
            "edge_weight shape: torch.Size([1516])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([816, 30])\n",
            "edge_index shape: torch.Size([2, 1766])\n",
            "edge_attr shape: torch.Size([1766, 12])\n",
            "edge_weight shape: torch.Size([1766])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([752, 30])\n",
            "edge_index shape: torch.Size([2, 1650])\n",
            "edge_attr shape: torch.Size([1650, 12])\n",
            "edge_weight shape: torch.Size([1650])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([695, 30])\n",
            "edge_index shape: torch.Size([2, 1500])\n",
            "edge_attr shape: torch.Size([1500, 12])\n",
            "edge_weight shape: torch.Size([1500])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([789, 30])\n",
            "edge_index shape: torch.Size([2, 1662])\n",
            "edge_attr shape: torch.Size([1662, 12])\n",
            "edge_weight shape: torch.Size([1662])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([711, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([395, 30])\n",
            "edge_index shape: torch.Size([2, 854])\n",
            "edge_attr shape: torch.Size([854, 12])\n",
            "edge_weight shape: torch.Size([854])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([742, 30])\n",
            "edge_index shape: torch.Size([2, 1606])\n",
            "edge_attr shape: torch.Size([1606, 12])\n",
            "edge_weight shape: torch.Size([1606])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([761, 30])\n",
            "edge_index shape: torch.Size([2, 1654])\n",
            "edge_attr shape: torch.Size([1654, 12])\n",
            "edge_weight shape: torch.Size([1654])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([674, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([673, 30])\n",
            "edge_index shape: torch.Size([2, 1446])\n",
            "edge_attr shape: torch.Size([1446, 12])\n",
            "edge_weight shape: torch.Size([1446])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([1052, 30])\n",
            "edge_index shape: torch.Size([2, 2300])\n",
            "edge_attr shape: torch.Size([2300, 12])\n",
            "edge_weight shape: torch.Size([2300])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([854, 30])\n",
            "edge_index shape: torch.Size([2, 1838])\n",
            "edge_attr shape: torch.Size([1838, 12])\n",
            "edge_weight shape: torch.Size([1838])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([680, 30])\n",
            "edge_index shape: torch.Size([2, 1458])\n",
            "edge_attr shape: torch.Size([1458, 12])\n",
            "edge_weight shape: torch.Size([1458])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([713, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([618, 30])\n",
            "edge_index shape: torch.Size([2, 1328])\n",
            "edge_attr shape: torch.Size([1328, 12])\n",
            "edge_weight shape: torch.Size([1328])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([782, 30])\n",
            "edge_index shape: torch.Size([2, 1692])\n",
            "edge_attr shape: torch.Size([1692, 12])\n",
            "edge_weight shape: torch.Size([1692])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([672, 30])\n",
            "edge_index shape: torch.Size([2, 1434])\n",
            "edge_attr shape: torch.Size([1434, 12])\n",
            "edge_weight shape: torch.Size([1434])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([768, 30])\n",
            "edge_index shape: torch.Size([2, 1644])\n",
            "edge_attr shape: torch.Size([1644, 12])\n",
            "edge_weight shape: torch.Size([1644])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([726, 30])\n",
            "edge_index shape: torch.Size([2, 1572])\n",
            "edge_attr shape: torch.Size([1572, 12])\n",
            "edge_weight shape: torch.Size([1572])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([791, 30])\n",
            "edge_index shape: torch.Size([2, 1686])\n",
            "edge_attr shape: torch.Size([1686, 12])\n",
            "edge_weight shape: torch.Size([1686])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([738, 30])\n",
            "edge_index shape: torch.Size([2, 1614])\n",
            "edge_attr shape: torch.Size([1614, 12])\n",
            "edge_weight shape: torch.Size([1614])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([664, 30])\n",
            "edge_index shape: torch.Size([2, 1422])\n",
            "edge_attr shape: torch.Size([1422, 12])\n",
            "edge_weight shape: torch.Size([1422])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([777, 30])\n",
            "edge_index shape: torch.Size([2, 1690])\n",
            "edge_attr shape: torch.Size([1690, 12])\n",
            "edge_weight shape: torch.Size([1690])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([747, 30])\n",
            "edge_index shape: torch.Size([2, 1616])\n",
            "edge_attr shape: torch.Size([1616, 12])\n",
            "edge_weight shape: torch.Size([1616])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([721, 30])\n",
            "edge_index shape: torch.Size([2, 1554])\n",
            "edge_attr shape: torch.Size([1554, 12])\n",
            "edge_weight shape: torch.Size([1554])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([792, 30])\n",
            "edge_index shape: torch.Size([2, 1708])\n",
            "edge_attr shape: torch.Size([1708, 12])\n",
            "edge_weight shape: torch.Size([1708])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([753, 30])\n",
            "edge_index shape: torch.Size([2, 1610])\n",
            "edge_attr shape: torch.Size([1610, 12])\n",
            "edge_weight shape: torch.Size([1610])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1654])\n",
            "edge_attr shape: torch.Size([1654, 12])\n",
            "edge_weight shape: torch.Size([1654])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([701, 30])\n",
            "edge_index shape: torch.Size([2, 1526])\n",
            "edge_attr shape: torch.Size([1526, 12])\n",
            "edge_weight shape: torch.Size([1526])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([830, 30])\n",
            "edge_index shape: torch.Size([2, 1794])\n",
            "edge_attr shape: torch.Size([1794, 12])\n",
            "edge_weight shape: torch.Size([1794])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([659, 30])\n",
            "edge_index shape: torch.Size([2, 1402])\n",
            "edge_attr shape: torch.Size([1402, 12])\n",
            "edge_weight shape: torch.Size([1402])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([297, 30])\n",
            "edge_index shape: torch.Size([2, 642])\n",
            "edge_attr shape: torch.Size([642, 12])\n",
            "edge_weight shape: torch.Size([642])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([822, 30])\n",
            "edge_index shape: torch.Size([2, 1762])\n",
            "edge_attr shape: torch.Size([1762, 12])\n",
            "edge_weight shape: torch.Size([1762])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([688, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([649, 30])\n",
            "edge_index shape: torch.Size([2, 1382])\n",
            "edge_attr shape: torch.Size([1382, 12])\n",
            "edge_weight shape: torch.Size([1382])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([849, 30])\n",
            "edge_index shape: torch.Size([2, 1828])\n",
            "edge_attr shape: torch.Size([1828, 12])\n",
            "edge_weight shape: torch.Size([1828])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([769, 30])\n",
            "edge_index shape: torch.Size([2, 1652])\n",
            "edge_attr shape: torch.Size([1652, 12])\n",
            "edge_weight shape: torch.Size([1652])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([732, 30])\n",
            "edge_index shape: torch.Size([2, 1592])\n",
            "edge_attr shape: torch.Size([1592, 12])\n",
            "edge_weight shape: torch.Size([1592])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1658])\n",
            "edge_attr shape: torch.Size([1658, 12])\n",
            "edge_weight shape: torch.Size([1658])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([717, 30])\n",
            "edge_index shape: torch.Size([2, 1536])\n",
            "edge_attr shape: torch.Size([1536, 12])\n",
            "edge_weight shape: torch.Size([1536])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([855, 30])\n",
            "edge_index shape: torch.Size([2, 1850])\n",
            "edge_attr shape: torch.Size([1850, 12])\n",
            "edge_weight shape: torch.Size([1850])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1684])\n",
            "edge_attr shape: torch.Size([1684, 12])\n",
            "edge_weight shape: torch.Size([1684])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([713, 30])\n",
            "edge_index shape: torch.Size([2, 1552])\n",
            "edge_attr shape: torch.Size([1552, 12])\n",
            "edge_weight shape: torch.Size([1552])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([725, 30])\n",
            "edge_index shape: torch.Size([2, 1578])\n",
            "edge_attr shape: torch.Size([1578, 12])\n",
            "edge_weight shape: torch.Size([1578])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([756, 30])\n",
            "edge_index shape: torch.Size([2, 1646])\n",
            "edge_attr shape: torch.Size([1646, 12])\n",
            "edge_weight shape: torch.Size([1646])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([691, 30])\n",
            "edge_index shape: torch.Size([2, 1520])\n",
            "edge_attr shape: torch.Size([1520, 12])\n",
            "edge_weight shape: torch.Size([1520])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([906, 30])\n",
            "edge_index shape: torch.Size([2, 1972])\n",
            "edge_attr shape: torch.Size([1972, 12])\n",
            "edge_weight shape: torch.Size([1972])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([695, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([685, 30])\n",
            "edge_index shape: torch.Size([2, 1454])\n",
            "edge_attr shape: torch.Size([1454, 12])\n",
            "edge_weight shape: torch.Size([1454])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([783, 30])\n",
            "edge_index shape: torch.Size([2, 1674])\n",
            "edge_attr shape: torch.Size([1674, 12])\n",
            "edge_weight shape: torch.Size([1674])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1518])\n",
            "edge_attr shape: torch.Size([1518, 12])\n",
            "edge_weight shape: torch.Size([1518])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([608, 30])\n",
            "edge_index shape: torch.Size([2, 1294])\n",
            "edge_attr shape: torch.Size([1294, 12])\n",
            "edge_weight shape: torch.Size([1294])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([832, 30])\n",
            "edge_index shape: torch.Size([2, 1812])\n",
            "edge_attr shape: torch.Size([1812, 12])\n",
            "edge_weight shape: torch.Size([1812])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([739, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([673, 30])\n",
            "edge_index shape: torch.Size([2, 1444])\n",
            "edge_attr shape: torch.Size([1444, 12])\n",
            "edge_weight shape: torch.Size([1444])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([824, 30])\n",
            "edge_index shape: torch.Size([2, 1800])\n",
            "edge_attr shape: torch.Size([1800, 12])\n",
            "edge_weight shape: torch.Size([1800])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([692, 30])\n",
            "edge_index shape: torch.Size([2, 1488])\n",
            "edge_attr shape: torch.Size([1488, 12])\n",
            "edge_weight shape: torch.Size([1488])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([720, 30])\n",
            "edge_index shape: torch.Size([2, 1554])\n",
            "edge_attr shape: torch.Size([1554, 12])\n",
            "edge_weight shape: torch.Size([1554])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([335, 30])\n",
            "edge_index shape: torch.Size([2, 698])\n",
            "edge_attr shape: torch.Size([698, 12])\n",
            "edge_weight shape: torch.Size([698])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([738, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([728, 30])\n",
            "edge_index shape: torch.Size([2, 1546])\n",
            "edge_attr shape: torch.Size([1546, 12])\n",
            "edge_weight shape: torch.Size([1546])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([931, 30])\n",
            "edge_index shape: torch.Size([2, 2020])\n",
            "edge_attr shape: torch.Size([2020, 12])\n",
            "edge_weight shape: torch.Size([2020])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([721, 30])\n",
            "edge_index shape: torch.Size([2, 1550])\n",
            "edge_attr shape: torch.Size([1550, 12])\n",
            "edge_weight shape: torch.Size([1550])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1650])\n",
            "edge_attr shape: torch.Size([1650, 12])\n",
            "edge_weight shape: torch.Size([1650])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([740, 30])\n",
            "edge_index shape: torch.Size([2, 1596])\n",
            "edge_attr shape: torch.Size([1596, 12])\n",
            "edge_weight shape: torch.Size([1596])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([672, 30])\n",
            "edge_index shape: torch.Size([2, 1444])\n",
            "edge_attr shape: torch.Size([1444, 12])\n",
            "edge_weight shape: torch.Size([1444])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([769, 30])\n",
            "edge_index shape: torch.Size([2, 1676])\n",
            "edge_attr shape: torch.Size([1676, 12])\n",
            "edge_weight shape: torch.Size([1676])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([744, 30])\n",
            "edge_index shape: torch.Size([2, 1616])\n",
            "edge_attr shape: torch.Size([1616, 12])\n",
            "edge_weight shape: torch.Size([1616])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([723, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([708, 30])\n",
            "edge_index shape: torch.Size([2, 1542])\n",
            "edge_attr shape: torch.Size([1542, 12])\n",
            "edge_weight shape: torch.Size([1542])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([696, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([661, 30])\n",
            "edge_index shape: torch.Size([2, 1428])\n",
            "edge_attr shape: torch.Size([1428, 12])\n",
            "edge_weight shape: torch.Size([1428])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([698, 30])\n",
            "edge_index shape: torch.Size([2, 1498])\n",
            "edge_attr shape: torch.Size([1498, 12])\n",
            "edge_weight shape: torch.Size([1498])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([640, 30])\n",
            "edge_index shape: torch.Size([2, 1370])\n",
            "edge_attr shape: torch.Size([1370, 12])\n",
            "edge_weight shape: torch.Size([1370])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([846, 30])\n",
            "edge_index shape: torch.Size([2, 1848])\n",
            "edge_attr shape: torch.Size([1848, 12])\n",
            "edge_weight shape: torch.Size([1848])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([833, 30])\n",
            "edge_index shape: torch.Size([2, 1812])\n",
            "edge_attr shape: torch.Size([1812, 12])\n",
            "edge_weight shape: torch.Size([1812])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([643, 30])\n",
            "edge_index shape: torch.Size([2, 1390])\n",
            "edge_attr shape: torch.Size([1390, 12])\n",
            "edge_weight shape: torch.Size([1390])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([703, 30])\n",
            "edge_index shape: torch.Size([2, 1526])\n",
            "edge_attr shape: torch.Size([1526, 12])\n",
            "edge_weight shape: torch.Size([1526])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([806, 30])\n",
            "edge_index shape: torch.Size([2, 1726])\n",
            "edge_attr shape: torch.Size([1726, 12])\n",
            "edge_weight shape: torch.Size([1726])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([800, 30])\n",
            "edge_index shape: torch.Size([2, 1724])\n",
            "edge_attr shape: torch.Size([1724, 12])\n",
            "edge_weight shape: torch.Size([1724])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([810, 30])\n",
            "edge_index shape: torch.Size([2, 1764])\n",
            "edge_attr shape: torch.Size([1764, 12])\n",
            "edge_weight shape: torch.Size([1764])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([850, 30])\n",
            "edge_index shape: torch.Size([2, 1808])\n",
            "edge_attr shape: torch.Size([1808, 12])\n",
            "edge_weight shape: torch.Size([1808])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([800, 30])\n",
            "edge_index shape: torch.Size([2, 1700])\n",
            "edge_attr shape: torch.Size([1700, 12])\n",
            "edge_weight shape: torch.Size([1700])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([668, 30])\n",
            "edge_index shape: torch.Size([2, 1412])\n",
            "edge_attr shape: torch.Size([1412, 12])\n",
            "edge_weight shape: torch.Size([1412])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([758, 30])\n",
            "edge_index shape: torch.Size([2, 1644])\n",
            "edge_attr shape: torch.Size([1644, 12])\n",
            "edge_weight shape: torch.Size([1644])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([262, 30])\n",
            "edge_index shape: torch.Size([2, 564])\n",
            "edge_attr shape: torch.Size([564, 12])\n",
            "edge_weight shape: torch.Size([564])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([804, 30])\n",
            "edge_index shape: torch.Size([2, 1730])\n",
            "edge_attr shape: torch.Size([1730, 12])\n",
            "edge_weight shape: torch.Size([1730])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([760, 30])\n",
            "edge_index shape: torch.Size([2, 1598])\n",
            "edge_attr shape: torch.Size([1598, 12])\n",
            "edge_weight shape: torch.Size([1598])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([898, 30])\n",
            "edge_index shape: torch.Size([2, 1928])\n",
            "edge_attr shape: torch.Size([1928, 12])\n",
            "edge_weight shape: torch.Size([1928])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([726, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1544])\n",
            "edge_attr shape: torch.Size([1544, 12])\n",
            "edge_weight shape: torch.Size([1544])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([738, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([668, 30])\n",
            "edge_index shape: torch.Size([2, 1424])\n",
            "edge_attr shape: torch.Size([1424, 12])\n",
            "edge_weight shape: torch.Size([1424])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([750, 30])\n",
            "edge_index shape: torch.Size([2, 1632])\n",
            "edge_attr shape: torch.Size([1632, 12])\n",
            "edge_weight shape: torch.Size([1632])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([648, 30])\n",
            "edge_index shape: torch.Size([2, 1384])\n",
            "edge_attr shape: torch.Size([1384, 12])\n",
            "edge_weight shape: torch.Size([1384])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([741, 30])\n",
            "edge_index shape: torch.Size([2, 1610])\n",
            "edge_attr shape: torch.Size([1610, 12])\n",
            "edge_weight shape: torch.Size([1610])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([770, 30])\n",
            "edge_index shape: torch.Size([2, 1670])\n",
            "edge_attr shape: torch.Size([1670, 12])\n",
            "edge_weight shape: torch.Size([1670])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([853, 30])\n",
            "edge_index shape: torch.Size([2, 1836])\n",
            "edge_attr shape: torch.Size([1836, 12])\n",
            "edge_weight shape: torch.Size([1836])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([742, 30])\n",
            "edge_index shape: torch.Size([2, 1600])\n",
            "edge_attr shape: torch.Size([1600, 12])\n",
            "edge_weight shape: torch.Size([1600])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([723, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([760, 30])\n",
            "edge_index shape: torch.Size([2, 1652])\n",
            "edge_attr shape: torch.Size([1652, 12])\n",
            "edge_weight shape: torch.Size([1652])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([802, 30])\n",
            "edge_index shape: torch.Size([2, 1762])\n",
            "edge_attr shape: torch.Size([1762, 12])\n",
            "edge_weight shape: torch.Size([1762])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([659, 30])\n",
            "edge_index shape: torch.Size([2, 1410])\n",
            "edge_attr shape: torch.Size([1410, 12])\n",
            "edge_weight shape: torch.Size([1410])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([801, 30])\n",
            "edge_index shape: torch.Size([2, 1738])\n",
            "edge_attr shape: torch.Size([1738, 12])\n",
            "edge_weight shape: torch.Size([1738])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([694, 30])\n",
            "edge_index shape: torch.Size([2, 1486])\n",
            "edge_attr shape: torch.Size([1486, 12])\n",
            "edge_weight shape: torch.Size([1486])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([724, 30])\n",
            "edge_index shape: torch.Size([2, 1534])\n",
            "edge_attr shape: torch.Size([1534, 12])\n",
            "edge_weight shape: torch.Size([1534])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([680, 30])\n",
            "edge_index shape: torch.Size([2, 1472])\n",
            "edge_attr shape: torch.Size([1472, 12])\n",
            "edge_weight shape: torch.Size([1472])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([686, 30])\n",
            "edge_index shape: torch.Size([2, 1494])\n",
            "edge_attr shape: torch.Size([1494, 12])\n",
            "edge_weight shape: torch.Size([1494])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([774, 30])\n",
            "edge_index shape: torch.Size([2, 1678])\n",
            "edge_attr shape: torch.Size([1678, 12])\n",
            "edge_weight shape: torch.Size([1678])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([872, 30])\n",
            "edge_index shape: torch.Size([2, 1886])\n",
            "edge_attr shape: torch.Size([1886, 12])\n",
            "edge_weight shape: torch.Size([1886])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([303, 30])\n",
            "edge_index shape: torch.Size([2, 658])\n",
            "edge_attr shape: torch.Size([658, 12])\n",
            "edge_weight shape: torch.Size([658])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([781, 30])\n",
            "edge_index shape: torch.Size([2, 1704])\n",
            "edge_attr shape: torch.Size([1704, 12])\n",
            "edge_weight shape: torch.Size([1704])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([676, 30])\n",
            "edge_index shape: torch.Size([2, 1468])\n",
            "edge_attr shape: torch.Size([1468, 12])\n",
            "edge_weight shape: torch.Size([1468])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([896, 30])\n",
            "edge_index shape: torch.Size([2, 1950])\n",
            "edge_attr shape: torch.Size([1950, 12])\n",
            "edge_weight shape: torch.Size([1950])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([681, 30])\n",
            "edge_index shape: torch.Size([2, 1458])\n",
            "edge_attr shape: torch.Size([1458, 12])\n",
            "edge_weight shape: torch.Size([1458])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([876, 30])\n",
            "edge_index shape: torch.Size([2, 1904])\n",
            "edge_attr shape: torch.Size([1904, 12])\n",
            "edge_weight shape: torch.Size([1904])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([812, 30])\n",
            "edge_index shape: torch.Size([2, 1748])\n",
            "edge_attr shape: torch.Size([1748, 12])\n",
            "edge_weight shape: torch.Size([1748])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([824, 30])\n",
            "edge_index shape: torch.Size([2, 1796])\n",
            "edge_attr shape: torch.Size([1796, 12])\n",
            "edge_weight shape: torch.Size([1796])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([703, 30])\n",
            "edge_index shape: torch.Size([2, 1516])\n",
            "edge_attr shape: torch.Size([1516, 12])\n",
            "edge_weight shape: torch.Size([1516])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([758, 30])\n",
            "edge_index shape: torch.Size([2, 1602])\n",
            "edge_attr shape: torch.Size([1602, 12])\n",
            "edge_weight shape: torch.Size([1602])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([800, 30])\n",
            "edge_index shape: torch.Size([2, 1706])\n",
            "edge_attr shape: torch.Size([1706, 12])\n",
            "edge_weight shape: torch.Size([1706])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([714, 30])\n",
            "edge_index shape: torch.Size([2, 1548])\n",
            "edge_attr shape: torch.Size([1548, 12])\n",
            "edge_weight shape: torch.Size([1548])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([669, 30])\n",
            "edge_index shape: torch.Size([2, 1436])\n",
            "edge_attr shape: torch.Size([1436, 12])\n",
            "edge_weight shape: torch.Size([1436])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([817, 30])\n",
            "edge_index shape: torch.Size([2, 1788])\n",
            "edge_attr shape: torch.Size([1788, 12])\n",
            "edge_weight shape: torch.Size([1788])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1626])\n",
            "edge_attr shape: torch.Size([1626, 12])\n",
            "edge_weight shape: torch.Size([1626])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([753, 30])\n",
            "edge_index shape: torch.Size([2, 1592])\n",
            "edge_attr shape: torch.Size([1592, 12])\n",
            "edge_weight shape: torch.Size([1592])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([783, 30])\n",
            "edge_index shape: torch.Size([2, 1694])\n",
            "edge_attr shape: torch.Size([1694, 12])\n",
            "edge_weight shape: torch.Size([1694])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([717, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([686, 30])\n",
            "edge_index shape: torch.Size([2, 1496])\n",
            "edge_attr shape: torch.Size([1496, 12])\n",
            "edge_weight shape: torch.Size([1496])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([796, 30])\n",
            "edge_index shape: torch.Size([2, 1706])\n",
            "edge_attr shape: torch.Size([1706, 12])\n",
            "edge_weight shape: torch.Size([1706])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([684, 30])\n",
            "edge_index shape: torch.Size([2, 1472])\n",
            "edge_attr shape: torch.Size([1472, 12])\n",
            "edge_weight shape: torch.Size([1472])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([670, 30])\n",
            "edge_index shape: torch.Size([2, 1428])\n",
            "edge_attr shape: torch.Size([1428, 12])\n",
            "edge_weight shape: torch.Size([1428])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([797, 30])\n",
            "edge_index shape: torch.Size([2, 1726])\n",
            "edge_attr shape: torch.Size([1726, 12])\n",
            "edge_weight shape: torch.Size([1726])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1628])\n",
            "edge_attr shape: torch.Size([1628, 12])\n",
            "edge_weight shape: torch.Size([1628])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([636, 30])\n",
            "edge_index shape: torch.Size([2, 1354])\n",
            "edge_attr shape: torch.Size([1354, 12])\n",
            "edge_weight shape: torch.Size([1354])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([649, 30])\n",
            "edge_index shape: torch.Size([2, 1398])\n",
            "edge_attr shape: torch.Size([1398, 12])\n",
            "edge_weight shape: torch.Size([1398])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([732, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([306, 30])\n",
            "edge_index shape: torch.Size([2, 660])\n",
            "edge_attr shape: torch.Size([660, 12])\n",
            "edge_weight shape: torch.Size([660])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([812, 30])\n",
            "edge_index shape: torch.Size([2, 1744])\n",
            "edge_attr shape: torch.Size([1744, 12])\n",
            "edge_weight shape: torch.Size([1744])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([832, 30])\n",
            "edge_index shape: torch.Size([2, 1802])\n",
            "edge_attr shape: torch.Size([1802, 12])\n",
            "edge_weight shape: torch.Size([1802])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([676, 30])\n",
            "edge_index shape: torch.Size([2, 1440])\n",
            "edge_attr shape: torch.Size([1440, 12])\n",
            "edge_weight shape: torch.Size([1440])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([797, 30])\n",
            "edge_index shape: torch.Size([2, 1706])\n",
            "edge_attr shape: torch.Size([1706, 12])\n",
            "edge_weight shape: torch.Size([1706])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([804, 30])\n",
            "edge_index shape: torch.Size([2, 1750])\n",
            "edge_attr shape: torch.Size([1750, 12])\n",
            "edge_weight shape: torch.Size([1750])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([790, 30])\n",
            "edge_index shape: torch.Size([2, 1696])\n",
            "edge_attr shape: torch.Size([1696, 12])\n",
            "edge_weight shape: torch.Size([1696])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([787, 30])\n",
            "edge_index shape: torch.Size([2, 1698])\n",
            "edge_attr shape: torch.Size([1698, 12])\n",
            "edge_weight shape: torch.Size([1698])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([733, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([758, 30])\n",
            "edge_index shape: torch.Size([2, 1638])\n",
            "edge_attr shape: torch.Size([1638, 12])\n",
            "edge_weight shape: torch.Size([1638])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([775, 30])\n",
            "edge_index shape: torch.Size([2, 1664])\n",
            "edge_attr shape: torch.Size([1664, 12])\n",
            "edge_weight shape: torch.Size([1664])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([834, 30])\n",
            "edge_index shape: torch.Size([2, 1780])\n",
            "edge_attr shape: torch.Size([1780, 12])\n",
            "edge_weight shape: torch.Size([1780])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([631, 30])\n",
            "edge_index shape: torch.Size([2, 1354])\n",
            "edge_attr shape: torch.Size([1354, 12])\n",
            "edge_weight shape: torch.Size([1354])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([793, 30])\n",
            "edge_index shape: torch.Size([2, 1730])\n",
            "edge_attr shape: torch.Size([1730, 12])\n",
            "edge_weight shape: torch.Size([1730])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1626])\n",
            "edge_attr shape: torch.Size([1626, 12])\n",
            "edge_weight shape: torch.Size([1626])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([718, 30])\n",
            "edge_index shape: torch.Size([2, 1550])\n",
            "edge_attr shape: torch.Size([1550, 12])\n",
            "edge_weight shape: torch.Size([1550])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([777, 30])\n",
            "edge_index shape: torch.Size([2, 1680])\n",
            "edge_attr shape: torch.Size([1680, 12])\n",
            "edge_weight shape: torch.Size([1680])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([745, 30])\n",
            "edge_index shape: torch.Size([2, 1626])\n",
            "edge_attr shape: torch.Size([1626, 12])\n",
            "edge_weight shape: torch.Size([1626])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([727, 30])\n",
            "edge_index shape: torch.Size([2, 1580])\n",
            "edge_attr shape: torch.Size([1580, 12])\n",
            "edge_weight shape: torch.Size([1580])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([695, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([709, 30])\n",
            "edge_index shape: torch.Size([2, 1530])\n",
            "edge_attr shape: torch.Size([1530, 12])\n",
            "edge_weight shape: torch.Size([1530])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([699, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([681, 30])\n",
            "edge_index shape: torch.Size([2, 1480])\n",
            "edge_attr shape: torch.Size([1480, 12])\n",
            "edge_weight shape: torch.Size([1480])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([690, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([837, 30])\n",
            "edge_index shape: torch.Size([2, 1836])\n",
            "edge_attr shape: torch.Size([1836, 12])\n",
            "edge_weight shape: torch.Size([1836])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([662, 30])\n",
            "edge_index shape: torch.Size([2, 1422])\n",
            "edge_attr shape: torch.Size([1422, 12])\n",
            "edge_weight shape: torch.Size([1422])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([291, 30])\n",
            "edge_index shape: torch.Size([2, 604])\n",
            "edge_attr shape: torch.Size([604, 12])\n",
            "edge_weight shape: torch.Size([604])\n",
            "y shape: torch.Size([14, 1])\n",
            "[DEBUG] Epoch: 41/100, Training Loss: 0.3778, Validation Loss: 0.3138\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([834, 30])\n",
            "edge_index shape: torch.Size([2, 1770])\n",
            "edge_attr shape: torch.Size([1770, 12])\n",
            "edge_weight shape: torch.Size([1770])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1674])\n",
            "edge_attr shape: torch.Size([1674, 12])\n",
            "edge_weight shape: torch.Size([1674])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1724])\n",
            "edge_attr shape: torch.Size([1724, 12])\n",
            "edge_weight shape: torch.Size([1724])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([716, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([773, 30])\n",
            "edge_index shape: torch.Size([2, 1678])\n",
            "edge_attr shape: torch.Size([1678, 12])\n",
            "edge_weight shape: torch.Size([1678])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([759, 30])\n",
            "edge_index shape: torch.Size([2, 1634])\n",
            "edge_attr shape: torch.Size([1634, 12])\n",
            "edge_weight shape: torch.Size([1634])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([740, 30])\n",
            "edge_index shape: torch.Size([2, 1610])\n",
            "edge_attr shape: torch.Size([1610, 12])\n",
            "edge_weight shape: torch.Size([1610])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([859, 30])\n",
            "edge_index shape: torch.Size([2, 1820])\n",
            "edge_attr shape: torch.Size([1820, 12])\n",
            "edge_weight shape: torch.Size([1820])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([762, 30])\n",
            "edge_index shape: torch.Size([2, 1664])\n",
            "edge_attr shape: torch.Size([1664, 12])\n",
            "edge_weight shape: torch.Size([1664])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([746, 30])\n",
            "edge_index shape: torch.Size([2, 1592])\n",
            "edge_attr shape: torch.Size([1592, 12])\n",
            "edge_weight shape: torch.Size([1592])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([753, 30])\n",
            "edge_index shape: torch.Size([2, 1632])\n",
            "edge_attr shape: torch.Size([1632, 12])\n",
            "edge_weight shape: torch.Size([1632])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([656, 30])\n",
            "edge_index shape: torch.Size([2, 1394])\n",
            "edge_attr shape: torch.Size([1394, 12])\n",
            "edge_weight shape: torch.Size([1394])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([824, 30])\n",
            "edge_index shape: torch.Size([2, 1788])\n",
            "edge_attr shape: torch.Size([1788, 12])\n",
            "edge_weight shape: torch.Size([1788])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([815, 30])\n",
            "edge_index shape: torch.Size([2, 1748])\n",
            "edge_attr shape: torch.Size([1748, 12])\n",
            "edge_weight shape: torch.Size([1748])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([667, 30])\n",
            "edge_index shape: torch.Size([2, 1450])\n",
            "edge_attr shape: torch.Size([1450, 12])\n",
            "edge_weight shape: torch.Size([1450])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([686, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([692, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1628])\n",
            "edge_attr shape: torch.Size([1628, 12])\n",
            "edge_weight shape: torch.Size([1628])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([668, 30])\n",
            "edge_index shape: torch.Size([2, 1436])\n",
            "edge_attr shape: torch.Size([1436, 12])\n",
            "edge_weight shape: torch.Size([1436])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([599, 30])\n",
            "edge_index shape: torch.Size([2, 1280])\n",
            "edge_attr shape: torch.Size([1280, 12])\n",
            "edge_weight shape: torch.Size([1280])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1544])\n",
            "edge_attr shape: torch.Size([1544, 12])\n",
            "edge_weight shape: torch.Size([1544])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([854, 30])\n",
            "edge_index shape: torch.Size([2, 1846])\n",
            "edge_attr shape: torch.Size([1846, 12])\n",
            "edge_weight shape: torch.Size([1846])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([846, 30])\n",
            "edge_index shape: torch.Size([2, 1824])\n",
            "edge_attr shape: torch.Size([1824, 12])\n",
            "edge_weight shape: torch.Size([1824])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([652, 30])\n",
            "edge_index shape: torch.Size([2, 1404])\n",
            "edge_attr shape: torch.Size([1404, 12])\n",
            "edge_weight shape: torch.Size([1404])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([744, 30])\n",
            "edge_index shape: torch.Size([2, 1582])\n",
            "edge_attr shape: torch.Size([1582, 12])\n",
            "edge_weight shape: torch.Size([1582])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1632])\n",
            "edge_attr shape: torch.Size([1632, 12])\n",
            "edge_weight shape: torch.Size([1632])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([301, 30])\n",
            "edge_index shape: torch.Size([2, 648])\n",
            "edge_attr shape: torch.Size([648, 12])\n",
            "edge_weight shape: torch.Size([648])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1640])\n",
            "edge_attr shape: torch.Size([1640, 12])\n",
            "edge_weight shape: torch.Size([1640])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([689, 30])\n",
            "edge_index shape: torch.Size([2, 1458])\n",
            "edge_attr shape: torch.Size([1458, 12])\n",
            "edge_weight shape: torch.Size([1458])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([684, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([740, 30])\n",
            "edge_index shape: torch.Size([2, 1594])\n",
            "edge_attr shape: torch.Size([1594, 12])\n",
            "edge_weight shape: torch.Size([1594])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([597, 30])\n",
            "edge_index shape: torch.Size([2, 1272])\n",
            "edge_attr shape: torch.Size([1272, 12])\n",
            "edge_weight shape: torch.Size([1272])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([756, 30])\n",
            "edge_index shape: torch.Size([2, 1620])\n",
            "edge_attr shape: torch.Size([1620, 12])\n",
            "edge_weight shape: torch.Size([1620])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([824, 30])\n",
            "edge_index shape: torch.Size([2, 1784])\n",
            "edge_attr shape: torch.Size([1784, 12])\n",
            "edge_weight shape: torch.Size([1784])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([806, 30])\n",
            "edge_index shape: torch.Size([2, 1756])\n",
            "edge_attr shape: torch.Size([1756, 12])\n",
            "edge_weight shape: torch.Size([1756])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1696])\n",
            "edge_attr shape: torch.Size([1696, 12])\n",
            "edge_weight shape: torch.Size([1696])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([839, 30])\n",
            "edge_index shape: torch.Size([2, 1786])\n",
            "edge_attr shape: torch.Size([1786, 12])\n",
            "edge_weight shape: torch.Size([1786])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([638, 30])\n",
            "edge_index shape: torch.Size([2, 1362])\n",
            "edge_attr shape: torch.Size([1362, 12])\n",
            "edge_weight shape: torch.Size([1362])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([662, 30])\n",
            "edge_index shape: torch.Size([2, 1428])\n",
            "edge_attr shape: torch.Size([1428, 12])\n",
            "edge_weight shape: torch.Size([1428])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([736, 30])\n",
            "edge_index shape: torch.Size([2, 1608])\n",
            "edge_attr shape: torch.Size([1608, 12])\n",
            "edge_weight shape: torch.Size([1608])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1518])\n",
            "edge_attr shape: torch.Size([1518, 12])\n",
            "edge_weight shape: torch.Size([1518])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([814, 30])\n",
            "edge_index shape: torch.Size([2, 1786])\n",
            "edge_attr shape: torch.Size([1786, 12])\n",
            "edge_weight shape: torch.Size([1786])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1536])\n",
            "edge_attr shape: torch.Size([1536, 12])\n",
            "edge_weight shape: torch.Size([1536])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([830, 30])\n",
            "edge_index shape: torch.Size([2, 1764])\n",
            "edge_attr shape: torch.Size([1764, 12])\n",
            "edge_weight shape: torch.Size([1764])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([701, 30])\n",
            "edge_index shape: torch.Size([2, 1510])\n",
            "edge_attr shape: torch.Size([1510, 12])\n",
            "edge_weight shape: torch.Size([1510])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([1015, 30])\n",
            "edge_index shape: torch.Size([2, 2174])\n",
            "edge_attr shape: torch.Size([2174, 12])\n",
            "edge_weight shape: torch.Size([2174])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([711, 30])\n",
            "edge_index shape: torch.Size([2, 1546])\n",
            "edge_attr shape: torch.Size([1546, 12])\n",
            "edge_weight shape: torch.Size([1546])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([664, 30])\n",
            "edge_index shape: torch.Size([2, 1440])\n",
            "edge_attr shape: torch.Size([1440, 12])\n",
            "edge_weight shape: torch.Size([1440])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([752, 30])\n",
            "edge_index shape: torch.Size([2, 1624])\n",
            "edge_attr shape: torch.Size([1624, 12])\n",
            "edge_weight shape: torch.Size([1624])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([762, 30])\n",
            "edge_index shape: torch.Size([2, 1660])\n",
            "edge_attr shape: torch.Size([1660, 12])\n",
            "edge_weight shape: torch.Size([1660])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([747, 30])\n",
            "edge_index shape: torch.Size([2, 1612])\n",
            "edge_attr shape: torch.Size([1612, 12])\n",
            "edge_weight shape: torch.Size([1612])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([792, 30])\n",
            "edge_index shape: torch.Size([2, 1710])\n",
            "edge_attr shape: torch.Size([1710, 12])\n",
            "edge_weight shape: torch.Size([1710])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([279, 30])\n",
            "edge_index shape: torch.Size([2, 602])\n",
            "edge_attr shape: torch.Size([602, 12])\n",
            "edge_weight shape: torch.Size([602])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([685, 30])\n",
            "edge_index shape: torch.Size([2, 1478])\n",
            "edge_attr shape: torch.Size([1478, 12])\n",
            "edge_weight shape: torch.Size([1478])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([773, 30])\n",
            "edge_index shape: torch.Size([2, 1658])\n",
            "edge_attr shape: torch.Size([1658, 12])\n",
            "edge_weight shape: torch.Size([1658])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1684])\n",
            "edge_attr shape: torch.Size([1684, 12])\n",
            "edge_weight shape: torch.Size([1684])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([754, 30])\n",
            "edge_index shape: torch.Size([2, 1640])\n",
            "edge_attr shape: torch.Size([1640, 12])\n",
            "edge_weight shape: torch.Size([1640])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([635, 30])\n",
            "edge_index shape: torch.Size([2, 1356])\n",
            "edge_attr shape: torch.Size([1356, 12])\n",
            "edge_weight shape: torch.Size([1356])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([841, 30])\n",
            "edge_index shape: torch.Size([2, 1818])\n",
            "edge_attr shape: torch.Size([1818, 12])\n",
            "edge_weight shape: torch.Size([1818])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([780, 30])\n",
            "edge_index shape: torch.Size([2, 1698])\n",
            "edge_attr shape: torch.Size([1698, 12])\n",
            "edge_weight shape: torch.Size([1698])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([741, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([692, 30])\n",
            "edge_index shape: torch.Size([2, 1490])\n",
            "edge_attr shape: torch.Size([1490, 12])\n",
            "edge_weight shape: torch.Size([1490])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([838, 30])\n",
            "edge_index shape: torch.Size([2, 1828])\n",
            "edge_attr shape: torch.Size([1828, 12])\n",
            "edge_weight shape: torch.Size([1828])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([744, 30])\n",
            "edge_index shape: torch.Size([2, 1628])\n",
            "edge_attr shape: torch.Size([1628, 12])\n",
            "edge_weight shape: torch.Size([1628])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([619, 30])\n",
            "edge_index shape: torch.Size([2, 1328])\n",
            "edge_attr shape: torch.Size([1328, 12])\n",
            "edge_weight shape: torch.Size([1328])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([823, 30])\n",
            "edge_index shape: torch.Size([2, 1798])\n",
            "edge_attr shape: torch.Size([1798, 12])\n",
            "edge_weight shape: torch.Size([1798])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([705, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([698, 30])\n",
            "edge_index shape: torch.Size([2, 1494])\n",
            "edge_attr shape: torch.Size([1494, 12])\n",
            "edge_weight shape: torch.Size([1494])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([758, 30])\n",
            "edge_index shape: torch.Size([2, 1622])\n",
            "edge_attr shape: torch.Size([1622, 12])\n",
            "edge_weight shape: torch.Size([1622])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([788, 30])\n",
            "edge_index shape: torch.Size([2, 1674])\n",
            "edge_attr shape: torch.Size([1674, 12])\n",
            "edge_weight shape: torch.Size([1674])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([873, 30])\n",
            "edge_index shape: torch.Size([2, 1906])\n",
            "edge_attr shape: torch.Size([1906, 12])\n",
            "edge_weight shape: torch.Size([1906])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1534])\n",
            "edge_attr shape: torch.Size([1534, 12])\n",
            "edge_weight shape: torch.Size([1534])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([563, 30])\n",
            "edge_index shape: torch.Size([2, 1206])\n",
            "edge_attr shape: torch.Size([1206, 12])\n",
            "edge_weight shape: torch.Size([1206])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([703, 30])\n",
            "edge_index shape: torch.Size([2, 1520])\n",
            "edge_attr shape: torch.Size([1520, 12])\n",
            "edge_weight shape: torch.Size([1520])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([733, 30])\n",
            "edge_index shape: torch.Size([2, 1576])\n",
            "edge_attr shape: torch.Size([1576, 12])\n",
            "edge_weight shape: torch.Size([1576])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([623, 30])\n",
            "edge_index shape: torch.Size([2, 1338])\n",
            "edge_attr shape: torch.Size([1338, 12])\n",
            "edge_weight shape: torch.Size([1338])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([896, 30])\n",
            "edge_index shape: torch.Size([2, 1952])\n",
            "edge_attr shape: torch.Size([1952, 12])\n",
            "edge_weight shape: torch.Size([1952])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([869, 30])\n",
            "edge_index shape: torch.Size([2, 1870])\n",
            "edge_attr shape: torch.Size([1870, 12])\n",
            "edge_weight shape: torch.Size([1870])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([792, 30])\n",
            "edge_index shape: torch.Size([2, 1712])\n",
            "edge_attr shape: torch.Size([1712, 12])\n",
            "edge_weight shape: torch.Size([1712])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([287, 30])\n",
            "edge_index shape: torch.Size([2, 618])\n",
            "edge_attr shape: torch.Size([618, 12])\n",
            "edge_weight shape: torch.Size([618])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([838, 30])\n",
            "edge_index shape: torch.Size([2, 1806])\n",
            "edge_attr shape: torch.Size([1806, 12])\n",
            "edge_weight shape: torch.Size([1806])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([734, 30])\n",
            "edge_index shape: torch.Size([2, 1576])\n",
            "edge_attr shape: torch.Size([1576, 12])\n",
            "edge_weight shape: torch.Size([1576])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([646, 30])\n",
            "edge_index shape: torch.Size([2, 1386])\n",
            "edge_attr shape: torch.Size([1386, 12])\n",
            "edge_weight shape: torch.Size([1386])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([748, 30])\n",
            "edge_index shape: torch.Size([2, 1628])\n",
            "edge_attr shape: torch.Size([1628, 12])\n",
            "edge_weight shape: torch.Size([1628])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([895, 30])\n",
            "edge_index shape: torch.Size([2, 1912])\n",
            "edge_attr shape: torch.Size([1912, 12])\n",
            "edge_weight shape: torch.Size([1912])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([829, 30])\n",
            "edge_index shape: torch.Size([2, 1814])\n",
            "edge_attr shape: torch.Size([1814, 12])\n",
            "edge_weight shape: torch.Size([1814])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([676, 30])\n",
            "edge_index shape: torch.Size([2, 1468])\n",
            "edge_attr shape: torch.Size([1468, 12])\n",
            "edge_weight shape: torch.Size([1468])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([732, 30])\n",
            "edge_index shape: torch.Size([2, 1572])\n",
            "edge_attr shape: torch.Size([1572, 12])\n",
            "edge_weight shape: torch.Size([1572])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([745, 30])\n",
            "edge_index shape: torch.Size([2, 1588])\n",
            "edge_attr shape: torch.Size([1588, 12])\n",
            "edge_weight shape: torch.Size([1588])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([691, 30])\n",
            "edge_index shape: torch.Size([2, 1460])\n",
            "edge_attr shape: torch.Size([1460, 12])\n",
            "edge_weight shape: torch.Size([1460])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1702])\n",
            "edge_attr shape: torch.Size([1702, 12])\n",
            "edge_weight shape: torch.Size([1702])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([710, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([679, 30])\n",
            "edge_index shape: torch.Size([2, 1464])\n",
            "edge_attr shape: torch.Size([1464, 12])\n",
            "edge_weight shape: torch.Size([1464])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([664, 30])\n",
            "edge_index shape: torch.Size([2, 1416])\n",
            "edge_attr shape: torch.Size([1416, 12])\n",
            "edge_weight shape: torch.Size([1416])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([740, 30])\n",
            "edge_index shape: torch.Size([2, 1612])\n",
            "edge_attr shape: torch.Size([1612, 12])\n",
            "edge_weight shape: torch.Size([1612])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([802, 30])\n",
            "edge_index shape: torch.Size([2, 1724])\n",
            "edge_attr shape: torch.Size([1724, 12])\n",
            "edge_weight shape: torch.Size([1724])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([731, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([713, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([779, 30])\n",
            "edge_index shape: torch.Size([2, 1692])\n",
            "edge_attr shape: torch.Size([1692, 12])\n",
            "edge_weight shape: torch.Size([1692])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([712, 30])\n",
            "edge_index shape: torch.Size([2, 1546])\n",
            "edge_attr shape: torch.Size([1546, 12])\n",
            "edge_weight shape: torch.Size([1546])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1542])\n",
            "edge_attr shape: torch.Size([1542, 12])\n",
            "edge_weight shape: torch.Size([1542])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([772, 30])\n",
            "edge_index shape: torch.Size([2, 1666])\n",
            "edge_attr shape: torch.Size([1666, 12])\n",
            "edge_weight shape: torch.Size([1666])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([828, 30])\n",
            "edge_index shape: torch.Size([2, 1808])\n",
            "edge_attr shape: torch.Size([1808, 12])\n",
            "edge_weight shape: torch.Size([1808])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([752, 30])\n",
            "edge_index shape: torch.Size([2, 1640])\n",
            "edge_attr shape: torch.Size([1640, 12])\n",
            "edge_weight shape: torch.Size([1640])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([336, 30])\n",
            "edge_index shape: torch.Size([2, 708])\n",
            "edge_attr shape: torch.Size([708, 12])\n",
            "edge_weight shape: torch.Size([708])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([638, 30])\n",
            "edge_index shape: torch.Size([2, 1366])\n",
            "edge_attr shape: torch.Size([1366, 12])\n",
            "edge_weight shape: torch.Size([1366])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([620, 30])\n",
            "edge_index shape: torch.Size([2, 1318])\n",
            "edge_attr shape: torch.Size([1318, 12])\n",
            "edge_weight shape: torch.Size([1318])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([765, 30])\n",
            "edge_index shape: torch.Size([2, 1642])\n",
            "edge_attr shape: torch.Size([1642, 12])\n",
            "edge_weight shape: torch.Size([1642])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([725, 30])\n",
            "edge_index shape: torch.Size([2, 1566])\n",
            "edge_attr shape: torch.Size([1566, 12])\n",
            "edge_weight shape: torch.Size([1566])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([678, 30])\n",
            "edge_index shape: torch.Size([2, 1470])\n",
            "edge_attr shape: torch.Size([1470, 12])\n",
            "edge_weight shape: torch.Size([1470])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([899, 30])\n",
            "edge_index shape: torch.Size([2, 1920])\n",
            "edge_attr shape: torch.Size([1920, 12])\n",
            "edge_weight shape: torch.Size([1920])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([697, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([773, 30])\n",
            "edge_index shape: torch.Size([2, 1668])\n",
            "edge_attr shape: torch.Size([1668, 12])\n",
            "edge_weight shape: torch.Size([1668])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([675, 30])\n",
            "edge_index shape: torch.Size([2, 1460])\n",
            "edge_attr shape: torch.Size([1460, 12])\n",
            "edge_weight shape: torch.Size([1460])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([769, 30])\n",
            "edge_index shape: torch.Size([2, 1680])\n",
            "edge_attr shape: torch.Size([1680, 12])\n",
            "edge_weight shape: torch.Size([1680])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([771, 30])\n",
            "edge_index shape: torch.Size([2, 1656])\n",
            "edge_attr shape: torch.Size([1656, 12])\n",
            "edge_weight shape: torch.Size([1656])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([821, 30])\n",
            "edge_index shape: torch.Size([2, 1808])\n",
            "edge_attr shape: torch.Size([1808, 12])\n",
            "edge_weight shape: torch.Size([1808])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([813, 30])\n",
            "edge_index shape: torch.Size([2, 1732])\n",
            "edge_attr shape: torch.Size([1732, 12])\n",
            "edge_weight shape: torch.Size([1732])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([672, 30])\n",
            "edge_index shape: torch.Size([2, 1432])\n",
            "edge_attr shape: torch.Size([1432, 12])\n",
            "edge_weight shape: torch.Size([1432])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([814, 30])\n",
            "edge_index shape: torch.Size([2, 1756])\n",
            "edge_attr shape: torch.Size([1756, 12])\n",
            "edge_weight shape: torch.Size([1756])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([771, 30])\n",
            "edge_index shape: torch.Size([2, 1680])\n",
            "edge_attr shape: torch.Size([1680, 12])\n",
            "edge_weight shape: torch.Size([1680])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([625, 30])\n",
            "edge_index shape: torch.Size([2, 1340])\n",
            "edge_attr shape: torch.Size([1340, 12])\n",
            "edge_weight shape: torch.Size([1340])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1514])\n",
            "edge_attr shape: torch.Size([1514, 12])\n",
            "edge_weight shape: torch.Size([1514])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([876, 30])\n",
            "edge_index shape: torch.Size([2, 1904])\n",
            "edge_attr shape: torch.Size([1904, 12])\n",
            "edge_weight shape: torch.Size([1904])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1702])\n",
            "edge_attr shape: torch.Size([1702, 12])\n",
            "edge_weight shape: torch.Size([1702])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([782, 30])\n",
            "edge_index shape: torch.Size([2, 1692])\n",
            "edge_attr shape: torch.Size([1692, 12])\n",
            "edge_weight shape: torch.Size([1692])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([729, 30])\n",
            "edge_index shape: torch.Size([2, 1574])\n",
            "edge_attr shape: torch.Size([1574, 12])\n",
            "edge_weight shape: torch.Size([1574])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([648, 30])\n",
            "edge_index shape: torch.Size([2, 1382])\n",
            "edge_attr shape: torch.Size([1382, 12])\n",
            "edge_weight shape: torch.Size([1382])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([759, 30])\n",
            "edge_index shape: torch.Size([2, 1658])\n",
            "edge_attr shape: torch.Size([1658, 12])\n",
            "edge_weight shape: torch.Size([1658])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([778, 30])\n",
            "edge_index shape: torch.Size([2, 1688])\n",
            "edge_attr shape: torch.Size([1688, 12])\n",
            "edge_weight shape: torch.Size([1688])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([777, 30])\n",
            "edge_index shape: torch.Size([2, 1690])\n",
            "edge_attr shape: torch.Size([1690, 12])\n",
            "edge_weight shape: torch.Size([1690])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([349, 30])\n",
            "edge_index shape: torch.Size([2, 750])\n",
            "edge_attr shape: torch.Size([750, 12])\n",
            "edge_weight shape: torch.Size([750])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([688, 30])\n",
            "edge_index shape: torch.Size([2, 1478])\n",
            "edge_attr shape: torch.Size([1478, 12])\n",
            "edge_weight shape: torch.Size([1478])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1526])\n",
            "edge_attr shape: torch.Size([1526, 12])\n",
            "edge_weight shape: torch.Size([1526])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([916, 30])\n",
            "edge_index shape: torch.Size([2, 1978])\n",
            "edge_attr shape: torch.Size([1978, 12])\n",
            "edge_weight shape: torch.Size([1978])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([773, 30])\n",
            "edge_index shape: torch.Size([2, 1686])\n",
            "edge_attr shape: torch.Size([1686, 12])\n",
            "edge_weight shape: torch.Size([1686])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([633, 30])\n",
            "edge_index shape: torch.Size([2, 1352])\n",
            "edge_attr shape: torch.Size([1352, 12])\n",
            "edge_weight shape: torch.Size([1352])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([669, 30])\n",
            "edge_index shape: torch.Size([2, 1450])\n",
            "edge_attr shape: torch.Size([1450, 12])\n",
            "edge_weight shape: torch.Size([1450])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([786, 30])\n",
            "edge_index shape: torch.Size([2, 1710])\n",
            "edge_attr shape: torch.Size([1710, 12])\n",
            "edge_weight shape: torch.Size([1710])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1648])\n",
            "edge_attr shape: torch.Size([1648, 12])\n",
            "edge_weight shape: torch.Size([1648])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([791, 30])\n",
            "edge_index shape: torch.Size([2, 1716])\n",
            "edge_attr shape: torch.Size([1716, 12])\n",
            "edge_weight shape: torch.Size([1716])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([769, 30])\n",
            "edge_index shape: torch.Size([2, 1668])\n",
            "edge_attr shape: torch.Size([1668, 12])\n",
            "edge_weight shape: torch.Size([1668])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([730, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([847, 30])\n",
            "edge_index shape: torch.Size([2, 1832])\n",
            "edge_attr shape: torch.Size([1832, 12])\n",
            "edge_weight shape: torch.Size([1832])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([736, 30])\n",
            "edge_index shape: torch.Size([2, 1582])\n",
            "edge_attr shape: torch.Size([1582, 12])\n",
            "edge_weight shape: torch.Size([1582])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([796, 30])\n",
            "edge_index shape: torch.Size([2, 1662])\n",
            "edge_attr shape: torch.Size([1662, 12])\n",
            "edge_weight shape: torch.Size([1662])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([762, 30])\n",
            "edge_index shape: torch.Size([2, 1662])\n",
            "edge_attr shape: torch.Size([1662, 12])\n",
            "edge_weight shape: torch.Size([1662])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([685, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([783, 30])\n",
            "edge_index shape: torch.Size([2, 1676])\n",
            "edge_attr shape: torch.Size([1676, 12])\n",
            "edge_weight shape: torch.Size([1676])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1650])\n",
            "edge_attr shape: torch.Size([1650, 12])\n",
            "edge_weight shape: torch.Size([1650])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([656, 30])\n",
            "edge_index shape: torch.Size([2, 1416])\n",
            "edge_attr shape: torch.Size([1416, 12])\n",
            "edge_weight shape: torch.Size([1416])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([801, 30])\n",
            "edge_index shape: torch.Size([2, 1736])\n",
            "edge_attr shape: torch.Size([1736, 12])\n",
            "edge_weight shape: torch.Size([1736])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([757, 30])\n",
            "edge_index shape: torch.Size([2, 1634])\n",
            "edge_attr shape: torch.Size([1634, 12])\n",
            "edge_weight shape: torch.Size([1634])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([683, 30])\n",
            "edge_index shape: torch.Size([2, 1446])\n",
            "edge_attr shape: torch.Size([1446, 12])\n",
            "edge_weight shape: torch.Size([1446])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([801, 30])\n",
            "edge_index shape: torch.Size([2, 1748])\n",
            "edge_attr shape: torch.Size([1748, 12])\n",
            "edge_weight shape: torch.Size([1748])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([727, 30])\n",
            "edge_index shape: torch.Size([2, 1590])\n",
            "edge_attr shape: torch.Size([1590, 12])\n",
            "edge_weight shape: torch.Size([1590])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([609, 30])\n",
            "edge_index shape: torch.Size([2, 1302])\n",
            "edge_attr shape: torch.Size([1302, 12])\n",
            "edge_weight shape: torch.Size([1302])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([312, 30])\n",
            "edge_index shape: torch.Size([2, 674])\n",
            "edge_attr shape: torch.Size([674, 12])\n",
            "edge_weight shape: torch.Size([674])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([782, 30])\n",
            "edge_index shape: torch.Size([2, 1690])\n",
            "edge_attr shape: torch.Size([1690, 12])\n",
            "edge_weight shape: torch.Size([1690])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([669, 30])\n",
            "edge_index shape: torch.Size([2, 1450])\n",
            "edge_attr shape: torch.Size([1450, 12])\n",
            "edge_weight shape: torch.Size([1450])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([711, 30])\n",
            "edge_index shape: torch.Size([2, 1540])\n",
            "edge_attr shape: torch.Size([1540, 12])\n",
            "edge_weight shape: torch.Size([1540])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([675, 30])\n",
            "edge_index shape: torch.Size([2, 1448])\n",
            "edge_attr shape: torch.Size([1448, 12])\n",
            "edge_weight shape: torch.Size([1448])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([874, 30])\n",
            "edge_index shape: torch.Size([2, 1884])\n",
            "edge_attr shape: torch.Size([1884, 12])\n",
            "edge_weight shape: torch.Size([1884])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1614])\n",
            "edge_attr shape: torch.Size([1614, 12])\n",
            "edge_weight shape: torch.Size([1614])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([718, 30])\n",
            "edge_index shape: torch.Size([2, 1544])\n",
            "edge_attr shape: torch.Size([1544, 12])\n",
            "edge_weight shape: torch.Size([1544])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([845, 30])\n",
            "edge_index shape: torch.Size([2, 1832])\n",
            "edge_attr shape: torch.Size([1832, 12])\n",
            "edge_weight shape: torch.Size([1832])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([710, 30])\n",
            "edge_index shape: torch.Size([2, 1524])\n",
            "edge_attr shape: torch.Size([1524, 12])\n",
            "edge_weight shape: torch.Size([1524])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([704, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([791, 30])\n",
            "edge_index shape: torch.Size([2, 1688])\n",
            "edge_attr shape: torch.Size([1688, 12])\n",
            "edge_weight shape: torch.Size([1688])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([823, 30])\n",
            "edge_index shape: torch.Size([2, 1774])\n",
            "edge_attr shape: torch.Size([1774, 12])\n",
            "edge_weight shape: torch.Size([1774])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([695, 30])\n",
            "edge_index shape: torch.Size([2, 1492])\n",
            "edge_attr shape: torch.Size([1492, 12])\n",
            "edge_weight shape: torch.Size([1492])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([727, 30])\n",
            "edge_index shape: torch.Size([2, 1556])\n",
            "edge_attr shape: torch.Size([1556, 12])\n",
            "edge_weight shape: torch.Size([1556])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([775, 30])\n",
            "edge_index shape: torch.Size([2, 1680])\n",
            "edge_attr shape: torch.Size([1680, 12])\n",
            "edge_weight shape: torch.Size([1680])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([784, 30])\n",
            "edge_index shape: torch.Size([2, 1708])\n",
            "edge_attr shape: torch.Size([1708, 12])\n",
            "edge_weight shape: torch.Size([1708])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([799, 30])\n",
            "edge_index shape: torch.Size([2, 1702])\n",
            "edge_attr shape: torch.Size([1702, 12])\n",
            "edge_weight shape: torch.Size([1702])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([694, 30])\n",
            "edge_index shape: torch.Size([2, 1518])\n",
            "edge_attr shape: torch.Size([1518, 12])\n",
            "edge_weight shape: torch.Size([1518])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([735, 30])\n",
            "edge_index shape: torch.Size([2, 1576])\n",
            "edge_attr shape: torch.Size([1576, 12])\n",
            "edge_weight shape: torch.Size([1576])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([736, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1656])\n",
            "edge_attr shape: torch.Size([1656, 12])\n",
            "edge_weight shape: torch.Size([1656])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([700, 30])\n",
            "edge_index shape: torch.Size([2, 1514])\n",
            "edge_attr shape: torch.Size([1514, 12])\n",
            "edge_weight shape: torch.Size([1514])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([732, 30])\n",
            "edge_index shape: torch.Size([2, 1598])\n",
            "edge_attr shape: torch.Size([1598, 12])\n",
            "edge_weight shape: torch.Size([1598])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([653, 30])\n",
            "edge_index shape: torch.Size([2, 1402])\n",
            "edge_attr shape: torch.Size([1402, 12])\n",
            "edge_weight shape: torch.Size([1402])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1626])\n",
            "edge_attr shape: torch.Size([1626, 12])\n",
            "edge_weight shape: torch.Size([1626])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([822, 30])\n",
            "edge_index shape: torch.Size([2, 1782])\n",
            "edge_attr shape: torch.Size([1782, 12])\n",
            "edge_weight shape: torch.Size([1782])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([299, 30])\n",
            "edge_index shape: torch.Size([2, 648])\n",
            "edge_attr shape: torch.Size([648, 12])\n",
            "edge_weight shape: torch.Size([648])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([885, 30])\n",
            "edge_index shape: torch.Size([2, 1926])\n",
            "edge_attr shape: torch.Size([1926, 12])\n",
            "edge_weight shape: torch.Size([1926])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([807, 30])\n",
            "edge_index shape: torch.Size([2, 1742])\n",
            "edge_attr shape: torch.Size([1742, 12])\n",
            "edge_weight shape: torch.Size([1742])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([730, 30])\n",
            "edge_index shape: torch.Size([2, 1586])\n",
            "edge_attr shape: torch.Size([1586, 12])\n",
            "edge_weight shape: torch.Size([1586])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([672, 30])\n",
            "edge_index shape: torch.Size([2, 1442])\n",
            "edge_attr shape: torch.Size([1442, 12])\n",
            "edge_weight shape: torch.Size([1442])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([599, 30])\n",
            "edge_index shape: torch.Size([2, 1242])\n",
            "edge_attr shape: torch.Size([1242, 12])\n",
            "edge_weight shape: torch.Size([1242])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([862, 30])\n",
            "edge_index shape: torch.Size([2, 1870])\n",
            "edge_attr shape: torch.Size([1870, 12])\n",
            "edge_weight shape: torch.Size([1870])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([698, 30])\n",
            "edge_index shape: torch.Size([2, 1512])\n",
            "edge_attr shape: torch.Size([1512, 12])\n",
            "edge_weight shape: torch.Size([1512])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([703, 30])\n",
            "edge_index shape: torch.Size([2, 1504])\n",
            "edge_attr shape: torch.Size([1504, 12])\n",
            "edge_weight shape: torch.Size([1504])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([881, 30])\n",
            "edge_index shape: torch.Size([2, 1920])\n",
            "edge_attr shape: torch.Size([1920, 12])\n",
            "edge_weight shape: torch.Size([1920])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([707, 30])\n",
            "edge_index shape: torch.Size([2, 1528])\n",
            "edge_attr shape: torch.Size([1528, 12])\n",
            "edge_weight shape: torch.Size([1528])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([709, 30])\n",
            "edge_index shape: torch.Size([2, 1536])\n",
            "edge_attr shape: torch.Size([1536, 12])\n",
            "edge_weight shape: torch.Size([1536])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([805, 30])\n",
            "edge_index shape: torch.Size([2, 1738])\n",
            "edge_attr shape: torch.Size([1738, 12])\n",
            "edge_weight shape: torch.Size([1738])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([711, 30])\n",
            "edge_index shape: torch.Size([2, 1526])\n",
            "edge_attr shape: torch.Size([1526, 12])\n",
            "edge_weight shape: torch.Size([1526])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([791, 30])\n",
            "edge_index shape: torch.Size([2, 1682])\n",
            "edge_attr shape: torch.Size([1682, 12])\n",
            "edge_weight shape: torch.Size([1682])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1556])\n",
            "edge_attr shape: torch.Size([1556, 12])\n",
            "edge_weight shape: torch.Size([1556])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([825, 30])\n",
            "edge_index shape: torch.Size([2, 1794])\n",
            "edge_attr shape: torch.Size([1794, 12])\n",
            "edge_weight shape: torch.Size([1794])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([664, 30])\n",
            "edge_index shape: torch.Size([2, 1428])\n",
            "edge_attr shape: torch.Size([1428, 12])\n",
            "edge_weight shape: torch.Size([1428])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([709, 30])\n",
            "edge_index shape: torch.Size([2, 1524])\n",
            "edge_attr shape: torch.Size([1524, 12])\n",
            "edge_weight shape: torch.Size([1524])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([810, 30])\n",
            "edge_index shape: torch.Size([2, 1758])\n",
            "edge_attr shape: torch.Size([1758, 12])\n",
            "edge_weight shape: torch.Size([1758])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([662, 30])\n",
            "edge_index shape: torch.Size([2, 1436])\n",
            "edge_attr shape: torch.Size([1436, 12])\n",
            "edge_weight shape: torch.Size([1436])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([633, 30])\n",
            "edge_index shape: torch.Size([2, 1372])\n",
            "edge_attr shape: torch.Size([1372, 12])\n",
            "edge_weight shape: torch.Size([1372])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([828, 30])\n",
            "edge_index shape: torch.Size([2, 1772])\n",
            "edge_attr shape: torch.Size([1772, 12])\n",
            "edge_weight shape: torch.Size([1772])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([741, 30])\n",
            "edge_index shape: torch.Size([2, 1604])\n",
            "edge_attr shape: torch.Size([1604, 12])\n",
            "edge_weight shape: torch.Size([1604])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([731, 30])\n",
            "edge_index shape: torch.Size([2, 1584])\n",
            "edge_attr shape: torch.Size([1584, 12])\n",
            "edge_weight shape: torch.Size([1584])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([864, 30])\n",
            "edge_index shape: torch.Size([2, 1866])\n",
            "edge_attr shape: torch.Size([1866, 12])\n",
            "edge_weight shape: torch.Size([1866])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([648, 30])\n",
            "edge_index shape: torch.Size([2, 1386])\n",
            "edge_attr shape: torch.Size([1386, 12])\n",
            "edge_weight shape: torch.Size([1386])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([320, 30])\n",
            "edge_index shape: torch.Size([2, 698])\n",
            "edge_attr shape: torch.Size([698, 12])\n",
            "edge_weight shape: torch.Size([698])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([802, 30])\n",
            "edge_index shape: torch.Size([2, 1742])\n",
            "edge_attr shape: torch.Size([1742, 12])\n",
            "edge_weight shape: torch.Size([1742])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([856, 30])\n",
            "edge_index shape: torch.Size([2, 1840])\n",
            "edge_attr shape: torch.Size([1840, 12])\n",
            "edge_weight shape: torch.Size([1840])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([604, 30])\n",
            "edge_index shape: torch.Size([2, 1290])\n",
            "edge_attr shape: torch.Size([1290, 12])\n",
            "edge_weight shape: torch.Size([1290])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([694, 30])\n",
            "edge_index shape: torch.Size([2, 1492])\n",
            "edge_attr shape: torch.Size([1492, 12])\n",
            "edge_weight shape: torch.Size([1492])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([680, 30])\n",
            "edge_index shape: torch.Size([2, 1464])\n",
            "edge_attr shape: torch.Size([1464, 12])\n",
            "edge_weight shape: torch.Size([1464])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1570])\n",
            "edge_attr shape: torch.Size([1570, 12])\n",
            "edge_weight shape: torch.Size([1570])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([713, 30])\n",
            "edge_index shape: torch.Size([2, 1528])\n",
            "edge_attr shape: torch.Size([1528, 12])\n",
            "edge_weight shape: torch.Size([1528])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([734, 30])\n",
            "edge_index shape: torch.Size([2, 1594])\n",
            "edge_attr shape: torch.Size([1594, 12])\n",
            "edge_weight shape: torch.Size([1594])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([872, 30])\n",
            "edge_index shape: torch.Size([2, 1884])\n",
            "edge_attr shape: torch.Size([1884, 12])\n",
            "edge_weight shape: torch.Size([1884])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([695, 30])\n",
            "edge_index shape: torch.Size([2, 1486])\n",
            "edge_attr shape: torch.Size([1486, 12])\n",
            "edge_weight shape: torch.Size([1486])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([721, 30])\n",
            "edge_index shape: torch.Size([2, 1546])\n",
            "edge_attr shape: torch.Size([1546, 12])\n",
            "edge_weight shape: torch.Size([1546])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([744, 30])\n",
            "edge_index shape: torch.Size([2, 1634])\n",
            "edge_attr shape: torch.Size([1634, 12])\n",
            "edge_weight shape: torch.Size([1634])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([712, 30])\n",
            "edge_index shape: torch.Size([2, 1528])\n",
            "edge_attr shape: torch.Size([1528, 12])\n",
            "edge_weight shape: torch.Size([1528])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([809, 30])\n",
            "edge_index shape: torch.Size([2, 1728])\n",
            "edge_attr shape: torch.Size([1728, 12])\n",
            "edge_weight shape: torch.Size([1728])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([948, 30])\n",
            "edge_index shape: torch.Size([2, 2082])\n",
            "edge_attr shape: torch.Size([2082, 12])\n",
            "edge_weight shape: torch.Size([2082])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([702, 30])\n",
            "edge_index shape: torch.Size([2, 1508])\n",
            "edge_attr shape: torch.Size([1508, 12])\n",
            "edge_weight shape: torch.Size([1508])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([620, 30])\n",
            "edge_index shape: torch.Size([2, 1326])\n",
            "edge_attr shape: torch.Size([1326, 12])\n",
            "edge_weight shape: torch.Size([1326])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([773, 30])\n",
            "edge_index shape: torch.Size([2, 1686])\n",
            "edge_attr shape: torch.Size([1686, 12])\n",
            "edge_weight shape: torch.Size([1686])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([841, 30])\n",
            "edge_index shape: torch.Size([2, 1800])\n",
            "edge_attr shape: torch.Size([1800, 12])\n",
            "edge_weight shape: torch.Size([1800])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([770, 30])\n",
            "edge_index shape: torch.Size([2, 1664])\n",
            "edge_attr shape: torch.Size([1664, 12])\n",
            "edge_weight shape: torch.Size([1664])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([880, 30])\n",
            "edge_index shape: torch.Size([2, 1916])\n",
            "edge_attr shape: torch.Size([1916, 12])\n",
            "edge_weight shape: torch.Size([1916])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([686, 30])\n",
            "edge_index shape: torch.Size([2, 1476])\n",
            "edge_attr shape: torch.Size([1476, 12])\n",
            "edge_weight shape: torch.Size([1476])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([742, 30])\n",
            "edge_index shape: torch.Size([2, 1600])\n",
            "edge_attr shape: torch.Size([1600, 12])\n",
            "edge_weight shape: torch.Size([1600])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([668, 30])\n",
            "edge_index shape: torch.Size([2, 1438])\n",
            "edge_attr shape: torch.Size([1438, 12])\n",
            "edge_weight shape: torch.Size([1438])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([762, 30])\n",
            "edge_index shape: torch.Size([2, 1634])\n",
            "edge_attr shape: torch.Size([1634, 12])\n",
            "edge_weight shape: torch.Size([1634])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([630, 30])\n",
            "edge_index shape: torch.Size([2, 1354])\n",
            "edge_attr shape: torch.Size([1354, 12])\n",
            "edge_weight shape: torch.Size([1354])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([337, 30])\n",
            "edge_index shape: torch.Size([2, 722])\n",
            "edge_attr shape: torch.Size([722, 12])\n",
            "edge_weight shape: torch.Size([722])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([713, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([733, 30])\n",
            "edge_index shape: torch.Size([2, 1562])\n",
            "edge_attr shape: torch.Size([1562, 12])\n",
            "edge_weight shape: torch.Size([1562])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([787, 30])\n",
            "edge_index shape: torch.Size([2, 1704])\n",
            "edge_attr shape: torch.Size([1704, 12])\n",
            "edge_weight shape: torch.Size([1704])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([640, 30])\n",
            "edge_index shape: torch.Size([2, 1386])\n",
            "edge_attr shape: torch.Size([1386, 12])\n",
            "edge_weight shape: torch.Size([1386])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([699, 30])\n",
            "edge_index shape: torch.Size([2, 1492])\n",
            "edge_attr shape: torch.Size([1492, 12])\n",
            "edge_weight shape: torch.Size([1492])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([899, 30])\n",
            "edge_index shape: torch.Size([2, 1910])\n",
            "edge_attr shape: torch.Size([1910, 12])\n",
            "edge_weight shape: torch.Size([1910])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([722, 30])\n",
            "edge_index shape: torch.Size([2, 1572])\n",
            "edge_attr shape: torch.Size([1572, 12])\n",
            "edge_weight shape: torch.Size([1572])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1568])\n",
            "edge_attr shape: torch.Size([1568, 12])\n",
            "edge_weight shape: torch.Size([1568])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([730, 30])\n",
            "edge_index shape: torch.Size([2, 1572])\n",
            "edge_attr shape: torch.Size([1572, 12])\n",
            "edge_weight shape: torch.Size([1572])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([752, 30])\n",
            "edge_index shape: torch.Size([2, 1610])\n",
            "edge_attr shape: torch.Size([1610, 12])\n",
            "edge_weight shape: torch.Size([1610])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([897, 30])\n",
            "edge_index shape: torch.Size([2, 1958])\n",
            "edge_attr shape: torch.Size([1958, 12])\n",
            "edge_weight shape: torch.Size([1958])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([805, 30])\n",
            "edge_index shape: torch.Size([2, 1734])\n",
            "edge_attr shape: torch.Size([1734, 12])\n",
            "edge_weight shape: torch.Size([1734])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([689, 30])\n",
            "edge_index shape: torch.Size([2, 1494])\n",
            "edge_attr shape: torch.Size([1494, 12])\n",
            "edge_weight shape: torch.Size([1494])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([767, 30])\n",
            "edge_index shape: torch.Size([2, 1644])\n",
            "edge_attr shape: torch.Size([1644, 12])\n",
            "edge_weight shape: torch.Size([1644])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([700, 30])\n",
            "edge_index shape: torch.Size([2, 1514])\n",
            "edge_attr shape: torch.Size([1514, 12])\n",
            "edge_weight shape: torch.Size([1514])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([698, 30])\n",
            "edge_index shape: torch.Size([2, 1482])\n",
            "edge_attr shape: torch.Size([1482, 12])\n",
            "edge_weight shape: torch.Size([1482])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([636, 30])\n",
            "edge_index shape: torch.Size([2, 1368])\n",
            "edge_attr shape: torch.Size([1368, 12])\n",
            "edge_weight shape: torch.Size([1368])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([725, 30])\n",
            "edge_index shape: torch.Size([2, 1566])\n",
            "edge_attr shape: torch.Size([1566, 12])\n",
            "edge_weight shape: torch.Size([1566])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([689, 30])\n",
            "edge_index shape: torch.Size([2, 1470])\n",
            "edge_attr shape: torch.Size([1470, 12])\n",
            "edge_weight shape: torch.Size([1470])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([738, 30])\n",
            "edge_index shape: torch.Size([2, 1602])\n",
            "edge_attr shape: torch.Size([1602, 12])\n",
            "edge_weight shape: torch.Size([1602])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([794, 30])\n",
            "edge_index shape: torch.Size([2, 1724])\n",
            "edge_attr shape: torch.Size([1724, 12])\n",
            "edge_weight shape: torch.Size([1724])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([697, 30])\n",
            "edge_index shape: torch.Size([2, 1512])\n",
            "edge_attr shape: torch.Size([1512, 12])\n",
            "edge_weight shape: torch.Size([1512])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([777, 30])\n",
            "edge_index shape: torch.Size([2, 1690])\n",
            "edge_attr shape: torch.Size([1690, 12])\n",
            "edge_weight shape: torch.Size([1690])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([865, 30])\n",
            "edge_index shape: torch.Size([2, 1880])\n",
            "edge_attr shape: torch.Size([1880, 12])\n",
            "edge_weight shape: torch.Size([1880])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([716, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([763, 30])\n",
            "edge_index shape: torch.Size([2, 1656])\n",
            "edge_attr shape: torch.Size([1656, 12])\n",
            "edge_weight shape: torch.Size([1656])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([364, 30])\n",
            "edge_index shape: torch.Size([2, 786])\n",
            "edge_attr shape: torch.Size([786, 12])\n",
            "edge_weight shape: torch.Size([786])\n",
            "y shape: torch.Size([14, 1])\n",
            "[DEBUG] Epoch: 51/100, Training Loss: 0.3685, Validation Loss: 0.2464\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([802, 30])\n",
            "edge_index shape: torch.Size([2, 1714])\n",
            "edge_attr shape: torch.Size([1714, 12])\n",
            "edge_weight shape: torch.Size([1714])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([765, 30])\n",
            "edge_index shape: torch.Size([2, 1676])\n",
            "edge_attr shape: torch.Size([1676, 12])\n",
            "edge_weight shape: torch.Size([1676])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([813, 30])\n",
            "edge_index shape: torch.Size([2, 1774])\n",
            "edge_attr shape: torch.Size([1774, 12])\n",
            "edge_weight shape: torch.Size([1774])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([856, 30])\n",
            "edge_index shape: torch.Size([2, 1840])\n",
            "edge_attr shape: torch.Size([1840, 12])\n",
            "edge_weight shape: torch.Size([1840])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([748, 30])\n",
            "edge_index shape: torch.Size([2, 1606])\n",
            "edge_attr shape: torch.Size([1606, 12])\n",
            "edge_weight shape: torch.Size([1606])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([697, 30])\n",
            "edge_index shape: torch.Size([2, 1504])\n",
            "edge_attr shape: torch.Size([1504, 12])\n",
            "edge_weight shape: torch.Size([1504])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([723, 30])\n",
            "edge_index shape: torch.Size([2, 1552])\n",
            "edge_attr shape: torch.Size([1552, 12])\n",
            "edge_weight shape: torch.Size([1552])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([744, 30])\n",
            "edge_index shape: torch.Size([2, 1610])\n",
            "edge_attr shape: torch.Size([1610, 12])\n",
            "edge_weight shape: torch.Size([1610])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([772, 30])\n",
            "edge_index shape: torch.Size([2, 1676])\n",
            "edge_attr shape: torch.Size([1676, 12])\n",
            "edge_weight shape: torch.Size([1676])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([683, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([770, 30])\n",
            "edge_index shape: torch.Size([2, 1650])\n",
            "edge_attr shape: torch.Size([1650, 12])\n",
            "edge_weight shape: torch.Size([1650])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1508])\n",
            "edge_attr shape: torch.Size([1508, 12])\n",
            "edge_weight shape: torch.Size([1508])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([689, 30])\n",
            "edge_index shape: torch.Size([2, 1468])\n",
            "edge_attr shape: torch.Size([1468, 12])\n",
            "edge_weight shape: torch.Size([1468])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([700, 30])\n",
            "edge_index shape: torch.Size([2, 1506])\n",
            "edge_attr shape: torch.Size([1506, 12])\n",
            "edge_weight shape: torch.Size([1506])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([832, 30])\n",
            "edge_index shape: torch.Size([2, 1812])\n",
            "edge_attr shape: torch.Size([1812, 12])\n",
            "edge_weight shape: torch.Size([1812])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1624])\n",
            "edge_attr shape: torch.Size([1624, 12])\n",
            "edge_weight shape: torch.Size([1624])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([697, 30])\n",
            "edge_index shape: torch.Size([2, 1506])\n",
            "edge_attr shape: torch.Size([1506, 12])\n",
            "edge_weight shape: torch.Size([1506])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([693, 30])\n",
            "edge_index shape: torch.Size([2, 1500])\n",
            "edge_attr shape: torch.Size([1500, 12])\n",
            "edge_weight shape: torch.Size([1500])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([747, 30])\n",
            "edge_index shape: torch.Size([2, 1614])\n",
            "edge_attr shape: torch.Size([1614, 12])\n",
            "edge_weight shape: torch.Size([1614])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([671, 30])\n",
            "edge_index shape: torch.Size([2, 1450])\n",
            "edge_attr shape: torch.Size([1450, 12])\n",
            "edge_weight shape: torch.Size([1450])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1658])\n",
            "edge_attr shape: torch.Size([1658, 12])\n",
            "edge_weight shape: torch.Size([1658])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([731, 30])\n",
            "edge_index shape: torch.Size([2, 1570])\n",
            "edge_attr shape: torch.Size([1570, 12])\n",
            "edge_weight shape: torch.Size([1570])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([725, 30])\n",
            "edge_index shape: torch.Size([2, 1568])\n",
            "edge_attr shape: torch.Size([1568, 12])\n",
            "edge_weight shape: torch.Size([1568])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([712, 30])\n",
            "edge_index shape: torch.Size([2, 1558])\n",
            "edge_attr shape: torch.Size([1558, 12])\n",
            "edge_weight shape: torch.Size([1558])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([815, 30])\n",
            "edge_index shape: torch.Size([2, 1766])\n",
            "edge_attr shape: torch.Size([1766, 12])\n",
            "edge_weight shape: torch.Size([1766])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([736, 30])\n",
            "edge_index shape: torch.Size([2, 1588])\n",
            "edge_attr shape: torch.Size([1588, 12])\n",
            "edge_weight shape: torch.Size([1588])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([346, 30])\n",
            "edge_index shape: torch.Size([2, 750])\n",
            "edge_attr shape: torch.Size([750, 12])\n",
            "edge_weight shape: torch.Size([750])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([647, 30])\n",
            "edge_index shape: torch.Size([2, 1382])\n",
            "edge_attr shape: torch.Size([1382, 12])\n",
            "edge_weight shape: torch.Size([1382])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([755, 30])\n",
            "edge_index shape: torch.Size([2, 1628])\n",
            "edge_attr shape: torch.Size([1628, 12])\n",
            "edge_weight shape: torch.Size([1628])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([797, 30])\n",
            "edge_index shape: torch.Size([2, 1734])\n",
            "edge_attr shape: torch.Size([1734, 12])\n",
            "edge_weight shape: torch.Size([1734])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([806, 30])\n",
            "edge_index shape: torch.Size([2, 1718])\n",
            "edge_attr shape: torch.Size([1718, 12])\n",
            "edge_weight shape: torch.Size([1718])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([770, 30])\n",
            "edge_index shape: torch.Size([2, 1670])\n",
            "edge_attr shape: torch.Size([1670, 12])\n",
            "edge_weight shape: torch.Size([1670])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([748, 30])\n",
            "edge_index shape: torch.Size([2, 1638])\n",
            "edge_attr shape: torch.Size([1638, 12])\n",
            "edge_weight shape: torch.Size([1638])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([718, 30])\n",
            "edge_index shape: torch.Size([2, 1522])\n",
            "edge_attr shape: torch.Size([1522, 12])\n",
            "edge_weight shape: torch.Size([1522])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([753, 30])\n",
            "edge_index shape: torch.Size([2, 1620])\n",
            "edge_attr shape: torch.Size([1620, 12])\n",
            "edge_weight shape: torch.Size([1620])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([733, 30])\n",
            "edge_index shape: torch.Size([2, 1568])\n",
            "edge_attr shape: torch.Size([1568, 12])\n",
            "edge_weight shape: torch.Size([1568])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([783, 30])\n",
            "edge_index shape: torch.Size([2, 1682])\n",
            "edge_attr shape: torch.Size([1682, 12])\n",
            "edge_weight shape: torch.Size([1682])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([710, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([726, 30])\n",
            "edge_index shape: torch.Size([2, 1566])\n",
            "edge_attr shape: torch.Size([1566, 12])\n",
            "edge_weight shape: torch.Size([1566])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([755, 30])\n",
            "edge_index shape: torch.Size([2, 1612])\n",
            "edge_attr shape: torch.Size([1612, 12])\n",
            "edge_weight shape: torch.Size([1612])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([662, 30])\n",
            "edge_index shape: torch.Size([2, 1442])\n",
            "edge_attr shape: torch.Size([1442, 12])\n",
            "edge_weight shape: torch.Size([1442])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([768, 30])\n",
            "edge_index shape: torch.Size([2, 1678])\n",
            "edge_attr shape: torch.Size([1678, 12])\n",
            "edge_weight shape: torch.Size([1678])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([771, 30])\n",
            "edge_index shape: torch.Size([2, 1668])\n",
            "edge_attr shape: torch.Size([1668, 12])\n",
            "edge_weight shape: torch.Size([1668])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([721, 30])\n",
            "edge_index shape: torch.Size([2, 1554])\n",
            "edge_attr shape: torch.Size([1554, 12])\n",
            "edge_weight shape: torch.Size([1554])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([699, 30])\n",
            "edge_index shape: torch.Size([2, 1516])\n",
            "edge_attr shape: torch.Size([1516, 12])\n",
            "edge_weight shape: torch.Size([1516])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([711, 30])\n",
            "edge_index shape: torch.Size([2, 1548])\n",
            "edge_attr shape: torch.Size([1548, 12])\n",
            "edge_weight shape: torch.Size([1548])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([717, 30])\n",
            "edge_index shape: torch.Size([2, 1534])\n",
            "edge_attr shape: torch.Size([1534, 12])\n",
            "edge_weight shape: torch.Size([1534])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([640, 30])\n",
            "edge_index shape: torch.Size([2, 1370])\n",
            "edge_attr shape: torch.Size([1370, 12])\n",
            "edge_weight shape: torch.Size([1370])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([734, 30])\n",
            "edge_index shape: torch.Size([2, 1578])\n",
            "edge_attr shape: torch.Size([1578, 12])\n",
            "edge_weight shape: torch.Size([1578])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([974, 30])\n",
            "edge_index shape: torch.Size([2, 2078])\n",
            "edge_attr shape: torch.Size([2078, 12])\n",
            "edge_weight shape: torch.Size([2078])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([796, 30])\n",
            "edge_index shape: torch.Size([2, 1734])\n",
            "edge_attr shape: torch.Size([1734, 12])\n",
            "edge_weight shape: torch.Size([1734])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([696, 30])\n",
            "edge_index shape: torch.Size([2, 1512])\n",
            "edge_attr shape: torch.Size([1512, 12])\n",
            "edge_weight shape: torch.Size([1512])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([775, 30])\n",
            "edge_index shape: torch.Size([2, 1682])\n",
            "edge_attr shape: torch.Size([1682, 12])\n",
            "edge_weight shape: torch.Size([1682])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([349, 30])\n",
            "edge_index shape: torch.Size([2, 760])\n",
            "edge_attr shape: torch.Size([760, 12])\n",
            "edge_weight shape: torch.Size([760])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([715, 30])\n",
            "edge_index shape: torch.Size([2, 1536])\n",
            "edge_attr shape: torch.Size([1536, 12])\n",
            "edge_weight shape: torch.Size([1536])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([964, 30])\n",
            "edge_index shape: torch.Size([2, 2072])\n",
            "edge_attr shape: torch.Size([2072, 12])\n",
            "edge_weight shape: torch.Size([2072])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([880, 30])\n",
            "edge_index shape: torch.Size([2, 1906])\n",
            "edge_attr shape: torch.Size([1906, 12])\n",
            "edge_weight shape: torch.Size([1906])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([731, 30])\n",
            "edge_index shape: torch.Size([2, 1564])\n",
            "edge_attr shape: torch.Size([1564, 12])\n",
            "edge_weight shape: torch.Size([1564])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([709, 30])\n",
            "edge_index shape: torch.Size([2, 1538])\n",
            "edge_attr shape: torch.Size([1538, 12])\n",
            "edge_weight shape: torch.Size([1538])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([651, 30])\n",
            "edge_index shape: torch.Size([2, 1394])\n",
            "edge_attr shape: torch.Size([1394, 12])\n",
            "edge_weight shape: torch.Size([1394])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([722, 30])\n",
            "edge_index shape: torch.Size([2, 1566])\n",
            "edge_attr shape: torch.Size([1566, 12])\n",
            "edge_weight shape: torch.Size([1566])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([758, 30])\n",
            "edge_index shape: torch.Size([2, 1668])\n",
            "edge_attr shape: torch.Size([1668, 12])\n",
            "edge_weight shape: torch.Size([1668])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([761, 30])\n",
            "edge_index shape: torch.Size([2, 1654])\n",
            "edge_attr shape: torch.Size([1654, 12])\n",
            "edge_weight shape: torch.Size([1654])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([663, 30])\n",
            "edge_index shape: torch.Size([2, 1432])\n",
            "edge_attr shape: torch.Size([1432, 12])\n",
            "edge_weight shape: torch.Size([1432])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([669, 30])\n",
            "edge_index shape: torch.Size([2, 1426])\n",
            "edge_attr shape: torch.Size([1426, 12])\n",
            "edge_weight shape: torch.Size([1426])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([765, 30])\n",
            "edge_index shape: torch.Size([2, 1654])\n",
            "edge_attr shape: torch.Size([1654, 12])\n",
            "edge_weight shape: torch.Size([1654])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([649, 30])\n",
            "edge_index shape: torch.Size([2, 1408])\n",
            "edge_attr shape: torch.Size([1408, 12])\n",
            "edge_weight shape: torch.Size([1408])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([734, 30])\n",
            "edge_index shape: torch.Size([2, 1586])\n",
            "edge_attr shape: torch.Size([1586, 12])\n",
            "edge_weight shape: torch.Size([1586])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([690, 30])\n",
            "edge_index shape: torch.Size([2, 1478])\n",
            "edge_attr shape: torch.Size([1478, 12])\n",
            "edge_weight shape: torch.Size([1478])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([766, 30])\n",
            "edge_index shape: torch.Size([2, 1666])\n",
            "edge_attr shape: torch.Size([1666, 12])\n",
            "edge_weight shape: torch.Size([1666])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([1024, 30])\n",
            "edge_index shape: torch.Size([2, 2204])\n",
            "edge_attr shape: torch.Size([2204, 12])\n",
            "edge_weight shape: torch.Size([2204])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([784, 30])\n",
            "edge_index shape: torch.Size([2, 1692])\n",
            "edge_attr shape: torch.Size([1692, 12])\n",
            "edge_weight shape: torch.Size([1692])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([729, 30])\n",
            "edge_index shape: torch.Size([2, 1578])\n",
            "edge_attr shape: torch.Size([1578, 12])\n",
            "edge_weight shape: torch.Size([1578])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([746, 30])\n",
            "edge_index shape: torch.Size([2, 1608])\n",
            "edge_attr shape: torch.Size([1608, 12])\n",
            "edge_weight shape: torch.Size([1608])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([701, 30])\n",
            "edge_index shape: torch.Size([2, 1506])\n",
            "edge_attr shape: torch.Size([1506, 12])\n",
            "edge_weight shape: torch.Size([1506])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([710, 30])\n",
            "edge_index shape: torch.Size([2, 1520])\n",
            "edge_attr shape: torch.Size([1520, 12])\n",
            "edge_weight shape: torch.Size([1520])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([645, 30])\n",
            "edge_index shape: torch.Size([2, 1376])\n",
            "edge_attr shape: torch.Size([1376, 12])\n",
            "edge_weight shape: torch.Size([1376])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([684, 30])\n",
            "edge_index shape: torch.Size([2, 1484])\n",
            "edge_attr shape: torch.Size([1484, 12])\n",
            "edge_weight shape: torch.Size([1484])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([761, 30])\n",
            "edge_index shape: torch.Size([2, 1652])\n",
            "edge_attr shape: torch.Size([1652, 12])\n",
            "edge_weight shape: torch.Size([1652])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([339, 30])\n",
            "edge_index shape: torch.Size([2, 728])\n",
            "edge_attr shape: torch.Size([728, 12])\n",
            "edge_weight shape: torch.Size([728])\n",
            "y shape: torch.Size([14, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([702, 30])\n",
            "edge_index shape: torch.Size([2, 1524])\n",
            "edge_attr shape: torch.Size([1524, 12])\n",
            "edge_weight shape: torch.Size([1524])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([749, 30])\n",
            "edge_index shape: torch.Size([2, 1638])\n",
            "edge_attr shape: torch.Size([1638, 12])\n",
            "edge_weight shape: torch.Size([1638])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([733, 30])\n",
            "edge_index shape: torch.Size([2, 1554])\n",
            "edge_attr shape: torch.Size([1554, 12])\n",
            "edge_weight shape: torch.Size([1554])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([842, 30])\n",
            "edge_index shape: torch.Size([2, 1812])\n",
            "edge_attr shape: torch.Size([1812, 12])\n",
            "edge_weight shape: torch.Size([1812])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([947, 30])\n",
            "edge_index shape: torch.Size([2, 2054])\n",
            "edge_attr shape: torch.Size([2054, 12])\n",
            "edge_weight shape: torch.Size([2054])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([662, 30])\n",
            "edge_index shape: torch.Size([2, 1418])\n",
            "edge_attr shape: torch.Size([1418, 12])\n",
            "edge_weight shape: torch.Size([1418])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([760, 30])\n",
            "edge_index shape: torch.Size([2, 1652])\n",
            "edge_attr shape: torch.Size([1652, 12])\n",
            "edge_weight shape: torch.Size([1652])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([625, 30])\n",
            "edge_index shape: torch.Size([2, 1334])\n",
            "edge_attr shape: torch.Size([1334, 12])\n",
            "edge_weight shape: torch.Size([1334])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([892, 30])\n",
            "edge_index shape: torch.Size([2, 1892])\n",
            "edge_attr shape: torch.Size([1892, 12])\n",
            "edge_weight shape: torch.Size([1892])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([718, 30])\n",
            "edge_index shape: torch.Size([2, 1556])\n",
            "edge_attr shape: torch.Size([1556, 12])\n",
            "edge_weight shape: torch.Size([1556])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([753, 30])\n",
            "edge_index shape: torch.Size([2, 1642])\n",
            "edge_attr shape: torch.Size([1642, 12])\n",
            "edge_weight shape: torch.Size([1642])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([796, 30])\n",
            "edge_index shape: torch.Size([2, 1718])\n",
            "edge_attr shape: torch.Size([1718, 12])\n",
            "edge_weight shape: torch.Size([1718])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([672, 30])\n",
            "edge_index shape: torch.Size([2, 1458])\n",
            "edge_attr shape: torch.Size([1458, 12])\n",
            "edge_weight shape: torch.Size([1458])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([795, 30])\n",
            "edge_index shape: torch.Size([2, 1724])\n",
            "edge_attr shape: torch.Size([1724, 12])\n",
            "edge_weight shape: torch.Size([1724])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([720, 30])\n",
            "edge_index shape: torch.Size([2, 1552])\n",
            "edge_attr shape: torch.Size([1552, 12])\n",
            "edge_weight shape: torch.Size([1552])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([752, 30])\n",
            "edge_index shape: torch.Size([2, 1626])\n",
            "edge_attr shape: torch.Size([1626, 12])\n",
            "edge_weight shape: torch.Size([1626])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([706, 30])\n",
            "edge_index shape: torch.Size([2, 1514])\n",
            "edge_attr shape: torch.Size([1514, 12])\n",
            "edge_weight shape: torch.Size([1514])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([764, 30])\n",
            "edge_index shape: torch.Size([2, 1636])\n",
            "edge_attr shape: torch.Size([1636, 12])\n",
            "edge_weight shape: torch.Size([1636])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([780, 30])\n",
            "edge_index shape: torch.Size([2, 1676])\n",
            "edge_attr shape: torch.Size([1676, 12])\n",
            "edge_weight shape: torch.Size([1676])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([695, 30])\n",
            "edge_index shape: torch.Size([2, 1508])\n",
            "edge_attr shape: torch.Size([1508, 12])\n",
            "edge_weight shape: torch.Size([1508])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([752, 30])\n",
            "edge_index shape: torch.Size([2, 1622])\n",
            "edge_attr shape: torch.Size([1622, 12])\n",
            "edge_weight shape: torch.Size([1622])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([740, 30])\n",
            "edge_index shape: torch.Size([2, 1612])\n",
            "edge_attr shape: torch.Size([1612, 12])\n",
            "edge_weight shape: torch.Size([1612])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([682, 30])\n",
            "edge_index shape: torch.Size([2, 1462])\n",
            "edge_attr shape: torch.Size([1462, 12])\n",
            "edge_weight shape: torch.Size([1462])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([719, 30])\n",
            "edge_index shape: torch.Size([2, 1544])\n",
            "edge_attr shape: torch.Size([1544, 12])\n",
            "edge_weight shape: torch.Size([1544])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([692, 30])\n",
            "edge_index shape: torch.Size([2, 1492])\n",
            "edge_attr shape: torch.Size([1492, 12])\n",
            "edge_weight shape: torch.Size([1492])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([773, 30])\n",
            "edge_index shape: torch.Size([2, 1688])\n",
            "edge_attr shape: torch.Size([1688, 12])\n",
            "edge_weight shape: torch.Size([1688])\n",
            "y shape: torch.Size([32, 1])\n",
            "\n",
            "[DEBUG] Training batch - data object properties:\n",
            "x shape: torch.Size([293, 30])\n",
            "edge_index shape: torch.Size([2, 624])\n",
            "edge_attr shape: torch.Size([624, 12])\n",
            "edge_weight shape: torch.Size([624])\n",
            "y shape: torch.Size([14, 1])\n",
            "Early stopping on epoch 55\n",
            "Test Loss (MSE): 0.2455\n",
            "Test R² Score: 0.0253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
        "        super(GNN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Additional convolutional layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn_fc = BatchNorm(hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index, edge_weight)\n",
        "            x = bn(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def train_and_evaluate(model, train_loader, validation_loader, optimizer, loss_fn, num_epochs=100, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_r2 = float('-inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "            loss = loss_fn(output, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        with torch.no_grad():\n",
        "            for data in validation_loader:\n",
        "                data = data.to(device)\n",
        "                preds = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "                val_loss += loss_fn(preds, data.y).item()\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(data.y.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(validation_loader)\n",
        "        val_r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_val_r2 = val_r2\n",
        "            best_model_state = model.state_dict()\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return best_val_loss, best_val_r2\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256])\n",
        "    num_layers = trial.suggest_int('num_layers', 2, 4)\n",
        "    dropout_prob = trial.suggest_float('dropout_prob', 0.2, 0.6)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = GNN(\n",
        "        input_dim=node_input_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout_prob=dropout_prob\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    val_loss, val_r2 = train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn)\n",
        "\n",
        "    # Store the R² score in trial attributes\n",
        "    trial.set_user_attr(\"val_r2\", val_r2)\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "# Create a study and start optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "top_trials = sorted(study.trials, key=lambda trial: trial.value)[:20]\n",
        "\n",
        "print(\"\\nTop 20 Best Hyperparameter Configurations:\")\n",
        "for i, trial in enumerate(top_trials, start=1):\n",
        "    print(f\"\\nRank {i}\")\n",
        "    print(f\"Validation Loss: {trial.value}\")\n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72m5PI06kklW",
        "outputId": "bfd77995-1616-4973-d6bb-f81207b6eb32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 06:36:17,965] A new study created in memory with name: no-name-ffaabd5e-6ca7-4c7a-a0ed-5beb39de7d11\n",
            "[I 2024-10-27 06:36:21,287] Trial 0 finished with value: 0.896653750911355 and parameters: {'learning_rate': 0.00048257973154716325, 'hidden_dim': 64, 'num_layers': 4, 'dropout_prob': 0.5914608336206875, 'weight_decay': 2.0302083617436925e-05}. Best is trial 0 with value: 0.896653750911355.\n",
            "[I 2024-10-27 06:36:27,116] Trial 1 finished with value: 0.3417006880044937 and parameters: {'learning_rate': 0.006870136818610554, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.26338755782920353, 'weight_decay': 0.0005565644190394335}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:36:36,057] Trial 2 finished with value: 0.6955573284067214 and parameters: {'learning_rate': 5.459278623091852e-05, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.3798436423534475, 'weight_decay': 3.9406217200116035e-06}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:36:40,839] Trial 3 finished with value: 0.6250426061451435 and parameters: {'learning_rate': 0.0002023871808052798, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.5238895591544047, 'weight_decay': 4.269751586169917e-05}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:36:45,697] Trial 4 finished with value: 0.4750446155667305 and parameters: {'learning_rate': 0.00016992395285236948, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.3855279473822772, 'weight_decay': 1.8349446435639715e-06}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:36:55,907] Trial 5 finished with value: 0.4967855475842953 and parameters: {'learning_rate': 4.8403175860307596e-05, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.2754736741446341, 'weight_decay': 2.3153345016414893e-05}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:06,864] Trial 6 finished with value: 0.37552038580179214 and parameters: {'learning_rate': 0.0006090397229119657, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.3758147080341817, 'weight_decay': 0.0007052998967063191}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:24,544] Trial 7 finished with value: 0.3677743338048458 and parameters: {'learning_rate': 0.00015888641403284486, 'hidden_dim': 256, 'num_layers': 2, 'dropout_prob': 0.3669592377216393, 'weight_decay': 1.7240549611100955e-05}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:31,411] Trial 8 finished with value: 0.3633970841765404 and parameters: {'learning_rate': 0.002036992375009779, 'hidden_dim': 256, 'num_layers': 2, 'dropout_prob': 0.2799766369961492, 'weight_decay': 6.6418530670667055e-06}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:37,424] Trial 9 finished with value: 0.40790707245469093 and parameters: {'learning_rate': 0.0028790920497289416, 'hidden_dim': 64, 'num_layers': 2, 'dropout_prob': 0.46536699298869466, 'weight_decay': 1.2185095209951455e-05}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:44,236] Trial 10 finished with value: 0.3491147346794605 and parameters: {'learning_rate': 0.008456702120950994, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.20126681351836107, 'weight_decay': 0.000818394652549568}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:48,019] Trial 11 finished with value: 0.4330192133784294 and parameters: {'learning_rate': 0.006822108003039771, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.20383217011795943, 'weight_decay': 0.0008523427674131201}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:37:57,500] Trial 12 finished with value: 0.36373190209269524 and parameters: {'learning_rate': 0.009915258401954187, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.21303213207786723, 'weight_decay': 0.0001502878733518798}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:38:03,559] Trial 13 finished with value: 0.37879490852355957 and parameters: {'learning_rate': 0.001880432255802555, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.28104809266608594, 'weight_decay': 0.00022833871425005504}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:38:10,220] Trial 14 finished with value: 0.3609762415289879 and parameters: {'learning_rate': 0.00438238262171429, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.240836246130779, 'weight_decay': 0.00023248375883677653}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:38:30,517] Trial 15 finished with value: 0.6128205135464668 and parameters: {'learning_rate': 1.2137033764397768e-05, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.32249845143557665, 'weight_decay': 6.917684067264771e-05}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:38:42,380] Trial 16 finished with value: 0.3777822032570839 and parameters: {'learning_rate': 0.001062798007643734, 'hidden_dim': 64, 'num_layers': 2, 'dropout_prob': 0.3117962543361661, 'weight_decay': 0.0004700790942615409}. Best is trial 1 with value: 0.3417006880044937.\n",
            "[I 2024-10-27 06:38:51,760] Trial 17 finished with value: 0.3279985152184963 and parameters: {'learning_rate': 0.0051212661884961815, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.43830323278471484, 'weight_decay': 9.40965312077232e-05}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:05,719] Trial 18 finished with value: 0.35723764821887016 and parameters: {'learning_rate': 0.001078972234755956, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.4510692644561726, 'weight_decay': 9.193104010098628e-05}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:10,092] Trial 19 finished with value: 0.3843635991215706 and parameters: {'learning_rate': 0.004070185978736958, 'hidden_dim': 128, 'num_layers': 2, 'dropout_prob': 0.4524248031087904, 'weight_decay': 0.0003830953323796981}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:22,485] Trial 20 finished with value: 0.36861515790224075 and parameters: {'learning_rate': 0.0013292567667821638, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.5010363385021609, 'weight_decay': 5.9669978123935126e-05}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:29,049] Trial 21 finished with value: 0.3701419122517109 and parameters: {'learning_rate': 0.007757263392079089, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.23587246239402776, 'weight_decay': 0.0009795798411465532}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:35,394] Trial 22 finished with value: 0.3505929782986641 and parameters: {'learning_rate': 0.004497230807277796, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.3358605318029549, 'weight_decay': 0.0003426761315251152}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:40,708] Trial 23 finished with value: 0.40210506692528725 and parameters: {'learning_rate': 0.009673202947591938, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.41504809575054413, 'weight_decay': 0.00012369243987146044}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:47,599] Trial 24 finished with value: 0.36345313489437103 and parameters: {'learning_rate': 0.0031021074933527395, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.24164932342204626, 'weight_decay': 0.0005248538488239451}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:39:57,662] Trial 25 finished with value: 0.3563055992126465 and parameters: {'learning_rate': 0.005603312345179077, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.41379762795987574, 'weight_decay': 0.00019697713153302602}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:40:05,872] Trial 26 finished with value: 0.3712984770536423 and parameters: {'learning_rate': 0.0019236437490632418, 'hidden_dim': 128, 'num_layers': 2, 'dropout_prob': 0.5223883371595618, 'weight_decay': 0.0003085595955147121}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:40:19,492] Trial 27 finished with value: 0.37649759091436863 and parameters: {'learning_rate': 0.0006298253999929619, 'hidden_dim': 64, 'num_layers': 4, 'dropout_prob': 0.34284231453429137, 'weight_decay': 4.106058282684223e-05}. Best is trial 17 with value: 0.3279985152184963.\n",
            "[I 2024-10-27 06:40:34,527] Trial 28 finished with value: 0.3163864277303219 and parameters: {'learning_rate': 0.002882571910894234, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.26727857244618713, 'weight_decay': 0.00011319747760963502}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:40:44,936] Trial 29 finished with value: 0.38942063227295876 and parameters: {'learning_rate': 0.00042913993225397, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.3003974156081056, 'weight_decay': 0.00010924082975516764}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:40:56,250] Trial 30 finished with value: 0.41471267119050026 and parameters: {'learning_rate': 0.002723490377623523, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.5742558726009337, 'weight_decay': 5.730219904079138e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:41:01,015] Trial 31 finished with value: 0.37284716218709946 and parameters: {'learning_rate': 0.005580577425749793, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.26470901841946937, 'weight_decay': 0.0006443532516765615}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:41:06,465] Trial 32 finished with value: 0.3986164405941963 and parameters: {'learning_rate': 0.007204741533845572, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.2027421798075121, 'weight_decay': 0.00017854447048380636}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:41:16,748] Trial 33 finished with value: 0.3411737009882927 and parameters: {'learning_rate': 0.003033501040043038, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.23499437634102274, 'weight_decay': 2.741774728298911e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:41:23,756] Trial 34 finished with value: 0.3763281777501106 and parameters: {'learning_rate': 0.0035269873471708, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.2582789117365097, 'weight_decay': 2.8279756460996373e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:41:29,236] Trial 35 finished with value: 0.35204608365893364 and parameters: {'learning_rate': 0.0014323830787825722, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.29631216643067937, 'weight_decay': 1.0099836632645198e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:41:35,751] Trial 36 finished with value: 0.4172995500266552 and parameters: {'learning_rate': 0.0009627579428649327, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.34826618631243456, 'weight_decay': 2.909144006213048e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:02,521] Trial 37 finished with value: 0.562867977656424 and parameters: {'learning_rate': 9.462603930151709e-05, 'hidden_dim': 64, 'num_layers': 4, 'dropout_prob': 0.4022809313221499, 'weight_decay': 3.9762169291867874e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:06,586] Trial 38 finished with value: 0.3442940264940262 and parameters: {'learning_rate': 0.0022176986497950478, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2292962737317858, 'weight_decay': 1.0801484229145903e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:10,891] Trial 39 finished with value: 0.5527047030627728 and parameters: {'learning_rate': 0.000246148874675674, 'hidden_dim': 64, 'num_layers': 2, 'dropout_prob': 0.4828551863791331, 'weight_decay': 1.6122981194155597e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:19,253] Trial 40 finished with value: 0.3687102049589157 and parameters: {'learning_rate': 0.000739563328954588, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.42562033404288957, 'weight_decay': 2.221577453773426e-05}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:32,118] Trial 41 finished with value: 0.33732135221362114 and parameters: {'learning_rate': 0.0022439430533884807, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2270855943515645, 'weight_decay': 1.0896155884858788e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:42,888] Trial 42 finished with value: 0.32412252202630043 and parameters: {'learning_rate': 0.005310253903197501, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2596159529072331, 'weight_decay': 1.5602937978011138e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:47,321] Trial 43 finished with value: 0.3464282378554344 and parameters: {'learning_rate': 0.002643618149356828, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.224585018062682, 'weight_decay': 1.224691542975762e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:42:57,155] Trial 44 finished with value: 0.32335106655955315 and parameters: {'learning_rate': 0.0016242962419757592, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2569740987348314, 'weight_decay': 2.725222769615258e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:43:03,960] Trial 45 finished with value: 0.37027059867978096 and parameters: {'learning_rate': 0.005623476674131479, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2676339925543612, 'weight_decay': 2.134147891355391e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:43:09,445] Trial 46 finished with value: 0.3654150515794754 and parameters: {'learning_rate': 0.0015881278535506424, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.28718180140002114, 'weight_decay': 5.002135676538458e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:43:19,196] Trial 47 finished with value: 0.3570265658199787 and parameters: {'learning_rate': 0.0003478087606183814, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.25319956628835466, 'weight_decay': 1.7916483333054706e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:43:27,905] Trial 48 finished with value: 0.6380456984043121 and parameters: {'learning_rate': 1.5005094222742845e-05, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.36800152786633156, 'weight_decay': 2.929260612951289e-06}. Best is trial 28 with value: 0.3163864277303219.\n",
            "[I 2024-10-27 06:43:33,780] Trial 49 finished with value: 0.34673086926341057 and parameters: {'learning_rate': 0.004478723726337494, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.3866341154672004, 'weight_decay': 1.4858096392275331e-06}. Best is trial 28 with value: 0.3163864277303219.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Best Hyperparameter Configurations:\n",
            "\n",
            "Rank 1\n",
            "Validation Loss: 0.3163864277303219\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002882571910894234\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.26727857244618713\n",
            "  weight_decay: 0.00011319747760963502\n",
            "\n",
            "Rank 2\n",
            "Validation Loss: 0.32335106655955315\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0016242962419757592\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.2569740987348314\n",
            "  weight_decay: 2.725222769615258e-06\n",
            "\n",
            "Rank 3\n",
            "Validation Loss: 0.32412252202630043\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.005310253903197501\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.2596159529072331\n",
            "  weight_decay: 1.5602937978011138e-06\n",
            "\n",
            "Rank 4\n",
            "Validation Loss: 0.3279985152184963\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0051212661884961815\n",
            "  hidden_dim: 128\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.43830323278471484\n",
            "  weight_decay: 9.40965312077232e-05\n",
            "\n",
            "Rank 5\n",
            "Validation Loss: 0.33732135221362114\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0022439430533884807\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.2270855943515645\n",
            "  weight_decay: 1.0896155884858788e-06\n",
            "\n",
            "Rank 6\n",
            "Validation Loss: 0.3411737009882927\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.003033501040043038\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.23499437634102274\n",
            "  weight_decay: 2.741774728298911e-05\n",
            "\n",
            "Rank 7\n",
            "Validation Loss: 0.3417006880044937\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.006870136818610554\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.26338755782920353\n",
            "  weight_decay: 0.0005565644190394335\n",
            "\n",
            "Rank 8\n",
            "Validation Loss: 0.3442940264940262\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0022176986497950478\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.2292962737317858\n",
            "  weight_decay: 1.0801484229145903e-06\n",
            "\n",
            "Rank 9\n",
            "Validation Loss: 0.3464282378554344\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002643618149356828\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.224585018062682\n",
            "  weight_decay: 1.224691542975762e-06\n",
            "\n",
            "Rank 10\n",
            "Validation Loss: 0.34673086926341057\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.004478723726337494\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.3866341154672004\n",
            "  weight_decay: 1.4858096392275331e-06\n",
            "\n",
            "Rank 11\n",
            "Validation Loss: 0.3491147346794605\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.008456702120950994\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.20126681351836107\n",
            "  weight_decay: 0.000818394652549568\n",
            "\n",
            "Rank 12\n",
            "Validation Loss: 0.3505929782986641\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.004497230807277796\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.3358605318029549\n",
            "  weight_decay: 0.0003426761315251152\n",
            "\n",
            "Rank 13\n",
            "Validation Loss: 0.35204608365893364\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0014323830787825722\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.29631216643067937\n",
            "  weight_decay: 1.0099836632645198e-05\n",
            "\n",
            "Rank 14\n",
            "Validation Loss: 0.3563055992126465\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.005603312345179077\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.41379762795987574\n",
            "  weight_decay: 0.00019697713153302602\n",
            "\n",
            "Rank 15\n",
            "Validation Loss: 0.3570265658199787\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0003478087606183814\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.25319956628835466\n",
            "  weight_decay: 1.7916483333054706e-06\n",
            "\n",
            "Rank 16\n",
            "Validation Loss: 0.35723764821887016\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.001078972234755956\n",
            "  hidden_dim: 128\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.4510692644561726\n",
            "  weight_decay: 9.193104010098628e-05\n",
            "\n",
            "Rank 17\n",
            "Validation Loss: 0.3609762415289879\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.00438238262171429\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.240836246130779\n",
            "  weight_decay: 0.00023248375883677653\n",
            "\n",
            "Rank 18\n",
            "Validation Loss: 0.3633970841765404\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002036992375009779\n",
            "  hidden_dim: 256\n",
            "  num_layers: 2\n",
            "  dropout_prob: 0.2799766369961492\n",
            "  weight_decay: 6.6418530670667055e-06\n",
            "\n",
            "Rank 19\n",
            "Validation Loss: 0.36345313489437103\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0031021074933527395\n",
            "  hidden_dim: 128\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.24164932342204626\n",
            "  weight_decay: 0.0005248538488239451\n",
            "\n",
            "Rank 20\n",
            "Validation Loss: 0.36373190209269524\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.009915258401954187\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.21303213207786723\n",
            "  weight_decay: 0.0001502878733518798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
        "        super(GNN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Additional convolutional layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn_fc = BatchNorm(hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index, edge_weight)\n",
        "            x = bn(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def train_and_evaluate(model, train_loader, validation_loader, optimizer, loss_fn, num_epochs=100, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_r2 = float('-inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "            loss = loss_fn(output, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        with torch.no_grad():\n",
        "            for data in validation_loader:\n",
        "                data = data.to(device)\n",
        "                preds = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "                val_loss += loss_fn(preds, data.y).item()\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(data.y.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(validation_loader)\n",
        "        val_r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_val_r2 = val_r2\n",
        "            best_model_state = model.state_dict()\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return best_val_loss, best_val_r2\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_float('learning_rate', 1e-3, 6e-3, log=True)\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256])\n",
        "    num_layers = trial.suggest_int('num_layers', 2, 4)\n",
        "    dropout_prob = trial.suggest_float('dropout_prob', 0.2, 0.4)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = GNN(\n",
        "        input_dim=node_input_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout_prob=dropout_prob\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    val_loss, val_r2 = train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn)\n",
        "\n",
        "    # Store the R² score in trial attributes\n",
        "    trial.set_user_attr(\"val_r2\", val_r2)\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "# Create a study and start optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "top_trials = sorted(study.trials, key=lambda trial: trial.value)[:20]\n",
        "\n",
        "print(\"\\nTop 20 Best Hyperparameter Configurations:\")\n",
        "for i, trial in enumerate(top_trials, start=1):\n",
        "    val_r2 = trial.user_attrs.get(\"val_r2\", \"N/A\")\n",
        "    print(f\"\\nRank {i}\")\n",
        "    print(f\"Validation Loss: {trial.value}\")\n",
        "    print(f\"Validation R²: {val_r2}\")\n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujwQSfokt9pp",
        "outputId": "e7823b06-36de-4872-ea7b-94812d94850b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 06:45:29,150] A new study created in memory with name: no-name-49a784ec-a62c-4bb3-9ebb-e9be098c7b05\n",
            "[I 2024-10-27 06:45:42,495] Trial 0 finished with value: 0.31867314875125885 and parameters: {'learning_rate': 0.005853889381856905, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.24643975196628254, 'weight_decay': 1.7335296621767495e-05}. Best is trial 0 with value: 0.31867314875125885.\n",
            "[I 2024-10-27 06:45:48,743] Trial 1 finished with value: 0.3852676711976528 and parameters: {'learning_rate': 0.0057663928447827025, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2834676439071332, 'weight_decay': 1.8584297492155293e-06}. Best is trial 0 with value: 0.31867314875125885.\n",
            "[I 2024-10-27 06:45:57,313] Trial 2 finished with value: 0.33688367158174515 and parameters: {'learning_rate': 0.005245552725711006, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.3070465171363026, 'weight_decay': 9.770128633092121e-06}. Best is trial 0 with value: 0.31867314875125885.\n",
            "[I 2024-10-27 06:46:04,277] Trial 3 finished with value: 0.34470198303461075 and parameters: {'learning_rate': 0.003461160922480123, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.29266980639019147, 'weight_decay': 2.5641098079471063e-06}. Best is trial 0 with value: 0.31867314875125885.\n",
            "[I 2024-10-27 06:46:12,291] Trial 4 finished with value: 0.31042029336094856 and parameters: {'learning_rate': 0.00546374466797597, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.39497063772830465, 'weight_decay': 4.826710735805794e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:46:19,893] Trial 5 finished with value: 0.3634543865919113 and parameters: {'learning_rate': 0.001800493596546638, 'hidden_dim': 64, 'num_layers': 4, 'dropout_prob': 0.32573909369705967, 'weight_decay': 1.3749024259195366e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:46:26,637] Trial 6 finished with value: 0.3613106273114681 and parameters: {'learning_rate': 0.004885698870841953, 'hidden_dim': 256, 'num_layers': 2, 'dropout_prob': 0.2570112029366706, 'weight_decay': 6.248742378342644e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:46:33,792] Trial 7 finished with value: 0.36847252026200294 and parameters: {'learning_rate': 0.003930562462195887, 'hidden_dim': 64, 'num_layers': 2, 'dropout_prob': 0.2921251309743374, 'weight_decay': 6.169299915399534e-06}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:46:39,343] Trial 8 finished with value: 0.39352525398135185 and parameters: {'learning_rate': 0.002576568641601335, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.35674658088411126, 'weight_decay': 1.0832740273931111e-06}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:46:42,812] Trial 9 finished with value: 0.40931234136223793 and parameters: {'learning_rate': 0.002552636064795882, 'hidden_dim': 128, 'num_layers': 2, 'dropout_prob': 0.2260108139812328, 'weight_decay': 2.336194479952916e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:46:53,412] Trial 10 finished with value: 0.34787018597126007 and parameters: {'learning_rate': 0.001193808399474865, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3998282550831931, 'weight_decay': 9.349855600991082e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:06,101] Trial 11 finished with value: 0.3441295176744461 and parameters: {'learning_rate': 0.0038902799520092696, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.2319028267492281, 'weight_decay': 3.5504490790489315e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:13,959] Trial 12 finished with value: 0.37328222393989563 and parameters: {'learning_rate': 0.0059123920788520645, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3967083764457927, 'weight_decay': 3.128414780289038e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:23,718] Trial 13 finished with value: 0.3255925700068474 and parameters: {'learning_rate': 0.003292130995771816, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.20234387394418824, 'weight_decay': 1.6829296598001328e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:28,622] Trial 14 finished with value: 0.34888242557644844 and parameters: {'learning_rate': 0.0017028333187121814, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.3509004939399817, 'weight_decay': 4.5457194493393655e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:33,690] Trial 15 finished with value: 0.3704368472099304 and parameters: {'learning_rate': 0.004676923201498814, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.25836937324924714, 'weight_decay': 5.790727551208581e-06}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:39,622] Trial 16 finished with value: 0.36637258529663086 and parameters: {'learning_rate': 0.004392866656280553, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.3684704331151298, 'weight_decay': 7.21695660065536e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:49,924] Trial 17 finished with value: 0.3189123459160328 and parameters: {'learning_rate': 0.0030786679323387615, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.26249959007520896, 'weight_decay': 8.282374497011999e-06}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:47:59,874] Trial 18 finished with value: 0.3467809520661831 and parameters: {'learning_rate': 0.0018765428414938214, 'hidden_dim': 64, 'num_layers': 3, 'dropout_prob': 0.32214569285247424, 'weight_decay': 1.8971068062072483e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:48:14,346] Trial 19 finished with value: 0.3224931638687849 and parameters: {'learning_rate': 0.0014334546675256166, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.22623588576402942, 'weight_decay': 4.438646435396314e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:48:19,749] Trial 20 finished with value: 0.3572124317288399 and parameters: {'learning_rate': 0.002111188909933052, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.3733901729598348, 'weight_decay': 2.616129439685909e-05}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:48:25,780] Trial 21 finished with value: 0.3417196162045002 and parameters: {'learning_rate': 0.003246158451748613, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.2662445255168239, 'weight_decay': 6.724375993636993e-06}. Best is trial 4 with value: 0.31042029336094856.\n",
            "[I 2024-10-27 06:48:38,715] Trial 22 finished with value: 0.304990042001009 and parameters: {'learning_rate': 0.002871644968927441, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.24586519228644454, 'weight_decay': 1.1392411953795333e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:48:47,485] Trial 23 finished with value: 0.3328186087310314 and parameters: {'learning_rate': 0.0010130735333437944, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.24148244361510707, 'weight_decay': 1.3525569041492973e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:00,605] Trial 24 finished with value: 0.3237554207444191 and parameters: {'learning_rate': 0.00395540333107784, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.20080573035866428, 'weight_decay': 3.9450834496146996e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:11,562] Trial 25 finished with value: 0.33329974859952927 and parameters: {'learning_rate': 0.0052946590129690765, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.27936398263364975, 'weight_decay': 4.1901197069534025e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:17,668] Trial 26 finished with value: 0.35291745886206627 and parameters: {'learning_rate': 0.0027859621965567512, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.24518576199850145, 'weight_decay': 1.1831466134129147e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:24,029] Trial 27 finished with value: 0.3726748898625374 and parameters: {'learning_rate': 0.0021956554320450568, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.2127019289232602, 'weight_decay': 5.134506371124958e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:32,969] Trial 28 finished with value: 0.36145492643117905 and parameters: {'learning_rate': 0.0059945534038083614, 'hidden_dim': 64, 'num_layers': 4, 'dropout_prob': 0.31260553907198, 'weight_decay': 1.8847660574921695e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:42,072] Trial 29 finished with value: 0.32662692107260227 and parameters: {'learning_rate': 0.005281284200303988, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.3405461237352161, 'weight_decay': 3.3723003987828035e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:50,806] Trial 30 finished with value: 0.3453981280326843 and parameters: {'learning_rate': 0.004245334438955331, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.2815080066915977, 'weight_decay': 8.771649708739777e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:49:57,110] Trial 31 finished with value: 0.37383587285876274 and parameters: {'learning_rate': 0.002803167310531802, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.2696253696300582, 'weight_decay': 8.049609290575946e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:01,089] Trial 32 finished with value: 0.4093259759247303 and parameters: {'learning_rate': 0.003014347348020346, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.24505793085861743, 'weight_decay': 4.086406700521144e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:10,406] Trial 33 finished with value: 0.34942618757486343 and parameters: {'learning_rate': 0.0035877658196341766, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.258596001147702, 'weight_decay': 9.930556749612202e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:20,376] Trial 34 finished with value: 0.3362922891974449 and parameters: {'learning_rate': 0.002244995467179098, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.21571848728920978, 'weight_decay': 2.0435645825750895e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:26,428] Trial 35 finished with value: 0.3888266272842884 and parameters: {'learning_rate': 0.004971425403303145, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3021836684584766, 'weight_decay': 1.2251771215884577e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:33,092] Trial 36 finished with value: 0.34849629551172256 and parameters: {'learning_rate': 0.005423530335217716, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.27253678350096827, 'weight_decay': 1.5935783070604415e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:39,334] Trial 37 finished with value: 0.3723687566816807 and parameters: {'learning_rate': 0.004507454245522578, 'hidden_dim': 64, 'num_layers': 2, 'dropout_prob': 0.2914553298509337, 'weight_decay': 5.047521465890893e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:45,072] Trial 38 finished with value: 0.37925755977630615 and parameters: {'learning_rate': 0.0036870943527095097, 'hidden_dim': 128, 'num_layers': 4, 'dropout_prob': 0.2393072358071448, 'weight_decay': 2.7896019883386607e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:50:50,704] Trial 39 finished with value: 0.383475573733449 and parameters: {'learning_rate': 0.002440791554049735, 'hidden_dim': 256, 'num_layers': 3, 'dropout_prob': 0.25375250232373237, 'weight_decay': 2.3397111839206736e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:51:02,192] Trial 40 finished with value: 0.3140828497707844 and parameters: {'learning_rate': 0.0028900572449254212, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.38586553761500564, 'weight_decay': 7.606109404026744e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:51:09,867] Trial 41 finished with value: 0.33952964283525944 and parameters: {'learning_rate': 0.0028557669910331557, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.37600963289235867, 'weight_decay': 7.8031315652564e-06}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:51:20,505] Trial 42 finished with value: 0.3056143447756767 and parameters: {'learning_rate': 0.003035524776798066, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.38676970372114594, 'weight_decay': 1.0110408780655643e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:51:29,269] Trial 43 finished with value: 0.3644491769373417 and parameters: {'learning_rate': 0.0024508903715478236, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.38527197741968816, 'weight_decay': 9.817213082634007e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:51:38,882] Trial 44 finished with value: 0.3449048399925232 and parameters: {'learning_rate': 0.0033156633358704998, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3857604593244645, 'weight_decay': 1.1098621000217083e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:51:47,050] Trial 45 finished with value: 0.39725175872445107 and parameters: {'learning_rate': 0.004134660554519778, 'hidden_dim': 64, 'num_layers': 4, 'dropout_prob': 0.3888129744840543, 'weight_decay': 1.707434221969197e-05}. Best is trial 22 with value: 0.304990042001009.\n",
            "[I 2024-10-27 06:52:00,025] Trial 46 finished with value: 0.2895614169538021 and parameters: {'learning_rate': 0.0019277663354745351, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3634031766923915, 'weight_decay': 6.81447655067068e-05}. Best is trial 46 with value: 0.2895614169538021.\n",
            "[I 2024-10-27 06:52:08,493] Trial 47 finished with value: 0.3397619407624006 and parameters: {'learning_rate': 0.002001041282340452, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3625016457451469, 'weight_decay': 7.298592639713261e-05}. Best is trial 46 with value: 0.2895614169538021.\n",
            "[I 2024-10-27 06:52:19,944] Trial 48 finished with value: 0.33161861449480057 and parameters: {'learning_rate': 0.001375629598633658, 'hidden_dim': 256, 'num_layers': 4, 'dropout_prob': 0.3451352986372318, 'weight_decay': 5.432674457660768e-05}. Best is trial 46 with value: 0.2895614169538021.\n",
            "[I 2024-10-27 06:52:28,984] Trial 49 finished with value: 0.3728751428425312 and parameters: {'learning_rate': 0.0016279023223267246, 'hidden_dim': 128, 'num_layers': 3, 'dropout_prob': 0.3933133434567916, 'weight_decay': 7.993365953847597e-05}. Best is trial 46 with value: 0.2895614169538021.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Best Hyperparameter Configurations:\n",
            "\n",
            "Rank 1\n",
            "Validation Loss: 0.2895614169538021\n",
            "Validation R²: 0.6852289256858486\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0019277663354745351\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.3634031766923915\n",
            "  weight_decay: 6.81447655067068e-05\n",
            "\n",
            "Rank 2\n",
            "Validation Loss: 0.304990042001009\n",
            "Validation R²: 0.6549825597142724\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002871644968927441\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.24586519228644454\n",
            "  weight_decay: 1.1392411953795333e-05\n",
            "\n",
            "Rank 3\n",
            "Validation Loss: 0.3056143447756767\n",
            "Validation R²: 0.6643124311415785\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.003035524776798066\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.38676970372114594\n",
            "  weight_decay: 1.0110408780655643e-05\n",
            "\n",
            "Rank 4\n",
            "Validation Loss: 0.31042029336094856\n",
            "Validation R²: 0.6531895434973767\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.00546374466797597\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.39497063772830465\n",
            "  weight_decay: 4.826710735805794e-05\n",
            "\n",
            "Rank 5\n",
            "Validation Loss: 0.3140828497707844\n",
            "Validation R²: 0.651839967551715\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0028900572449254212\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.38586553761500564\n",
            "  weight_decay: 7.606109404026744e-06\n",
            "\n",
            "Rank 6\n",
            "Validation Loss: 0.31867314875125885\n",
            "Validation R²: 0.6394969643955101\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.005853889381856905\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.24643975196628254\n",
            "  weight_decay: 1.7335296621767495e-05\n",
            "\n",
            "Rank 7\n",
            "Validation Loss: 0.3189123459160328\n",
            "Validation R²: 0.6504253449699545\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0030786679323387615\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.26249959007520896\n",
            "  weight_decay: 8.282374497011999e-06\n",
            "\n",
            "Rank 8\n",
            "Validation Loss: 0.3224931638687849\n",
            "Validation R²: 0.635751053539336\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0014334546675256166\n",
            "  hidden_dim: 128\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.22623588576402942\n",
            "  weight_decay: 4.438646435396314e-05\n",
            "\n",
            "Rank 9\n",
            "Validation Loss: 0.3237554207444191\n",
            "Validation R²: 0.66817178393422\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.00395540333107784\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.20080573035866428\n",
            "  weight_decay: 3.9450834496146996e-06\n",
            "\n",
            "Rank 10\n",
            "Validation Loss: 0.3255925700068474\n",
            "Validation R²: 0.6535992011994194\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.003292130995771816\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.20234387394418824\n",
            "  weight_decay: 1.6829296598001328e-05\n",
            "\n",
            "Rank 11\n",
            "Validation Loss: 0.32662692107260227\n",
            "Validation R²: 0.627460564844115\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.005281284200303988\n",
            "  hidden_dim: 128\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.3405461237352161\n",
            "  weight_decay: 3.3723003987828035e-05\n",
            "\n",
            "Rank 12\n",
            "Validation Loss: 0.33161861449480057\n",
            "Validation R²: 0.6338150864840673\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.001375629598633658\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.3451352986372318\n",
            "  weight_decay: 5.432674457660768e-05\n",
            "\n",
            "Rank 13\n",
            "Validation Loss: 0.3328186087310314\n",
            "Validation R²: 0.6376371345500194\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0010130735333437944\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.24148244361510707\n",
            "  weight_decay: 1.3525569041492973e-05\n",
            "\n",
            "Rank 14\n",
            "Validation Loss: 0.33329974859952927\n",
            "Validation R²: 0.6323506840697982\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0052946590129690765\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.27936398263364975\n",
            "  weight_decay: 4.1901197069534025e-06\n",
            "\n",
            "Rank 15\n",
            "Validation Loss: 0.3362922891974449\n",
            "Validation R²: 0.6365607308760723\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002244995467179098\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.21571848728920978\n",
            "  weight_decay: 2.0435645825750895e-06\n",
            "\n",
            "Rank 16\n",
            "Validation Loss: 0.33688367158174515\n",
            "Validation R²: 0.6226060884293765\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.005245552725711006\n",
            "  hidden_dim: 256\n",
            "  num_layers: 3\n",
            "  dropout_prob: 0.3070465171363026\n",
            "  weight_decay: 9.770128633092121e-06\n",
            "\n",
            "Rank 17\n",
            "Validation Loss: 0.33952964283525944\n",
            "Validation R²: 0.6126872167445604\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0028557669910331557\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.37600963289235867\n",
            "  weight_decay: 7.8031315652564e-06\n",
            "\n",
            "Rank 18\n",
            "Validation Loss: 0.3397619407624006\n",
            "Validation R²: 0.6150892139289499\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002001041282340452\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.3625016457451469\n",
            "  weight_decay: 7.298592639713261e-05\n",
            "\n",
            "Rank 19\n",
            "Validation Loss: 0.3417196162045002\n",
            "Validation R²: 0.6399007925995774\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.003246158451748613\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.2662445255168239\n",
            "  weight_decay: 6.724375993636993e-06\n",
            "\n",
            "Rank 20\n",
            "Validation Loss: 0.3441295176744461\n",
            "Validation R²: 0.6227640066325837\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0038902799520092696\n",
            "  hidden_dim: 256\n",
            "  num_layers: 4\n",
            "  dropout_prob: 0.2319028267492281\n",
            "  weight_decay: 3.5504490790489315e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
        "        super(GNN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Additional convolutional layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn_fc = BatchNorm(hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index, edge_weight)\n",
        "            x = bn(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def train_and_evaluate(model, train_loader, validation_loader, optimizer, loss_fn, num_epochs=100, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_r2 = float('-inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "            loss = loss_fn(output, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        with torch.no_grad():\n",
        "            for data in validation_loader:\n",
        "                data = data.to(device)\n",
        "                preds = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
        "                val_loss += loss_fn(preds, data.y).item()\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(data.y.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(validation_loader)\n",
        "        val_r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_val_r2 = val_r2\n",
        "            best_model_state = model.state_dict()\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return best_val_loss, best_val_r2\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_float('learning_rate', 1e-3, 6e-3, log=True)\n",
        "    hidden_dim = 256  # Fixed based on best results\n",
        "    num_layers = 4  # Fixed to 4 based on top performances\n",
        "    dropout_prob = trial.suggest_float('dropout_prob', 0.2, 0.4)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = GNN(\n",
        "        input_dim=node_input_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout_prob=dropout_prob\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    val_loss, val_r2 = train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn)\n",
        "\n",
        "    # Store the R² score in trial attributes\n",
        "    trial.set_user_attr(\"val_r2\", val_r2)\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "# Create a study and start optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "top_trials = sorted(study.trials, key=lambda trial: trial.value)[:20]\n",
        "\n",
        "print(\"\\nTop 20 Best Hyperparameter Configurations:\")\n",
        "for i, trial in enumerate(top_trials, start=1):\n",
        "    val_r2 = trial.user_attrs.get(\"val_r2\", \"N/A\")\n",
        "    print(f\"\\nRank {i}\")\n",
        "    print(f\"Validation Loss: {trial.value}\")\n",
        "    print(f\"Validation R²: {val_r2}\")\n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHRHp_r1wkX9",
        "outputId": "fe4060a5-0c5b-4188-ab63-05deab6a76ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-27 06:55:50,097] A new study created in memory with name: no-name-ad9a6a2a-9750-450e-813f-6e0b274480f7\n",
            "[I 2024-10-27 06:56:04,948] Trial 0 finished with value: 0.30732857435941696 and parameters: {'learning_rate': 0.003690423687348638, 'dropout_prob': 0.3845251089301673, 'weight_decay': 7.625542466351823e-05}. Best is trial 0 with value: 0.30732857435941696.\n",
            "[I 2024-10-27 06:56:10,353] Trial 1 finished with value: 0.37188570946455 and parameters: {'learning_rate': 0.0011494708692105415, 'dropout_prob': 0.2101116626043263, 'weight_decay': 5.6163724423547785e-05}. Best is trial 0 with value: 0.30732857435941696.\n",
            "[I 2024-10-27 06:56:23,869] Trial 2 finished with value: 0.31907840073108673 and parameters: {'learning_rate': 0.0011701797072965765, 'dropout_prob': 0.3233232978470258, 'weight_decay': 4.824390995170893e-06}. Best is trial 0 with value: 0.30732857435941696.\n",
            "[I 2024-10-27 06:56:29,742] Trial 3 finished with value: 0.32279281690716743 and parameters: {'learning_rate': 0.001335740275972585, 'dropout_prob': 0.2456322349666216, 'weight_decay': 1.1069275653425898e-06}. Best is trial 0 with value: 0.30732857435941696.\n",
            "[I 2024-10-27 06:56:42,401] Trial 4 finished with value: 0.287671597674489 and parameters: {'learning_rate': 0.0019415019430134137, 'dropout_prob': 0.3025101833745779, 'weight_decay': 1.0255748898490875e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:56:47,270] Trial 5 finished with value: 0.3582408130168915 and parameters: {'learning_rate': 0.0025789680118705163, 'dropout_prob': 0.31254459995761197, 'weight_decay': 7.832455003191583e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:01,651] Trial 6 finished with value: 0.33405205607414246 and parameters: {'learning_rate': 0.0011514035391335265, 'dropout_prob': 0.3826710252532794, 'weight_decay': 5.076889442930839e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:08,281] Trial 7 finished with value: 0.3564656116068363 and parameters: {'learning_rate': 0.001960715460541524, 'dropout_prob': 0.24404725479356418, 'weight_decay': 4.9417283330397275e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:14,401] Trial 8 finished with value: 0.3752657398581505 and parameters: {'learning_rate': 0.002893120485618687, 'dropout_prob': 0.3993815331528091, 'weight_decay': 2.6756009107872777e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:25,301] Trial 9 finished with value: 0.3312520533800125 and parameters: {'learning_rate': 0.0019047305485225818, 'dropout_prob': 0.36518521019155353, 'weight_decay': 8.921650456943968e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:29,891] Trial 10 finished with value: 0.38509775325655937 and parameters: {'learning_rate': 0.005442455635799552, 'dropout_prob': 0.2761448699676481, 'weight_decay': 2.409511555473073e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:36,644] Trial 11 finished with value: 0.3730316795408726 and parameters: {'learning_rate': 0.003966374613498236, 'dropout_prob': 0.3465923435708157, 'weight_decay': 1.927912489520456e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:45,157] Trial 12 finished with value: 0.3580436408519745 and parameters: {'learning_rate': 0.003779865660241326, 'dropout_prob': 0.2802376526828053, 'weight_decay': 8.852393775772139e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:51,556] Trial 13 finished with value: 0.34349774569272995 and parameters: {'learning_rate': 0.0017026423230335616, 'dropout_prob': 0.3462709135682148, 'weight_decay': 2.2616468948192203e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:57:55,799] Trial 14 finished with value: 0.3480789549648762 and parameters: {'learning_rate': 0.003238262931227339, 'dropout_prob': 0.2862574076374269, 'weight_decay': 2.5547517048728745e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:03,594] Trial 15 finished with value: 0.37703927978873253 and parameters: {'learning_rate': 0.005915519042135934, 'dropout_prob': 0.32907759623303257, 'weight_decay': 1.402813082797961e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:13,148] Trial 16 finished with value: 0.3188674822449684 and parameters: {'learning_rate': 0.002259833306542871, 'dropout_prob': 0.2541009793851111, 'weight_decay': 9.446131763520155e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:32,769] Trial 17 finished with value: 0.3048689793795347 and parameters: {'learning_rate': 0.004726958377266424, 'dropout_prob': 0.36338284551107825, 'weight_decay': 4.697008492781254e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:38,580] Trial 18 finished with value: 0.41162774339318275 and parameters: {'learning_rate': 0.004737344974218077, 'dropout_prob': 0.35126937815320236, 'weight_decay': 4.382785918142259e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:47,397] Trial 19 finished with value: 0.3571116477251053 and parameters: {'learning_rate': 0.0015785817329550342, 'dropout_prob': 0.3030180340796789, 'weight_decay': 1.0397480372477455e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:51,587] Trial 20 finished with value: 0.3298380374908447 and parameters: {'learning_rate': 0.0023441752228096067, 'dropout_prob': 0.21922453404916298, 'weight_decay': 5.3393987079746585e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:58:56,455] Trial 21 finished with value: 0.38556136190891266 and parameters: {'learning_rate': 0.00463873529214253, 'dropout_prob': 0.3825140224123921, 'weight_decay': 2.7911797748833547e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:59:01,271] Trial 22 finished with value: 0.3961340934038162 and parameters: {'learning_rate': 0.00374668645999464, 'dropout_prob': 0.3686769677913684, 'weight_decay': 3.311641261005447e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:59:19,564] Trial 23 finished with value: 0.3201024606823921 and parameters: {'learning_rate': 0.00459954553899807, 'dropout_prob': 0.39489157524467816, 'weight_decay': 1.2562157450518683e-05}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:59:27,032] Trial 24 finished with value: 0.3416457623243332 and parameters: {'learning_rate': 0.003097910937821406, 'dropout_prob': 0.36549376264446826, 'weight_decay': 6.396553951790634e-06}. Best is trial 4 with value: 0.287671597674489.\n",
            "[I 2024-10-27 06:59:42,447] Trial 25 finished with value: 0.2696421407163143 and parameters: {'learning_rate': 0.003474142115985913, 'dropout_prob': 0.33307380744353443, 'weight_decay': 1.4830221862325078e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 06:59:47,302] Trial 26 finished with value: 0.3247831277549267 and parameters: {'learning_rate': 0.0027138217196424654, 'dropout_prob': 0.3289945834659324, 'weight_decay': 1.9166156212525537e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 06:59:57,062] Trial 27 finished with value: 0.3324417769908905 and parameters: {'learning_rate': 0.0032701308928516715, 'dropout_prob': 0.29308808221368277, 'weight_decay': 1.6985831025013256e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:00:09,480] Trial 28 finished with value: 0.32138723880052567 and parameters: {'learning_rate': 0.00420844037243854, 'dropout_prob': 0.30925274008645065, 'weight_decay': 3.7154392788872043e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:00:20,990] Trial 29 finished with value: 0.329617153853178 and parameters: {'learning_rate': 0.005221114850485769, 'dropout_prob': 0.33538458993844444, 'weight_decay': 1.6549080230413954e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:00:33,894] Trial 30 finished with value: 0.318766538053751 and parameters: {'learning_rate': 0.0021051560980569297, 'dropout_prob': 0.26254999432018233, 'weight_decay': 1.2610819372955173e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:00:44,360] Trial 31 finished with value: 0.3399197496473789 and parameters: {'learning_rate': 0.0033334963712719217, 'dropout_prob': 0.3794627016021416, 'weight_decay': 3.2844670219311974e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:00:55,508] Trial 32 finished with value: 0.3492012657225132 and parameters: {'learning_rate': 0.003625065723110156, 'dropout_prob': 0.35613372048912034, 'weight_decay': 6.501690385284302e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:01,171] Trial 33 finished with value: 0.3267863430082798 and parameters: {'learning_rate': 0.0014971826522905813, 'dropout_prob': 0.3183460819071775, 'weight_decay': 1.4492904817048177e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:09,231] Trial 34 finished with value: 0.3292132467031479 and parameters: {'learning_rate': 0.002860289565432326, 'dropout_prob': 0.3371240235115637, 'weight_decay': 3.652089485173413e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:15,259] Trial 35 finished with value: 0.35472840443253517 and parameters: {'learning_rate': 0.0024887211668127135, 'dropout_prob': 0.37468528377163873, 'weight_decay': 8.501967690835307e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:23,052] Trial 36 finished with value: 0.34503260999917984 and parameters: {'learning_rate': 0.0041744638514805395, 'dropout_prob': 0.38822720512899206, 'weight_decay': 5.869681722462093e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:30,920] Trial 37 finished with value: 0.3804156295955181 and parameters: {'learning_rate': 0.005065112914770733, 'dropout_prob': 0.35667531897962146, 'weight_decay': 4.187774139324704e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:38,678] Trial 38 finished with value: 0.34128307923674583 and parameters: {'learning_rate': 0.0013709585672490397, 'dropout_prob': 0.3172039082338398, 'weight_decay': 1.191734602578327e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:48,341] Trial 39 finished with value: 0.3486753888428211 and parameters: {'learning_rate': 0.0018239274065957683, 'dropout_prob': 0.3005736218188228, 'weight_decay': 2.281827085826542e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:01:56,846] Trial 40 finished with value: 0.3732161708176136 and parameters: {'learning_rate': 0.004300224367730164, 'dropout_prob': 0.33844051249663953, 'weight_decay': 3.4680311649828746e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:01,749] Trial 41 finished with value: 0.3775559514760971 and parameters: {'learning_rate': 0.0020846741401570196, 'dropout_prob': 0.26582076121288745, 'weight_decay': 1.375193336263942e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:12,549] Trial 42 finished with value: 0.330270042642951 and parameters: {'learning_rate': 0.0026250107095989687, 'dropout_prob': 0.26709849006541186, 'weight_decay': 1.0821617519504351e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:17,008] Trial 43 finished with value: 0.3619111515581608 and parameters: {'learning_rate': 0.0020851952522207276, 'dropout_prob': 0.23854289678740692, 'weight_decay': 1.729297681699022e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:24,004] Trial 44 finished with value: 0.3289468679577112 and parameters: {'learning_rate': 0.0021849459482528744, 'dropout_prob': 0.22002445678374266, 'weight_decay': 7.290957263886919e-06}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:39,271] Trial 45 finished with value: 0.31396373733878136 and parameters: {'learning_rate': 0.0017806744386747709, 'dropout_prob': 0.29393166456727066, 'weight_decay': 2.724544771399403e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:49,524] Trial 46 finished with value: 0.31380713172256947 and parameters: {'learning_rate': 0.001788110240025861, 'dropout_prob': 0.290040210153462, 'weight_decay': 6.76280969055158e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:02:59,561] Trial 47 finished with value: 0.32663315907120705 and parameters: {'learning_rate': 0.0013148895652733915, 'dropout_prob': 0.27903342427880556, 'weight_decay': 5.895589012079283e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:03:05,922] Trial 48 finished with value: 0.32492564246058464 and parameters: {'learning_rate': 0.0035189706854162664, 'dropout_prob': 0.31118463764091736, 'weight_decay': 6.76210938980202e-05}. Best is trial 25 with value: 0.2696421407163143.\n",
            "[I 2024-10-27 07:03:12,799] Trial 49 finished with value: 0.36396006122231483 and parameters: {'learning_rate': 0.0012455680330219745, 'dropout_prob': 0.3999286189265104, 'weight_decay': 8.065037935922823e-05}. Best is trial 25 with value: 0.2696421407163143.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Best Hyperparameter Configurations:\n",
            "\n",
            "Rank 1\n",
            "Validation Loss: 0.2696421407163143\n",
            "Validation R²: 0.6969332438923312\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.003474142115985913\n",
            "  dropout_prob: 0.33307380744353443\n",
            "  weight_decay: 1.4830221862325078e-06\n",
            "\n",
            "Rank 2\n",
            "Validation Loss: 0.287671597674489\n",
            "Validation R²: 0.6772383800963893\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0019415019430134137\n",
            "  dropout_prob: 0.3025101833745779\n",
            "  weight_decay: 1.0255748898490875e-05\n",
            "\n",
            "Rank 3\n",
            "Validation Loss: 0.3048689793795347\n",
            "Validation R²: 0.6524945475232564\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.004726958377266424\n",
            "  dropout_prob: 0.36338284551107825\n",
            "  weight_decay: 4.697008492781254e-06\n",
            "\n",
            "Rank 4\n",
            "Validation Loss: 0.30732857435941696\n",
            "Validation R²: 0.6706581531157694\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.003690423687348638\n",
            "  dropout_prob: 0.3845251089301673\n",
            "  weight_decay: 7.625542466351823e-05\n",
            "\n",
            "Rank 5\n",
            "Validation Loss: 0.31380713172256947\n",
            "Validation R²: 0.6428831210087961\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.001788110240025861\n",
            "  dropout_prob: 0.290040210153462\n",
            "  weight_decay: 6.76280969055158e-05\n",
            "\n",
            "Rank 6\n",
            "Validation Loss: 0.31396373733878136\n",
            "Validation R²: 0.6478711145073994\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0017806744386747709\n",
            "  dropout_prob: 0.29393166456727066\n",
            "  weight_decay: 2.724544771399403e-05\n",
            "\n",
            "Rank 7\n",
            "Validation Loss: 0.318766538053751\n",
            "Validation R²: 0.6766432659136925\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0021051560980569297\n",
            "  dropout_prob: 0.26254999432018233\n",
            "  weight_decay: 1.2610819372955173e-05\n",
            "\n",
            "Rank 8\n",
            "Validation Loss: 0.3188674822449684\n",
            "Validation R²: 0.6521050868329183\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002259833306542871\n",
            "  dropout_prob: 0.2541009793851111\n",
            "  weight_decay: 9.446131763520155e-05\n",
            "\n",
            "Rank 9\n",
            "Validation Loss: 0.31907840073108673\n",
            "Validation R²: 0.6619888563332397\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0011701797072965765\n",
            "  dropout_prob: 0.3233232978470258\n",
            "  weight_decay: 4.824390995170893e-06\n",
            "\n",
            "Rank 10\n",
            "Validation Loss: 0.3201024606823921\n",
            "Validation R²: 0.6761305747889421\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.00459954553899807\n",
            "  dropout_prob: 0.39489157524467816\n",
            "  weight_decay: 1.2562157450518683e-05\n",
            "\n",
            "Rank 11\n",
            "Validation Loss: 0.32138723880052567\n",
            "Validation R²: 0.6566054129950019\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.00420844037243854\n",
            "  dropout_prob: 0.30925274008645065\n",
            "  weight_decay: 3.7154392788872043e-06\n",
            "\n",
            "Rank 12\n",
            "Validation Loss: 0.32279281690716743\n",
            "Validation R²: 0.6471636884738948\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.001335740275972585\n",
            "  dropout_prob: 0.2456322349666216\n",
            "  weight_decay: 1.1069275653425898e-06\n",
            "\n",
            "Rank 13\n",
            "Validation Loss: 0.3247831277549267\n",
            "Validation R²: 0.653722762189701\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0027138217196424654\n",
            "  dropout_prob: 0.3289945834659324\n",
            "  weight_decay: 1.9166156212525537e-06\n",
            "\n",
            "Rank 14\n",
            "Validation Loss: 0.32492564246058464\n",
            "Validation R²: 0.6421191029259432\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0035189706854162664\n",
            "  dropout_prob: 0.31118463764091736\n",
            "  weight_decay: 6.76210938980202e-05\n",
            "\n",
            "Rank 15\n",
            "Validation Loss: 0.32663315907120705\n",
            "Validation R²: 0.643688544649764\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0013148895652733915\n",
            "  dropout_prob: 0.27903342427880556\n",
            "  weight_decay: 5.895589012079283e-05\n",
            "\n",
            "Rank 16\n",
            "Validation Loss: 0.3267863430082798\n",
            "Validation R²: 0.6467296129969723\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0014971826522905813\n",
            "  dropout_prob: 0.3183460819071775\n",
            "  weight_decay: 1.4492904817048177e-06\n",
            "\n",
            "Rank 17\n",
            "Validation Loss: 0.3289468679577112\n",
            "Validation R²: 0.6254476484964164\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0021849459482528744\n",
            "  dropout_prob: 0.22002445678374266\n",
            "  weight_decay: 7.290957263886919e-06\n",
            "\n",
            "Rank 18\n",
            "Validation Loss: 0.3292132467031479\n",
            "Validation R²: 0.6433722313139598\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.002860289565432326\n",
            "  dropout_prob: 0.3371240235115637\n",
            "  weight_decay: 3.652089485173413e-06\n",
            "\n",
            "Rank 19\n",
            "Validation Loss: 0.329617153853178\n",
            "Validation R²: 0.6547355924584626\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.005221114850485769\n",
            "  dropout_prob: 0.33538458993844444\n",
            "  weight_decay: 1.6549080230413954e-06\n",
            "\n",
            "Rank 20\n",
            "Validation Loss: 0.3298380374908447\n",
            "Validation R²: 0.6510290037229277\n",
            "Hyperparameters:\n",
            "  learning_rate: 0.0023441752228096067\n",
            "  dropout_prob: 0.21922453404916298\n",
            "  weight_decay: 5.3393987079746585e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion**"
      ],
      "metadata": {
        "id": "4M0mIYe7wwOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Classification Results with RandomForest Classifier\n",
        "\n",
        "The classification model achieved a strong performance, with an accuracy of 0.88, a precision of 0.86, recall of 0.81, and an F1 score of 0.84. This indicates that the model effectively distinguishes between the positive and negative classes in the dataset. The high accuracy and balanced precision-recall values reflect the classifier's ability to make reliable predictions across both classes. The results suggest that the molecular descriptors are likely capturing relevant information related to the target classification, contributing to the model’s robustness."
      ],
      "metadata": {
        "id": "qygl8MFMzH4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Regression Results\n",
        "\n"
      ],
      "metadata": {
        "id": "EkIykZIYzWxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1. RandomForest and XGBoost Models:\n",
        "The RandomForest models using either descriptors or ECFP independently achieved moderate R² scores (0.53 and 0.49, respectively). Combining descriptors with ECFP improved the R² to 0.52, indicating that incorporating both structural and physicochemical features enhances model performance. After hyperparameter tuning, the RandomForest model achieved an R² of 0.56, showing improvements from optimized parameter selection, such as increased depth and refined feature splitting criteria.\n"
      ],
      "metadata": {
        "id": "ZVnlcFaPzb45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2. PLS Regression Model:\n",
        "The Partial Least Squares (PLS) regression model with combined descriptors and ECFP showed R²improvement to 0.586, the highest among the regression models. This result implies that PLS is effective at capturing relationships between molecular features and the target property, likely due to its dimension reduction capabilities, which help alleviate redundancy in features."
      ],
      "metadata": {
        "id": "f6QlwtaVzhzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-3. MLP Model : The MLP model’s test R² score of 0.4821 indicates that it captures the relationship between descriptors and the target property, although not as effectively as tree-based models like RandomForest. However, the best trial achieved a validation R² of 0.576, suggesting that further tuning or potentially deeper architectures might improve generalization. The MLP model’s performance could also benefit from additional regularization techniques or by experimenting with alternative neural architectures, such as those incorporating molecular graph information."
      ],
      "metadata": {
        "id": "1No8n9PZzsdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-4. GNN Model : The GNN model showed the best overall performance, achieving a validation R² of 0.6969 with tuned hyperparameters, including learning rate, dropout, and weight decay. The GNN’s success underscores the advantage of using graph-based models to capture the intricate topological and bonding relationships within molecules, which traditional descriptor- or fingerprint-based models might not fully encapsulate. This result highlights the value of GNNs for tasks that require capturing molecular structures' inherent complexity and relational information."
      ],
      "metadata": {
        "id": "dbwlLknbz2Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, these findings suggest that the choice of model type and feature combination significantly affects predictive performance. Tree-based models like RandomForest and XGBoost excel with descriptor-fingerprint combinations, while graph-based neural networks like GNNs provide the highest accuracy by directly leveraging molecular connectivity. Future work could focus on ensembling these approaches or testing more complex graph-based architectures, which could further refine predictions and advance the model's applicability in drug discovery and molecular property prediction."
      ],
      "metadata": {
        "id": "xYxdRGh50C1e"
      }
    }
  ]
}